{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pengantar Data Mining \u00b6 Tujuan Pembelajaran \u00b6 Tujuan Pembelajaran 1. Mahasiswa dapat memahami konsep data mining 2. Mahasiswa dapat mengetahui proses data mining 3. Mahasiswa dapat mengetahui teknik teknik yang digunakan dalam data mining. Apa itu data mining \u00b6 ***The non-trivial extraction of implicit, previously unknown, and potentially useful information from data* 1 {silahkan dibaca dengan teliti} Dengan perkembangan jumlah data yang pesat disebabkan oleh adanya perkembangan teknologi informasi, sehingga hal ini yang memungkinkan data terkumpul dalam jumlah besar. Transaksi data yang dilakukan secara digital telah berkembang secara pesat diberbagai sektor bisnis. Perkembangan internet yang cukup cepat juga memiliki kontribusi dalam hal terciptanya data yang sangat besar. Fenomena ini secara signifikan berdampak pada terjadinya data transaksi yang sangat tinggi dari sisi volume dan jenis data telah dihasilkan. Bersamaan dengan kejadian ini memungkinkan terjadinya kondisi yang disebut dengan istilah kaya data tetapi miskin pengetahuan, disebabkan dengan adanya banyak data yang terkumpul tetapi sedikit dengan manfaat yang diperoleh dari besarnya data tersebut untuk kebutuhan bisnis dan lain sebgainya. Dengan data yang melimpah ini butuh suatu metode analisis data secara otomatis dari data yang berjumlah besar atau kompleks dengan tujuan untuk menemukan pola atau kecenderungan yang penting yang biasanya tidak disadari keberadaannya. Data mining merupakan merupakan subjek yang melibatkan berbagai disiplin bidang yang bertujuan bagaimana mendapatkan pengetahuan/informasi dari tumpukan data. Jika dianalogikan adalah seperti penambangan secara konvensianol misalkan menambang emas yang didapatkan dari tumpukan batu, tanah dan pasir (link video) . Jadi penekanan dari istilah penambangan adalah proses menemukan sesuatu yang berharga dari bahan-bahan mentah yang ada. Dalam kontek penambangan data adalah bagaimana mendapatkan informasi yang berharga untuk pijakan pengambilan keputusan atau tujuan tertentu. Dalama proses data mining melibatkan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar 2 . Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu **Knowledge discovery in databases (KDD) , ekstraksi pengetahuan (knowledge extraction), Analisa data/pola (data/pattern analysis), kecerdasan bisnis (business intelligence) dan data archaeology dan data dredging (Larose, 2005) Keunggulan data mining \u00b6 Studi statistik tradisional menggunakan informasi masa lalu untuk menentukan keadaan sistem di masa depan (sering disebut prediksi), sedangkan studi penambangan data menggunakan informasi masa lalu untuk membangun pola yang berdasarkan tidak hanya pada input data tetapi juga pada konsekuensi logis dari data tersebut. Proses ini juga disebut prediksi, tetapi mengandung elemen penting yang tidak ditemukan dalam analisis statistik: kemampuan untuk memberikan ekspresi tentang yang mungkin terjadi di masa depan, dibandingkan dengan apa yang ada di masa lalu (berdasarkan asumsi dari metode statistik). Dibandingkan dengan studi statistik tradisional, yang sering melihat ke belakang, bidang data mining menemukan pola dan klasifikasi yang melihat dan bahkan memprediksi masa depan. Secara singkata Data mining dapat memberikan pemahaman data yang lebih lengkap dengan menemukan pola yang sebelumnya tidak terlihat dan Membuat model yang untuk memprediksi, sehingga memungkinkan orang untuk membuat keputusan yang lebih baik, sebagai acuan tindakan untuk yang akan datang dalam membuat keputusan Tugas utama data mining \u00b6 Aktifitas utama dalam data mining adalah sebagai berikut 2 Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk Proses Data mining \u00b6 Dalam data mining proses tahapan yang dilakukan sebagai Proses standar lintas industri untuk penambangan data yang dihasilkan oleh konsorsium , yang dikenal sebagai CRISP-DM, adalah model proses standar terbuka yang menggambarkan pendekatan umum yang digunakan oleh para ahli penambangan data. Model proses untuk penambangan data ini memberikan gambaran umum siklus hidup proyek penambangan data. Ini berisi fase proyek, tugas masing-masing dan hubungan antara aktifitas aktifitas yang dilakukan setiap tahapan seperti gambar berikut: Gambar 2.1 CRISP-DM 3 Memahami Bisnis (Business Understanding ) \u00b6 Menentukan Tujuan Bisnis \u00b6 Tahapan ini menfokuskan pada pemahaman tujuan projek dan kebutuhan-kebutuhan yang diinginkan bisnis, kemudian merubahnya pengetahuan ini untuk mendefinisikan data mining dan rencana yang ingin dilakukan untuk mencapai tujuan bisnis. Tahaapan ini dilakukan untuk memahami secara menyeluruh, dari perspektif bisnis, apa yang benar-benar ingin dicapai oleh pelanggan. Selain itu pada tahapan ini analisa perlu dilakukan untuk mengungkap faktor-faktor penting, yang dapat mempengaruhi hasil proyek data mining. Membuat Rencana Proyek \u00b6 Menjelaskan rencana projek dilakuan untuk mencapai tujuan penambangan data sehingga dapat mencapai tujuan bisnis yang diinginkan. Rencana tersebut harus menentukan langkah-langkah yang harus dilakukan selama proyek , termasuk pemilihan tool dan teknik yang akan digunakan Menilai situasi dan kondisi \u00b6 Dalam hal pencarian fakta yang lebih rinci tentang semua sumber daya, kendala, asumsi, dan faktor-faktor lain yang harus dipertimbangkan dalam menentukan tujuan analisis data dan rencana proyek. Menilai sefara rinci kondisi lingkungan bisnis dan tujuan bisnis akan menentukan keberhasilan projek data mining Output dari ini adalah Inventaris sumber daya Daftar sumber daya yang tersedia untuk proyek, termasuk personil (pakar bisnis, pakar data, dukungan teknis, pakar penambangan data), data ( akses ke data langsung, gudang, atau operasional), sumber daya komputasi (platform perangkat keras), dan perangkat lunak (alat penambangan data, perangkat lunak lain yang relevan) Persyaratan, asumsi dan kendala Mendaftar semua persyaratan proyek, termasuk jadwal penyelesaian, kelengkapan dan kualitas hasil, dan keamanan, serta masalah hukum. Salah satu bagian dari tahapan ini akan memastikan apakah secara legal diizinkan menggunakannya data. Buat daftar asumsi-asumsi terkait dengan projek. Misalakan apakah apakah diasumsikan untuk memungkinkan memverifikasi selama penambangan data. Buat daftar kendala pada proyek. Ini mungkin merupakan kendala pada ketersediaan sumber daya, tetapi juga dapat mencakup kendala teknologi seperti ukuran data yang digunakan untuk pemodelan Mendaftar resiko yang akan dihadapi selama projek berlangsung (jika kemungkinan projek gagal) Memahami data ( data understanding ) \u00b6 Tahapan memahami data dimulai dengan mengumpulkan data awal dan dilanjutkan dengan dengan kegiatan-kegiatan untuk mendapatkan data yang lazim serta identifikasi data yang berkualitas, pemahaman data sangat diperlukan untuk mendeteksi bagian yang menarik dari data sehingga dapat membangun hipotesa terhadap informasi yang tersembunyi a. Mengumpulkan data awal \u00b6 Tugas Mendaftar data yang ada. Pengumpulan data diperlukan untuk memahami data. Misalkan, jika anda menggunakan tool khusus untuk memahami data, untuk menjadikan benar-benar memahami data maka data tersebut perlu diproses kedalam tool ini. Langkah ini terkait dengan langkah persiapan data. Jika anda membutuhkan berbagai sumber data, integrasi atau penyatuan data diperlukan Keluaran Daftar data yang di hasilkan dan dimana data tersebut berada, serta metode yang digunakan untuk mendapatkan data tersebut dan masalah-masalah dari data tersebut. Pada masa yang akan datang hasil dari tahapan ini sangat membantu jika kita melakukan data mining pada projek yang sama b. Mendiskripsikan data \u00b6 Tugas Mendeskripsikan data. Mengamati secara kasar dan yang tampak dari data yang diperoleh dan mendokumentasikan deskripsi data tersebut Keluaran Report dari diskripsi data. Mendeskripsikan data yang didapat, diantaranya; format dari data, jumlah data, misalkan jumlah record dan field dari masing-masing tabel, identitas dari field-field (atribut-atribut) dan karakteristik yang tampak dari data yang sudah dikumpulkan. Apakah data memenuhi kebutuhan yang terkait c. Ekplorasi data /Menyelidiki data \u00b6 Tugas Melakukan pertanyaan data mining yang dapat dilakukan dengan menggunakan queri, visualisasi dan reporting. tugas prediksi, analisa statistic sederhana, hubungan antara atribut, tugas ini terkait dengan tujuan data mining, dan persiapan data lebih lanjut Keluaran Laporan data explorasi. Pada kegitatan ini adalah menemukan hipotesa awal dan pengaruhnya pada akhir projek. Keluaran dari kegiatan ini diantaranya adalah grafik, dan plot yang menentukan karakteristik data atau yang terkait dengan sebagian data untuk penyelidikan lebih lanjut d. Verifikasi qualitas data \u00b6 Tugas Menyelidiki qualitas data dilakukan dengan untuk mengetahui apakah data lengkap ( apakah mencakup semua kebutuhan data yang diperlukan?). Apakah data tersebut mengandung error dan jika ada error-error bagaimana data yang seharusnya ?Apakah ada missing value dalam data? Jika ada maka bagaimanamenyelesaikannya? Keluaran Laporan data yang berkualitas. Daftar dari verifikasi data yang berkualitas, jika ada masalah dengan kualitas ada, maka daftar penyelesaian yang memungkinkan untuk memperbaiki kualitas data. Penyelesaian dari qualitas data secara umum sangat bergantung pada data. Persiapan data \u00b6 Tahap mempersiapkan data mencakup semua aktifitas untuk membangun dataset akhir (data yang digunakan untuk tool pemodelan) dari data mentah awal. Tugas persiapan data lebih memungkinkan untuk dilakukan beberapa kali dan tidak ada ketentuan. Tugas-tugasnya diantaranya adalah memilih table, record dan atribut juga tranformasi dan membersihkan data. Output dari persiapan data adalah data set. Data data ini akan digunakan untuk pemodelan atau tugas analisa utama dari projek. Selain itu deskripsi dari data yang akan digunakan untuk pemodelan atau pekerjaan analisa utama dari projek. Tugas-tugas dari persiapan data diantaranya adalah: a. Memilih data ( Select data) \u00b6 Tugas Menentukan data yang digunakan untuk analisa. Kriteria yang digunakan harus ada keterkaitan dengan tujuan data mining, kualitas data batasan-batasan teknis seperti batasan volume data tipe data. Perhatikan bahwa pemilihan data mencakup pemilihan atribut ( kolom-kolom ) dan juga pemilihan records (baris-baris) dalam tabel Keluaran Daftar data yang akan digunakan dan dikeluarkan serta alasan-alasan mengapa data digunakan atau dikeluarkan. b. Membersihkan data ( Clean data) \u00b6 Tugas Pada tahapan ini adalah bagaimana meningkatkan kualitas data sesuai dengan teknik yang dipilih. Beberapa diantaranya adalah memilih sebagian data yang bersih dan menyisipkan data yang hilang dengan teknik menyisipkan data hilang menggunakan model yang baik. Keluaran Penjelasan atas keputusan dan tindakan apa yang diambil untuk menangani kualitas data serta serta dampak kemungkinan hasil analisis c. Integrasi data ( integrate data) \u00b6 Tugas Pada tahapan ini dilakukan proses penggabungan dari beberapa informasi misalkan dalam bentuk beberapa tabel untuk membentuk inforasi baru yang merupakan gabungan dari beberapa tabel Keluaran Data gabungan yang terbentuk dari beberapa tabel Pemodelan \u00b6 Pada tahapan ini membangun suatu model dari data yang diperoleh dari langkah sebelumnya untuk menjawab pertanyaan kebutuhan bisnis dengan berbagai macam metode. Beberapa metode yang dapat digunakan adalah metode statistik, pembelajaran mesin, riset operasi dan sebagainya. Dalam melakukan pemodelan data beberapa hal yang dilakukan adalah memilih variabel model, menjalankan model, dan mendiagnosa. Penjelasan Data Mining adalah proses yang menggunakan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar (Turban dkk. 2005). Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu Knowledge discovery in databases (KDD), ekstraksi pengetahuan (knowledge extraction), Analisa data/pola (data/pattern analysis), kecerdasan bisnis (business intelligence) dan data archaeology dan data dredging (Larose, 2005) Referensi \u00b6 Frawley, W., Piatetsky-Shapiro, G., Matheus, C., 1991. Knowledge discovery in databases\u2014an overview. KnowledgeDiscovery in Databases 1991 1\u201330. Reprinted in AI Magazine, Fall 1992 \u21a9 Hand, D., Mannila, H., Smyth, P., 2001. Principles of Data Mining. The MIT Press: A Bradford Book, Cambridge, MA/London \u21a9 \u21a9 http://crisp-dm.eu/home/about-crisp-dm/ \u21a9","title":"Pengantar Data Mining"},{"location":"#pengantar-data-mining","text":"","title":"Pengantar Data Mining"},{"location":"#tujuan-pembelajaran","text":"Tujuan Pembelajaran 1. Mahasiswa dapat memahami konsep data mining 2. Mahasiswa dapat mengetahui proses data mining 3. Mahasiswa dapat mengetahui teknik teknik yang digunakan dalam data mining.","title":"Tujuan Pembelajaran"},{"location":"#apa-itu-data-mining","text":"***The non-trivial extraction of implicit, previously unknown, and potentially useful information from data* 1 {silahkan dibaca dengan teliti} Dengan perkembangan jumlah data yang pesat disebabkan oleh adanya perkembangan teknologi informasi, sehingga hal ini yang memungkinkan data terkumpul dalam jumlah besar. Transaksi data yang dilakukan secara digital telah berkembang secara pesat diberbagai sektor bisnis. Perkembangan internet yang cukup cepat juga memiliki kontribusi dalam hal terciptanya data yang sangat besar. Fenomena ini secara signifikan berdampak pada terjadinya data transaksi yang sangat tinggi dari sisi volume dan jenis data telah dihasilkan. Bersamaan dengan kejadian ini memungkinkan terjadinya kondisi yang disebut dengan istilah kaya data tetapi miskin pengetahuan, disebabkan dengan adanya banyak data yang terkumpul tetapi sedikit dengan manfaat yang diperoleh dari besarnya data tersebut untuk kebutuhan bisnis dan lain sebgainya. Dengan data yang melimpah ini butuh suatu metode analisis data secara otomatis dari data yang berjumlah besar atau kompleks dengan tujuan untuk menemukan pola atau kecenderungan yang penting yang biasanya tidak disadari keberadaannya. Data mining merupakan merupakan subjek yang melibatkan berbagai disiplin bidang yang bertujuan bagaimana mendapatkan pengetahuan/informasi dari tumpukan data. Jika dianalogikan adalah seperti penambangan secara konvensianol misalkan menambang emas yang didapatkan dari tumpukan batu, tanah dan pasir (link video) . Jadi penekanan dari istilah penambangan adalah proses menemukan sesuatu yang berharga dari bahan-bahan mentah yang ada. Dalam kontek penambangan data adalah bagaimana mendapatkan informasi yang berharga untuk pijakan pengambilan keputusan atau tujuan tertentu. Dalama proses data mining melibatkan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar 2 . Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu **Knowledge discovery in databases (KDD) , ekstraksi pengetahuan (knowledge extraction), Analisa data/pola (data/pattern analysis), kecerdasan bisnis (business intelligence) dan data archaeology dan data dredging (Larose, 2005)","title":"Apa itu data mining"},{"location":"#keunggulan-data-mining","text":"Studi statistik tradisional menggunakan informasi masa lalu untuk menentukan keadaan sistem di masa depan (sering disebut prediksi), sedangkan studi penambangan data menggunakan informasi masa lalu untuk membangun pola yang berdasarkan tidak hanya pada input data tetapi juga pada konsekuensi logis dari data tersebut. Proses ini juga disebut prediksi, tetapi mengandung elemen penting yang tidak ditemukan dalam analisis statistik: kemampuan untuk memberikan ekspresi tentang yang mungkin terjadi di masa depan, dibandingkan dengan apa yang ada di masa lalu (berdasarkan asumsi dari metode statistik). Dibandingkan dengan studi statistik tradisional, yang sering melihat ke belakang, bidang data mining menemukan pola dan klasifikasi yang melihat dan bahkan memprediksi masa depan. Secara singkata Data mining dapat memberikan pemahaman data yang lebih lengkap dengan menemukan pola yang sebelumnya tidak terlihat dan Membuat model yang untuk memprediksi, sehingga memungkinkan orang untuk membuat keputusan yang lebih baik, sebagai acuan tindakan untuk yang akan datang dalam membuat keputusan","title":"Keunggulan data mining"},{"location":"#tugas-utama-data-mining","text":"Aktifitas utama dalam data mining adalah sebagai berikut 2 Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk","title":"Tugas utama data mining"},{"location":"#proses-data-mining","text":"Dalam data mining proses tahapan yang dilakukan sebagai Proses standar lintas industri untuk penambangan data yang dihasilkan oleh konsorsium , yang dikenal sebagai CRISP-DM, adalah model proses standar terbuka yang menggambarkan pendekatan umum yang digunakan oleh para ahli penambangan data. Model proses untuk penambangan data ini memberikan gambaran umum siklus hidup proyek penambangan data. Ini berisi fase proyek, tugas masing-masing dan hubungan antara aktifitas aktifitas yang dilakukan setiap tahapan seperti gambar berikut: Gambar 2.1 CRISP-DM 3","title":"Proses Data  mining"},{"location":"#memahami-bisnis-business-understanding","text":"","title":"Memahami Bisnis (Business Understanding )"},{"location":"#menentukan-tujuan-bisnis","text":"Tahapan ini menfokuskan pada pemahaman tujuan projek dan kebutuhan-kebutuhan yang diinginkan bisnis, kemudian merubahnya pengetahuan ini untuk mendefinisikan data mining dan rencana yang ingin dilakukan untuk mencapai tujuan bisnis. Tahaapan ini dilakukan untuk memahami secara menyeluruh, dari perspektif bisnis, apa yang benar-benar ingin dicapai oleh pelanggan. Selain itu pada tahapan ini analisa perlu dilakukan untuk mengungkap faktor-faktor penting, yang dapat mempengaruhi hasil proyek data mining.","title":"Menentukan Tujuan Bisnis"},{"location":"#membuat-rencana-proyek","text":"Menjelaskan rencana projek dilakuan untuk mencapai tujuan penambangan data sehingga dapat mencapai tujuan bisnis yang diinginkan. Rencana tersebut harus menentukan langkah-langkah yang harus dilakukan selama proyek , termasuk pemilihan tool dan teknik yang akan digunakan","title":"Membuat Rencana Proyek"},{"location":"#menilai-situasi-dan-kondisi","text":"Dalam hal pencarian fakta yang lebih rinci tentang semua sumber daya, kendala, asumsi, dan faktor-faktor lain yang harus dipertimbangkan dalam menentukan tujuan analisis data dan rencana proyek. Menilai sefara rinci kondisi lingkungan bisnis dan tujuan bisnis akan menentukan keberhasilan projek data mining Output dari ini adalah Inventaris sumber daya Daftar sumber daya yang tersedia untuk proyek, termasuk personil (pakar bisnis, pakar data, dukungan teknis, pakar penambangan data), data ( akses ke data langsung, gudang, atau operasional), sumber daya komputasi (platform perangkat keras), dan perangkat lunak (alat penambangan data, perangkat lunak lain yang relevan) Persyaratan, asumsi dan kendala Mendaftar semua persyaratan proyek, termasuk jadwal penyelesaian, kelengkapan dan kualitas hasil, dan keamanan, serta masalah hukum. Salah satu bagian dari tahapan ini akan memastikan apakah secara legal diizinkan menggunakannya data. Buat daftar asumsi-asumsi terkait dengan projek. Misalakan apakah apakah diasumsikan untuk memungkinkan memverifikasi selama penambangan data. Buat daftar kendala pada proyek. Ini mungkin merupakan kendala pada ketersediaan sumber daya, tetapi juga dapat mencakup kendala teknologi seperti ukuran data yang digunakan untuk pemodelan Mendaftar resiko yang akan dihadapi selama projek berlangsung (jika kemungkinan projek gagal)","title":"Menilai situasi dan kondisi"},{"location":"#memahami-data-data-understanding","text":"Tahapan memahami data dimulai dengan mengumpulkan data awal dan dilanjutkan dengan dengan kegiatan-kegiatan untuk mendapatkan data yang lazim serta identifikasi data yang berkualitas, pemahaman data sangat diperlukan untuk mendeteksi bagian yang menarik dari data sehingga dapat membangun hipotesa terhadap informasi yang tersembunyi","title":"Memahami data ( data understanding )"},{"location":"#a-mengumpulkan-data-awal","text":"Tugas Mendaftar data yang ada. Pengumpulan data diperlukan untuk memahami data. Misalkan, jika anda menggunakan tool khusus untuk memahami data, untuk menjadikan benar-benar memahami data maka data tersebut perlu diproses kedalam tool ini. Langkah ini terkait dengan langkah persiapan data. Jika anda membutuhkan berbagai sumber data, integrasi atau penyatuan data diperlukan Keluaran Daftar data yang di hasilkan dan dimana data tersebut berada, serta metode yang digunakan untuk mendapatkan data tersebut dan masalah-masalah dari data tersebut. Pada masa yang akan datang hasil dari tahapan ini sangat membantu jika kita melakukan data mining pada projek yang sama","title":"a. Mengumpulkan data awal"},{"location":"#b-mendiskripsikan-data","text":"Tugas Mendeskripsikan data. Mengamati secara kasar dan yang tampak dari data yang diperoleh dan mendokumentasikan deskripsi data tersebut Keluaran Report dari diskripsi data. Mendeskripsikan data yang didapat, diantaranya; format dari data, jumlah data, misalkan jumlah record dan field dari masing-masing tabel, identitas dari field-field (atribut-atribut) dan karakteristik yang tampak dari data yang sudah dikumpulkan. Apakah data memenuhi kebutuhan yang terkait","title":"b. Mendiskripsikan data"},{"location":"#c-ekplorasi-data-menyelidiki-data","text":"Tugas Melakukan pertanyaan data mining yang dapat dilakukan dengan menggunakan queri, visualisasi dan reporting. tugas prediksi, analisa statistic sederhana, hubungan antara atribut, tugas ini terkait dengan tujuan data mining, dan persiapan data lebih lanjut Keluaran Laporan data explorasi. Pada kegitatan ini adalah menemukan hipotesa awal dan pengaruhnya pada akhir projek. Keluaran dari kegiatan ini diantaranya adalah grafik, dan plot yang menentukan karakteristik data atau yang terkait dengan sebagian data untuk penyelidikan lebih lanjut","title":"c. Ekplorasi data /Menyelidiki data"},{"location":"#d-verifikasi-qualitas-data","text":"Tugas Menyelidiki qualitas data dilakukan dengan untuk mengetahui apakah data lengkap ( apakah mencakup semua kebutuhan data yang diperlukan?). Apakah data tersebut mengandung error dan jika ada error-error bagaimana data yang seharusnya ?Apakah ada missing value dalam data? Jika ada maka bagaimanamenyelesaikannya? Keluaran Laporan data yang berkualitas. Daftar dari verifikasi data yang berkualitas, jika ada masalah dengan kualitas ada, maka daftar penyelesaian yang memungkinkan untuk memperbaiki kualitas data. Penyelesaian dari qualitas data secara umum sangat bergantung pada data.","title":"d. Verifikasi qualitas data"},{"location":"#persiapan-data","text":"Tahap mempersiapkan data mencakup semua aktifitas untuk membangun dataset akhir (data yang digunakan untuk tool pemodelan) dari data mentah awal. Tugas persiapan data lebih memungkinkan untuk dilakukan beberapa kali dan tidak ada ketentuan. Tugas-tugasnya diantaranya adalah memilih table, record dan atribut juga tranformasi dan membersihkan data. Output dari persiapan data adalah data set. Data data ini akan digunakan untuk pemodelan atau tugas analisa utama dari projek. Selain itu deskripsi dari data yang akan digunakan untuk pemodelan atau pekerjaan analisa utama dari projek. Tugas-tugas dari persiapan data diantaranya adalah:","title":"Persiapan data"},{"location":"#a-memilih-data-select-data","text":"Tugas Menentukan data yang digunakan untuk analisa. Kriteria yang digunakan harus ada keterkaitan dengan tujuan data mining, kualitas data batasan-batasan teknis seperti batasan volume data tipe data. Perhatikan bahwa pemilihan data mencakup pemilihan atribut ( kolom-kolom ) dan juga pemilihan records (baris-baris) dalam tabel Keluaran Daftar data yang akan digunakan dan dikeluarkan serta alasan-alasan mengapa data digunakan atau dikeluarkan.","title":"a. Memilih data ( Select data)"},{"location":"#b-membersihkan-data-clean-data","text":"Tugas Pada tahapan ini adalah bagaimana meningkatkan kualitas data sesuai dengan teknik yang dipilih. Beberapa diantaranya adalah memilih sebagian data yang bersih dan menyisipkan data yang hilang dengan teknik menyisipkan data hilang menggunakan model yang baik. Keluaran Penjelasan atas keputusan dan tindakan apa yang diambil untuk menangani kualitas data serta serta dampak kemungkinan hasil analisis","title":"b. Membersihkan data ( Clean data)"},{"location":"#c-integrasi-data-integrate-data","text":"Tugas Pada tahapan ini dilakukan proses penggabungan dari beberapa informasi misalkan dalam bentuk beberapa tabel untuk membentuk inforasi baru yang merupakan gabungan dari beberapa tabel Keluaran Data gabungan yang terbentuk dari beberapa tabel","title":"c. Integrasi data ( integrate data)"},{"location":"#pemodelan","text":"Pada tahapan ini membangun suatu model dari data yang diperoleh dari langkah sebelumnya untuk menjawab pertanyaan kebutuhan bisnis dengan berbagai macam metode. Beberapa metode yang dapat digunakan adalah metode statistik, pembelajaran mesin, riset operasi dan sebagainya. Dalam melakukan pemodelan data beberapa hal yang dilakukan adalah memilih variabel model, menjalankan model, dan mendiagnosa. Penjelasan Data Mining adalah proses yang menggunakan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar (Turban dkk. 2005). Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu Knowledge discovery in databases (KDD), ekstraksi pengetahuan (knowledge extraction), Analisa data/pola (data/pattern analysis), kecerdasan bisnis (business intelligence) dan data archaeology dan data dredging (Larose, 2005)","title":"Pemodelan"},{"location":"#referensi","text":"Frawley, W., Piatetsky-Shapiro, G., Matheus, C., 1991. Knowledge discovery in databases\u2014an overview. KnowledgeDiscovery in Databases 1991 1\u201330. Reprinted in AI Magazine, Fall 1992 \u21a9 Hand, D., Mannila, H., Smyth, P., 2001. Principles of Data Mining. The MIT Press: A Bradford Book, Cambridge, MA/London \u21a9 \u21a9 http://crisp-dm.eu/home/about-crisp-dm/ \u21a9","title":"Referensi"},{"location":"Autokorelasi%20dan%20Analisa%20Deret%20Berkala/","text":"Autokorelasi dan Analisa Deret Berkala 1 \u00b6 http://www.real-statistics.com/time-series-analysis/stochastic-processes/partial-autocorrelation-function/ Deret berkala adalah pengamatan data terhadap waktu. Jika data pengamatan berkaitan dengan waktu, data deret berkala adala be-rautokorelasi Ye, Nong. Data mining: theories, algorithms, and examples . CRC press, 2013. \u21a9","title":"Autokorelasi dan Analisa Deret Berkala"},{"location":"Autokorelasi%20dan%20Analisa%20Deret%20Berkala/#autokorelasi-dan-analisa-deret-berkala1","text":"http://www.real-statistics.com/time-series-analysis/stochastic-processes/partial-autocorrelation-function/ Deret berkala adalah pengamatan data terhadap waktu. Jika data pengamatan berkaitan dengan waktu, data deret berkala adala be-rautokorelasi Ye, Nong. Data mining: theories, algorithms, and examples . CRC press, 2013. \u21a9","title":"Autokorelasi dan Analisa Deret Berkala1"},{"location":"Eksplorasi%20data-2/","text":"Sampel Mean Disumsikan bahwa setiap titk simbol x_i \\in D x_i \\in D dipetakan ke variabel x_i=X(x_i) x_i=X(x_i) . Data yang telah dipetakan x_1,x_2,....x_n x_1,x_2,....x_n adalah kemudian diasumsikan sampel acak IID dengan X X . Kita dapat menghitung sampel mean dengan menempatkan massa proabilitas dari $ \\frac {1}{n}$ pada setiap titik $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\sum _ { i = 1 } ^ { m } \\frac { n _ { i } } { n } e _ { i } = \\left( \\begin{array} { c } { n _ { 1 } / n } \\ { n _ { 2 } / n } \\ { \\vdots } \\ { n _ { m } / n } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\ { \\vdots } \\ { \\hat { p } _ { m } } \\end{array} \\right) = \\hat { p } $$ dimana n_i n_i adalah banyaknya kejadian dari nilai vektor e_i e_i dalam sampel, yang ekivalen dengan banyaknya kejadian dari simbol a_i a_i . Selanjutnya, kita memiliki \\sum_{i=1}^m n_i=n \\sum_{i=1}^m n_i=n , yang mengikuti dari fakta bahwa X X hanya dapat diperoleh pada m m yang berbeda e_i e_i dan perhitungan setiap nilai haru ditambahkan hingga ke ukuran sampel n n Contoh3.4. Sampel Mean . Perthatikan jumlah yang diamati untuk setiap nilai a_i a_i (e_i) (e_i) dari diskritisasi atribut sepal length dalam tabel 3.1. Karena jumlah sampel adalah n=150 n=150 , dari sini kita dapat estimasi \\hat p_i \\hat p_i sebagai berikut $$ \\left. \\begin{array} { l } { \\hat { p } _ { 1 } = 45 / 150 = 0.3 } \\ { \\hat { p } _ { 2 } = 50 / 150 = 0.333 } \\ { \\hat { p } _ { 3 } = 43 / 150 = 0.287 } \\ { \\hat { p } _ { 4 } = 12 / 150 = 0.08 } \\end{array} \\right. $$ Fungsi Massa Probabilias diplot dalam gambar 3.1 dan sample mean untuk X dinyatakan dengan $$ \\hat { \\mu } = \\hat { p } = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) $$ Matrik Covarian Perhatikan lagi bahwa m m -dimensi variabel multivariate Bernouli adalah sederhananya vektor dari m m variabel Bernoulli. Misalkan X=(A_1,A_2,...A_m)^T X=(A_1,A_2,...A_m)^T dimana A_i A_i adalah variabel Bernoulli yang terkait dengan simbol a_i a_i . Informasi variansi covarian antara unsur-unsur variabel Bernoully yang menghasilkan matrik untuk X X Marilah kita pertama kita perhatikan variansi dari setiap variabel Bernoulli A_i A_i . Dengan persamaan (3.1),kita segera memiliki $$ \\sigma _ { i } ^ { 2 } = \\operatorname { var } ( A _ { i } ) = p _ { i } ( 1 - p _ { i } ) $$ Berikutnya perhatikan covariasi antara A_1 A_1 dan A_j A_j . Dengan memanfaatkan identitas (2.21) kita miliki $$ \\sigma _ { i j } = E [ A _ { i } A _ { j } ] - E [ A _ { i } ] \\cdot E [ A _ { j } ] = 0 - p _ { i } p _ { j } = - p _ { i } p _ { j } $$ yang mengikuti dari fakta bahwa E[A_iA_j]=0 E[A_iA_j]=0 sehingga A_1 A_1 dan A_2 A_2 keduanya tidak sama dengan 1 dan kemudian perkalian A_iA_j=0 A_iA_j=0 . Fakta yang sama ini terkait dengan relasi negatif antara A_i A_i dan A_j A_j . Yang menarik adalah bahwa derajat keterkaitan negatif adalah proporsional pada perkalian dari nilai mean A_i A_i dan A_j A_j . Sari esperesi sebelumnya untuk varian dan covarian, m\\times m m\\times m","title":"Eksplorasi data 2"},{"location":"Eksplorasi%20data/","text":"Ekplorasi data \u00b6 Atribut Data numerik \u00b6 Dalam bab ini, kita membahas metode statistik dasar untuk analisis ekploarasi data atribut numerik. Kita membahas ukuran kecenderungan pusat (central tendency), ukuran dispersi atau sebaran, dan ukuran ketergantungan linier atau hubungan antara atribut. Kita menekankan hubungan antara probabilistik dan geometris dan aljabar dari sudut pandang data matriks Analisa univariat \u00b6 Analisis univariat dilakukan pada atribut tunggal ( X X ); dengan demikian matriks data D bisa dianggap sebagai matriks n \u00d7 1 n \u00d7 1 , atau sebagai vektor kolom, yang dianyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ \\hline x _ { 1 } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana X X adalah atribut numerik yang dimaksudkan, dengan $ x _ { i } \\in \\mathbb{R} $. X X diasumsikan adalah variabel random, dengan setiap titik $ x _ { i } ( 1 \\leq i \\leq n ) $ , merupakan variabel acak. Kita asumsikan bawa data pengamatan adalah. Kami berasumsi bahwa data yang diamati adalah sampel acak yang diambil dari X X , artinya, setiap variabel x_i x_i adalah saling bebas dan berdistribus sama (iid). Dalam sudut pandang vektor, kami memperlakukan sampel sebagai vektor n-dimensi, dan menulis $ X \\in \\mathbb R ^ { n } $ Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif $ F(x),$ untuk atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi. Fungsi distribusi Kumulatif Empiris \u00b6 Fungsi distribusi kumulatif empiris (CDF ) dari X X dinyatakan dengan $$ \\hat { F } ( x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } \\leq x ) $$ dimana $$ I ( x _ { i } \\leq x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } \\leq x } \\ { 0 } & { \\text { if } x _ { i } > x } \\end{array} \\right. $$ adalah variabel indikator biner yang menyatakan variabel indikator biner yang menunjukkan apakah kondisi yang diberikan terpenuhi atau tidak. Fungsi distribusi kumulatif Invers \u00b6 Definisi fungsi distribusi kumulatif invers atau fungsi quantile untuk variabel acak X X sebagai berikut : $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] $$ Fungsi distribusi kumulatif Invers empiris dapat diperoleh dari persamaan (2) Fungsi massa Probabilitas Empiris \u00b6 Fungsi massa probabilitas empiris dari X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ dimana $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } = x } \\ { 0 } & { \\text { if } x _ { i } \\neq x } \\end{array} \\right. $$ Fungsi massa probabilitas empiris juga menempatkan massa probabitas \\frac {1}{n} \\frac {1}{n} pada setipa titik x_i x_i Mengukur kecenduran terpusat \u00b6 Ukuran ini memberikan indikasi tentang konsentrasi massa probabilitas , nilai tengah dan lainnya. Mean \u00b6 Mean juga disebut dengan nilai harapan dari variabel acak X X adalah rata rata aritmetika dari nilai X X . Itu merupakan salah satu dari kecenderungan terpusat dari X X . Mean atau nilai harapan dari variabel acak X X didefinisikan dengan $$ \\mu = E [ X ] = \\sum _ { x } x f ( x ) $$ diman f(x) f(x) adalah fungsi massa probabilitas dari X X . Nilai harapan dari variabel acak kontinu X X dinyakan dengan $$ \\mu = E [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d x $$ dimana f(x) f(x) adalah fungsi padat probabilitas dari X X . Sample Mean . Sample mean adalah statistik, yaitu fungsi $ \\hat { \\mu } : { x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } } \\rightarrow \\mathbb R$, didefinisikan sebagai nilai rata-rata dari x_i x_i : $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ nilai adalah sebagai pengestimasi nilai mean yang tidak diketahui dari X X . Nilai tersebut diperoleh dengan memasukkan dalam fungsi massa probabilitas empiris dalam persamaan (7) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Sample mean adalah tidak bias . Estimator \\hat { \\theta } \\hat { \\theta } disebut dengan unbiased estimatore (stimator tidak bias) untuk parameter \\theta \\theta jika E[\\hat \\theta] = \\theta E[\\hat \\theta] = \\theta untuk setiap kemungkinan nilai dari \\theta \\theta . Sample mean \\hat \\mu \\hat \\mu adalah unbiased estimator untuk mean populasi \\mu \\mu sehingga $$ E [ \\hat { \\mu } ] = E [ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mu = \\mu $$ dimana kita gunakan fakta bahwa variabel acak x_i x_i adalah IID sesuai dengan X X , yang berarti bahwa mereka memiliki rata-rata \\mu \\mu yang sama seperti X X , yaitu,$ E [x_i] =\\mu$ untuk semua x_i x_i . Kita juga menggunakan fakta bahwa fungsi ekpektasi E E adalah linier operator yaitu untuk suatu dua bilangan acak X X dan Y Y dan bilangan real a a dan b b , kita memiliki E [ a X + b Y ] = a E [ X ] + b E [ Y ] E [ a X + b Y ] = a E [ X ] + b E [ Y ] Robustnes Kita mengatakan bahwa statistik adalah robust jika tidak dipengaruhi oleh suatu nilai ekstrim ( misal outlier/pencilan) dalam data. Rata-rata sampel sayangnya tidak kuat karena ada satu nilai besar (outlier) dapat mejadikan rata-rata yang tidak sebenarnya. Ukuran yang lebih robust adalah trimmed mean yang didapatkan setalah mengabaikan sebagian kecil dari nilai nilai ekstrim pada salah satu ujungnya. Median Median dari suatu variabel acak didefinisikan dengan nilai m m sehingga $$ P ( X \\leq m ) \\geq \\frac { 1 } { 2 } \\text { and } P ( X \\geq m ) \\geq \\frac { 1 } { 2 } $$ Degan kata lain, median m m adalah nilai paling tengan (middle-most). Dalam istliah (invers) cumulatif distribution function , median m m dinyatakan dengan $$ F ( m ) = 0.5 \\text { or } m = F ^ { - 1 } ( 0.5 ) $$ Sample median dapat diperoleh dari Fungsi distribusi kumulatif invers atau fungsi distribusi kumulatif invers empiris dengan dihitung $$ \\hat { F } ( m ) = 0.5 \\text { atau } m = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Pendekatan paling sederhan untuk menghitung sample median adalah pertama kai dari mengurutkan semua nilai x_i x_i (i \\in [1,n]) (i \\in [1,n]) dengan urutan naik. Jika n n adalah ganjil , media adalah nilai pada posisi \\frac {n+1}{2} \\frac {n+1}{2} . Jika n n adalah genap, nilai padan posisi \\frac {n}{2} \\frac {n}{2} dan \\frac {n}{2}+1 \\frac {n}{2}+1 adalah keduanaya median. Tidak seperti mean, media adalah robust, sehingga ia tidak dipengaruhi oleh banyak nilai extrim. Juga nilai tersebut terjadi dalam sample dan nilai yang bisa diasumsikan oleh variabel acak. Mode Nilai mode dari variabel acak adalah nilai dimana fungsi massa probabilitas atau fungsi padat probabilitas mencapai nilai maximumnya, bergantung pada apakah X X adalah diskrit atau kontinu. Sample mode adalah nila untuk fungsi probabilitas empiris mencapai nilai maksimum, dinyatakan dengan $$ mode(X) =\\arg \\underset{x}{max} {\\hat f(x)} $$ Mode ini mungkin bukan ukuran kecenderungan sentral yang sangat berguna untuk sampel, karena kemungkinan elemen yang tidak representatif menjadi elemen yang paling sering muncul. Selanjutnya, jika semua nilai dalam sampel berbeda, maka masing-masing akan menjadi mode Contoh . (Sample Mean, Median, dan Mode) . Perhatikan atribut sepal length (Xi) (Xi) dalam data iris. Data iris, dimana nilainya seperti yang ditunjukkan dalam tebel 1.2 . Sample mean dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { 150 } ( 5.9 + 6.9 + \\cdots + 7.7 + 5.1 ) = \\frac { 876.5 } { 150 } = 5.843 $$ Gambar 2.1 menunjukkan semua dari 150 nilai sepal length dan sample mean. Gambar 2.2a menunjukkan fungsi distribusi kumulatif empiri dan gambar 2.2b menunjukkan fungsi distribusi kumulatif empiris untuk sepal length Karena n=150 n=150 adalah genap, sample median adalah nilai pada posisi \\frac {n}{2}=75 \\frac {n}{2}=75 dan \\frac {n}{2}+1=76 \\frac {n}{2}+1=76 setelah diurutkan. Untuk sepal length kedua nilainya adalah 5.8, kemudian sample media adalah 5.8 . Dari fungsi distribusi kumulatif invers dalam gambar 2.2b, kita dapat melihat bahwa $$ \\hat { F } ( 5.8 ) = 0.5 \\text { or } 5.8 = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Sample mode untuk sepal length adalah 5. yang dapat dilihat dari frequency dari 5 dalam gambar 2.1. Massa probabilitas empiris pada x=5 x=5 adalah $$ \\hat { f } ( 5 ) = \\frac { 10 } { 150 } = 0.067 $$ Mengukur sebaran (dispersion) \u00b6 Mengukur dispersi memberikan indikasi tentang sebaran atau variasi pada nilai nilai variabel acak. Jangkauan Jangkauan nilai atau secara sederhana jangkauan (range) variabel acak X X adalah perbedaan antara nilai maximum dan nilai minimum dari X X dinyatakan dengan $$ r = \\operatorname { max } { X } - \\operatorname { min } { X } $$ Sample range adalah statistik, dinyatakan dengan $$ \\ \\hat r = {\\overset{n}{\\underset{i=1}{max }}} {{{x_i}}}-{\\overset{n}{\\underset{i=1}{min }}} {{{x_i}}} $$ Dengan definisi, jangkauan adalah sensitif terhadap nilai extrime sehingga tidak robust. Jangkauan antar interquartile Quartile adalah nilai khusus dari fungsi quantile persaman (2.2) yang membagi data kedalam empat bagian. Furthermore quartile terkati dengan nilai-nilai quantile 0.25, 0.5, dan 0.74 dan 1.0. Quantile pertama adalah nilai q_1 =F^{-1}(0.25) q_1 =F^{-1}(0.25) 25% dari sebelah kiri rentang titik, kuartile ke dua adalah sama dengan nilai median q_2 =F^{-1}(0.5) q_2 =F^{-1}(0.5) , 50 % dari sebelah kiri data dan q_3=F^{-1}(0.75) q_3=F^{-1}(0.75) adalah nilai 75% dari sebelah kiri dan quantile ke empat adalah nilai maximum dari X X , 100 % sebelah kiri dari rentang data. Ukuran yang lebih robust dari seberan X X adalah jangkauan interquartile (IQR) dinyatakan dengan $$ I Q R = q _ { 3 } - q _ { 1 } = F ^ { - 1 } ( 0.75 ) - F ^ { - 1 } ( 0.25 ) $$ Variansi dan standar deviasi Variansi dari variabel acak X X memberikan pengukuran berapa banyak nilai nilai dari penyimpangan X X dari rata-rata atau nilai harapan dari X X . Lebih tepatnya variansi adalah nilai harapan dari penyimpangan dari mean yang dikuadratkan yang didefinisikan dengan $$ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ ( X - \\mu ) ^ { 2 } ] = \\left{ \\begin{array} { l l } { \\sum _ { x } ( x - \\mu ) ^ { 2 } f ( x ) } & { \\text {jika } X \\text { adalah diskrit } } \\ { \\int _ { - \\infty } ^ { \\infty } ( x - \\mu ) ^ { 2 } f ( x ) d x } & { \\text { jika } X \\text { adalah kontinu } } \\end{array} \\right. $$ Standar deviasi \\sigma \\sigma didefinisikan sebagai akar kuadrat positif dari variansi \\sigma^2 \\sigma^2 . Kita dapat juga menulis variansi sebagai selisih antara ekpektasi X^2 X^2 dan akar dari ekpektasi X X : Variansi Sampel Variansi sampel didefinisikan dengan $$ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } $$ Standar deviasi adalah akar dari variansi sample yang dinyatakan dengan $$ \\hat { \\sigma } = \\sqrt { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } } $$ Analisa Bivariate \u00b6 Dalam analisa bivariate, kita memandang dua atribut pada waktu yang sama. Kita fokus untuk memahami keterkaitan atau kebergantunga antara dua variabel atau atribut tersebut, jika ada. Kita lalu membatasi pada dua variabel X_1 X_1 dan X_2 X_2 , dengan D D dinyatakan sebagai matrik dengan ukuran $ n \\times 2$ $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Secara geometri, kita dapat memandang D D dalam dua cara. Itu dapat dianggap sebagai n n titik atau vektor dalam 2-ruang dimensi terhadap atribut X_1 X_1 dan X_2 X_2 yaitu x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 .Selain itu dapat dilihat sebagai 2 titik atau vektor dalam n n -ruang dimensi yang berisi titik, yaitu setiap kolom adalah vektor dalam $ \\mathbb R^n$ sebagai berikut : $$ \\left. \\begin{array} { l } { X _ { 1 } = ( x _ { 11 } , x _ { 21 } , \\ldots , x _ { n 1 } ) ^ { T } } \\ { X _ { 2 } = ( x _ { 12 } , x _ { 22 } , \\ldots , x _ { n 2 } ) ^ { T } } \\end{array} \\right. $$ Dalam sudut pandang probabilistik, vektor kolom X=(X_1,X_2)^T X=(X_1,X_2)^T dianggapa variabel acak bivariate dan titik titik x _ { i } ( 1 \\leq i \\leq n ) x _ { i } ( 1 \\leq i \\leq n ) dinyatakan sebagai sampel acak yang diperoleh dari X X , yaitu x_i x_i dianggap independent and identically distributed (iid) seperti X X . Fungsi Massa Probabilitas Gabungan Empiris Fungsi Massa Probabilitas Gabungan Empiris untuk X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) dimana I I adalah variabel indikator yang bernilai 1 jika argumen argumennya benar $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { i 1 } = x _ { 1 } \\text { dan } x _ { i 2 } = x _ { 2 } } \\ { 0 } & { \\text { untuk yang lainnya } } \\end{array} \\right. $$ Seperti dalam kasus univariate, fungsi probabilitas menempatkan massa probabilitas \\frac {1}{n} \\frac {1}{n} pada setiap objek dalam data sampel. Mengukur Dispersi \u00b6 Mean Rata rata bivariate didefinisikan sebagai nilai harapan dari variabel acak vektor X X , didefinisikan sebagai berikut : $$ \\mu = E [ X ] = E \\left[ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) \\right] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) $$ Dengan kata lain, rata-rata bivariate adalah nilai harapan dari masing masing atribut. Rata-rata sampel dapat diperoleh dari \\hat f_{x_1} \\hat f_{x_1} dan \\hat f_{x_2} \\hat f_{x_2} , fungsi massa probabilitas empiris dari X_1 X_1 dan X_2 X_2 , menggunakan persamaan (2.5). Dapat juga dihitung dari gabungan fungsi massa probabilitas empiris dalam persamaan (2.17) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x )\\right ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Variansi Kita dapat menghitung variansi masing masing atribut, yaitu \\sigma_1^2 \\sigma_1^2 untuk X_1 X_1 dan \\sigma_2^2 \\sigma_2^2 untuk X_2 X_2 mengggunkan persamaan (2.8). Variansi secara keseluruhan (1.4) dinyatakan dengan $$ var(D)=\\sigma_1^2 +\\sigma_2^2 $$ Variansi sampel \\hat \\sigma_1^2 + \\hat \\sigma_2^2 \\hat \\sigma_1^2 + \\hat \\sigma_2^2 dapat diestimasi dengan menggunakanpersamaan (2.10) dan jumlah variansi sample adalah \\sigma_1^2 +\\sigma_2^2 \\sigma_1^2 +\\sigma_2^2 2.2.2. Mengukur keterkaitan Covarian Kovarian antara dua atribut X_1 X_1 dan X_2 X_2 mengukur keterkaitan antara kebergantungan linier diantaranya dan didefinisikan dengan $$ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] $$ Dengan linieraritas dari harapan, kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] }\\{ = E [ X _ { 1 } X _ { 2 } - X _ { 1 } \\mu _ { 2 } - X _ { 2 } \\mu _ { 1 } + \\mu _ { 1 } \\mu _ { 2 } ] }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 2 } E [ X _ { 1 } ] - \\mu _ { 1 } E [ X _ { 2 } ] + \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] }\\end{array} \\right. $$ Persamaan (2.21) dapat dianggap sebagai generalisasi dari variansi univariate persamaan (2.9) pada kasus bivariate. Jika X_1 X_1 dan X_2 X_2 adalah variabel acak saling bebas, maka kita dapat simpulkan bahwa covariannya adalah nol. Ini karena jika X_1 X_1 dan X_2 X_2 adalah saling bebas, maka kita memiliki $$ E [ X _ { 1 } X _ { 2 } ] = E [ X _ { 1 } ] \\cdot E [ X _ { 2 } ] $$ yang pada akhirnya menyiratkan bahwa $$ \\sigma{12}= 0 $$ Namaun sebaliknya tidak benar. Yaitu jika \\sigma_{12}=0 \\sigma_{12}=0 , kita tidak dapat mengklaim bahwa $X_1 $ dan X_2 X_2 adalah saling bebas. Semuanya kita katakan bahwa tidak adalah kebergantung linier antara keduanya. Kovarian sampel antra X1 X1 dan X_2 X_2 dinyatakan dengan $$ \\hat { \\sigma } _ { 12 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) $$ Korelasi Korelasi antara variabel X_1 X_1 dan X_2 X_2 adalah standarisasi kovarian, yang didapatkan dengan menormalisasi kovarian dengan standar deviasi masing masing variabel dinyatakan dengan \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } Matrik Kovarian Variansi dari untuk dua atribut X_1 X_1 dan X_2 X_2 dapat diringkas dalam matrik covarianse bujursangkar denga ukuran $2 \\times 2 $ dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] }\\{ = E \\left[ \\left( \\begin{array} { c } { X _ { 1 } - \\mu _ { 1 } } \\ { X _ { 2 } - \\mu _ { 2 } } \\end{array} \\right) ( X _ { 1 } - \\mu _ { 1 } \\quad X _ { 2 } - \\mu _ { 2 } ) \\right ] }\\{ = \\left( \\begin{array} { c c } { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\ { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } \\end{array} \\right) }\\end{array} \\right. $$ Karena \\sigma_{12}=\\sigma_{21} \\sigma_{12}=\\sigma_{21} , $\\Sigma $ adalah matrik simetris. Matrik vovarian merekam variansi tertentu atribut pada diagonal utamanya, dan informasi covarian pada elemen element bukan diagonal. Total variance dari dua atribut dinyatakan sebagai jumlah elemen elemen diagonal dari $ \\Sigma $ , yang juga disebut trace dari $ \\Sigma $ dinyatakan dengan $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } $$ Kita segera memiliki $ tr(\\Sigma)\\geq 0$ Secara umum covarian adalah non-negatif, karena $$ | \\Sigma | = \\operatorname { det } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\sigma _ { 12 } ^ { 2 } = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\rho _ { 12 } ^ { 2 } \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } = ( 1 - \\rho _ { 12 } ^ { 2 } ) \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } $$ dimana kitu gunakan persamaan (2.23), yaiut \\rho_{12}\\sigma_1\\sigma_2 \\rho_{12}\\sigma_1\\sigma_2 . dengan |\\Sigma| |\\Sigma| adalah determinan dari matrik kovarian. Perhatikan bahwa |\\rho_{12}|\\leq 1 |\\rho_{12}|\\leq 1 menyebabkan \\rho_{12}^2 \\leq 1 \\rho_{12}^2 \\leq 1 sehingga det (\\Sigma) \\geq 1 (\\Sigma) \\geq 1 furthermore determinannya adalah non-negative. Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\sigma } _ { 1 } ^ { 2 } } & { \\hat { \\sigma } _ { 12 } } \\ { \\hat { \\sigma } _ { 12 } } & { \\hat { \\sigma } _ { 2 } ^ { 2 } } \\end{array} \\right) $$ Matrik kovarian sampe $ \\hat \\Sigma$ memilki karakteristik sama seperti \\Sigma \\Sigma , yaitu simetris dan |\\hat \\Sigma| \\geq 0 |\\hat \\Sigma| \\geq 0 dan itu dapat digunakan untum memudahkan mendapatkan total sampel dan variansi secara umum Contoh (Rata rata Sampel dan Covarian) Perhatikan atribut sepal length dan sepal width untuk data iris, seperti yang diplot dalam gambar 2.4. Ada n=150 data dalam d=2 d=2 ruang dimensi. Rata rata sampel adalah $$ \\hat { \\mu } = \\left( \\begin{array} { l } { 5.843 } \\ { 3.054 } \\end{array} \\right) $$ Matrik covarian dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.681 } & { - 0.039 } \\ { - 0.039 } & { 0.187 } \\end{array} \\right) $$ Variansi untuk sepal length adalah \\hat \\sigma_1^2=0.681 \\hat \\sigma_1^2=0.681 dan sepal width adalah \\hat \\sigma_2^2=0.187 \\hat \\sigma_2^2=0.187 . Covarian antara dua atribut adalah \\hat \\sigma_{12}=-0.039 \\hat \\sigma_{12}=-0.039 dan korelasi antara dua atribut tersebut adalah $$ \\hat { \\rho } _ { 12 } = \\frac { - 0.039 } { \\sqrt { 0.681 \\cdot 0.187 } } = - 0.109 $$ Lalu, ada korelasi yang sangat lemah antara dua atribut tersebut Total variansi sampel dinyatakan dengan $$ \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 = 0.868 $$ dan variansi secara umum dinyatakan dengan $$ \\hat { \\Sigma } | = \\operatorname { det } ( \\hat { \\Sigma } ) = 0.681 \\cdot 0.187 - ( - 0.039 ) ^ { 2 } = 0.126 $$ Analisa Multivariate \u00b6 Dalam analisa multivariate, kita melihat atribut numerik dengan d d dimensi X_1,X_2,...X_d X_1,X_2,...X_d . Data dinyatakan degan matrik n\\times d n\\times d seperti berikut $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Jika dilihat dari baris data memiliki n n objek atatu vektor dalam d d ruang dimensi atribut $$ x _ { i } = ( x _ { i 1 } , x _ { i 2 } , \\ldots , x _ { i d } ) ^ { T } \\in \\mathbb R ^ { d } $$ Jika dilihat dari sudut pandang kolom, data diangga sebagai d d objek atau vektor dalam n n dimensi ruang dengan titik-titik data $$ X _ { j } = ( x _ { 1 j } , x _ { 2 j } , \\ldots , x _ { n j } ) ^ { T } \\in R ^ { n } $$ Jika dilihat dari sudut pandang probabilitas, d d atribut dimodelkan dengan variabel acak vektor X=(X_1,X_2,...X_d)^T X=(X_1,X_2,...X_d)^T dan titik titik x_i x_i dianggap sebagai sampel acak yang diperoleh dari X X , atribut atribut tersebut independent and identfically distributed dari X X (i.i.d X X ) Mean Generalisasi persamaan (2.18) rata-rata vektor multivariate diperoleh dari masing-masing atribut yang dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\ { \\vdots } \\ { E [ X _ { d } ] } \\end{array} \\right) = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) $$ Generalisasi persamaan (2.19) rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Matrik Kovarian Generalisasi persamaan (2.26) untuk d d dimensi, kovarian multicovariate di dinyatakan dengan matrik kovarian simetris $ d\\times d $yang menyatakan kovarian untuk setiap pasangan atribut $$ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\cdots } & { \\sigma _ { 1 d } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\cdots } & { \\sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\cdots } & { \\cdots } \\ { \\sigma _ { d 1 } } & { \\sigma _ { d 2 } } & { \\cdots } & { \\sigma _ { d } ^ { 2 } } \\end{array} \\right) $$ Elemen diagonal $\\sigma_i^2 $ menyatakan variansi atribut X_i X_i , dimana elemen-elemen bukan diagonal \\sigma_{ij} = \\sigma_{ji} \\sigma_{ij} = \\sigma_{ji} menyatakan kovarian antara atribut pasangan X_i X_i dan X_j X_j . Matrik kovarian adalah positif semidefinite Contoh Rata-rata sample dan matrik covarian. Perhatikan semua atribut numerik untuk data iris, namanya sepal length, petal length, dan petal width. Rata rata multivarean dinyatakan dengan \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 Contoh Perkalian dalam dan perkalian luar . Untuk mengdeskripsikan komputasi perkalian dalam dan perkalian luar dari matrik covarian, perhatikan data 2-dimensi $$ D = \\left( \\begin{array} { l l } { A _ { 1 } } & { A _ { 2 } } \\ \\hline 1 & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) $$ Rata-rata vektor adalah sebagai berikut $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { 15 / 3 } \\ { 8.7 / 3 } \\end{array} \\right) = \\left( \\begin{array} { c } { 5 } \\ { 2.9 } \\end{array} \\right) $$ dan matrik data terpusat dinyatakan $$ Z = D - 1 \\cdot \\mu ^ { T } = \\left( \\begin{array} { l l } { 1 } & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) - \\left( \\begin{array} { l } { 1 } \\ { 1 } \\ { 1 } \\end{array} \\right) \\left( \\begin{array} { l l } { 5 } & { 2.9 } \\end{array} \\right) = \\left( \\begin{array} { r r } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) $$ Pendekatan perkalian dalam [pers. 2.30] untuk menghitung matrik kovarian adalah $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\frac { 1 } { n } Z ^ { T } Z = \\frac { 1 } { 3 } \\left( \\begin{array} { c c c } { - 4 } & { 0 } & { 4 } \\ { - 2.1 } & { - 0.5 } & { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) }\\{ = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32 } & { 18.8 } \\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\ { 6.27 } & { 3.81 } \\end{array} \\right) }\\end{array} \\right. $$ Pendekatan lain yaitu dengan perkalian luar [pers. 2.31] dibyatakan dengan $$ \\hat { \\Sigma } = \\frac { 1 } { n } \\sum _ { j = 1 } ^ { n } z _ { i } \\cdot z _ { i } ^ { T } $$ = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. dimana data terpusat z_i z_i adalah baris dari Z Z Atribut Kategorikal \u00b6 Kita asumsikan bahwa data terdiri dari satu atribut X X . Domain dari X X terdiri dari m m nilai simbolis dom(X)={a_1,a_2,...a_m} dom(X)={a_1,a_2,...a_m} . Data D D adalah n\\times 1 n\\times 1 matrik data simbolis yang dinyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ { x _ { 1 } } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana setiap nilai x_i \\in dom(X) x_i \\in dom(X) Variabel Bernouli \u00b6 Marilah kita lihat kasus ketika atribut kategorikal X X memililik domain $ {a_1,a_2}$ dengan m=2 m=2 . Kita dapat memodelkan X X sebagai variabel acak Bernouli, yang didasarkan pada dua nilai berbeda yaitu 1 dan 0, sesuai dengan pemetaan $$ X ( v ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } v = a _ { 1 } } \\ { 0 } & { \\text { if } v = a _ { 2 } } \\end{array} \\right. $$ Fungsi massa probabilitas (PMF) dari X X dinyatakan dengan $$ P ( X = x ) = f ( x ) = \\left{ \\begin{array} { l l } { p _ { 1 } } & { \\text { if } x = 1 } \\ { p _ { 0 } } & { \\text { if } x = 0 } \\end{array} \\right. $$ dimana p_1 p_1 dan p_0 p_0 adalah parameter distribusi, yang harus memenuhi kondisi $$ p_1+p_0=1 $$ Karena hanya ada satu parameter bebas, biasanya menotasikan p_1=p p_1=p maka p_0=1-p p_0=1-p . Fungsi Massa Probabilitas dari variabel acak Bernouli X X dapat kemudian ditulis dengan $$ P ( X = x ) = f ( x ) = p ^ { x } ( 1 - p ) ^ { 1 - x } $$ Kita dapat melihat bahwa P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p seperti yand diharapkan Mean dan Variansi Nilai harapan dari X X dinyatakan dengan $$ \\mu = E [ X ] = 1 \\cdot p + 0 \\cdot ( 1 - p ) = p $$ dan variansi dari X X dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ X ^ { 2 } ] - ( E [ X ] ) ^ { 2 } }\\ \\hspace{7mm}= ( 1 ^ { 2 } \\cdot p + 0 ^ { 2 } \\cdot ( 1 - p ) ) - p ^ { 2 } = p - p ^ { 2 } = p ( 1 - p ) \\\\end{array} \\right. $$ Rata-rata sampel dan Variansi Untuk mengestimasi parameter dari variabel Bernouli X X , kita asumsikan bahwa setiap simbol dipetakan ke nilai biner. Sehingga, sekumpulan nilai {x_1,x_2,...x_n} {x_1,x_2,...x_n} diasumsikan menjadi sampel acak yang diperoleh dari X X (yaitu setiap $ x_i$ adalah IID dengan X X . Rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { n _ { 1 } } { n } = \\hat { p } $$ dimana n_1 n_1 adalah banyaknya titik dengan x_1=1 x_1=1 dalam sampel acak (sama dengan banyak kejadian dari simbol a_1 a_1 ) Misal n_0=n-n_1 n_0=n-n_1 menyatakan banyak titik dengan x_i=0 x_i=0 dalam sampel acak. Variansi sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } }\\ \\hspace{7mm}{ = \\frac { n _ { 1 } } { n } ( 1 - \\hat { p } ) ^ { 2 } + \\frac { n - n _ { 1 } } { n } ( - \\hat { p } ) ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ^ { 2 } + ( 1 - \\hat { p } ) \\hat { p } ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ( 1 - \\hat { p } + \\hat { p } ) }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) }\\end{array} \\right. $$ Variansi sampel dapat juga diperoleh langsung dari persamaan(3.1) dengan mensubsitusikan \\hat p \\hat p untuk p p . Contoh Perhatikan atribut sepal length ( X X ) untuk dataset iris dalam tabel 1.1. Marilah kita definisikan bunga iris dengan Long jika bunga itu sepal length dalam range [7, \\infty ] [7, \\infty ] , dan short jika sepal length dalam range [-\\infty,7] [-\\infty,7] . Kemudian X_1 X_1 dapat dinyatakan dengan atribut kategorikan dengan domain {Long,Short}. Dari sampel yang diamati ukuran n=150 n=150 , kita menemukan 13 iris long. Rata-rata sampel dari X_1 X_1 adalah $$ \\hat { \\mu } = \\hat { p } = 13 / 150 = 0.087 $$ dan variansinya adalah $$ \\hat { \\sigma } ^ { 2 } = \\hat { p } ( 1 - \\hat { p } ) = 0.087 ( 1 - 0.087 ) = 0.087 \\cdot 0.913 = 0.079 $$ Ditribusi binomial : banyaknya kejadian Diberikan variabel Bernoulli X X , misal \\{x_1,x_2,...x_n\\} \\{x_1,x_2,...x_n\\} menyatakan sampel acak dari ukuran n n yang diperoleh dari X X . Misal N N adalah variabel acak yang menyatakan numlah kejadi dari simbol a_1 a_1 (nilai X=1 X=1 ) dalam sampe. N adalah distribusi binomial yang dinyatakan dengan $$ f ( N = n _ { 1 } | n , p ) = \\left( \\begin{array} { l } { n } \\ { n _ { 1 } } \\end{array} \\right) p ^ { n _ { 1 } } ( 1 - p ) ^ { n - n _ { 1 } } $$ Dalam kenyataannya, N N adalah jumlah dari n n variabel acak Bernoulli x_i x_i yang saling bebas dan (IID) dengan X X yaitu N=\\sum_{i=1}^n x_i N=\\sum_{i=1}^n x_i . Dengan liniearitas dari ekpektasi, mean atau jumlah harapan dari kejadian simbol a_i a_i dinyatakan dengan $$ \\mu _ { N } = E [ N ] = E \\left[ \\sum _ { i = 1 } ^ { n } x _ { i } \\right] = \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\sum _ { i = 1 } ^ { n } p = n p $$ Karena x_i x_i adalah semuanya saling bebas, variansi dari N N dinyatakan dengan $$ \\sigma _ { N } ^ { 2 } = \\operatorname { var } ( N ) = \\sum _ { i = 1 } ^ { n } \\operatorname { var } ( x _ { i } ) = \\sum _ { i = 1 } ^ { n } p ( 1 - p ) = n p ( 1 - p ) $$ Contoh 3.2. Dengan meneruskan contoh 3.1, kita dapat menggunakan parameter yang telah diestimasi \\hat p=0.087 \\hat p=0.087 untuk menghitung banyaknya kejadian yang diharapkan N long dari sepal length. distribusi binomial Iris $$ E [ N ] = n \\hat { p } = 150 \\cdot 0.087 = 13 $$ Dalam kasus ini, karena p p dihitung dari sample melalui \\hat p \\hat p , tidak mengherankan bahwa jumlah kejadian diharapkan dari Long Iris sama dengan kejadian yang sebenarnya. Akan tetapi yang lebih menarik adalah kita dapat menghitung variansi jumlah kejadian $$ \\operatorname { var } ( N ) = n \\hat { p } ( 1 - \\hat { p } ) = 150 \\cdot 0.079 = 11.9 $$ Meningkatnya ukuran sample, distribusi binomial seperti yang diberikan dapalam persamaan 3.3 cenderung ke distribusi normal dengan \\mu=13 \\mu=13 dan \\sigma=\\sqrt{11.9}=3.45 \\sigma=\\sqrt{11.9}=3.45 . Sehingga dengan kepercaan lebih besar dari 95%, kita dapat mengklam bahwa jumlah kejadian dari a_i a_i akan terletak dalam rentang \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] yang mengikuti dari fakta bahwa untuk distribusi normal 95,45% dari massa probabilitas terletak dalam dua standar deviasi dari rata-rata. Variable multivariate Bernoulli \u00b6 Sekarang kita memandang kasus umum ketika X X adalah atribut kategorical dengan domain \\{a_1,a_2,...a_m\\} \\{a_1,a_2,...a_m\\} . Kita dapat memodelkan X X sebagai variabel acak Bernoulli m m -dimensi X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } dimana setiap A_i A_i adalah variabel Bernoulli dengan parameter p_i p_i yang menotasikan probabilitas dari pengamatan simbol a_i a_i . Akan tetapi karena X X dapat mengasumsikan hanya satu dari nilai simbolik pada suatu waktum jika X=a_i X=a_i maka A_i=1 A_i=1 dan A_j=0 A_j=0 untuk semua j \\neq i j \\neq i . Variabel acak X \\in {0,1}^m X \\in {0,1}^m , dan jika X=a_i X=a_i , maka X=e_i X=e_i , dimana e_i e_i adalah standar vektor basis ke i, e_i\\in\\mathbb R^m e_i\\in\\mathbb R^m yang dinyatakan dengan $$ e _ { i } = ( \\overbrace { 0 , \\ldots , 0 } ^ { i - 1 } , 1 , \\overbrace { 0 , \\ldots , 0 } ^ { m - i } ) ^ { T } $$ Pada e_i e_i hanya elemen ke i adalah 1 ( e_{ii}=1 e_{ii}=1 ) , sedangkan semua elemen yang lain adalah nol, ( e_{ij}=0, \\forall j \\neq i e_{ij}=0, \\forall j \\neq i ). Disini, definis yang lebih tepat dari variabel Bernoulli multivariate , yaitu generalisasi dari variabel Bernoullii dari dua hasil ke m m hasil. Kita kemudian memodelkan atribut kategorical X X sebagai variabel Bernoulli multivariate X X didefinisikan dengan $$ X ( v ) = e _ { i } \\text { if } v = a _ { i } $$ Rentang dari X X terdiri dari m m nilai vektor berbeda \\{e_1,e_2,...e_m\\} \\{e_1,e_2,...e_m\\} dengan fungsi massa probabilitas dari X X dinyatakan dengan $$ P ( X = e _ { i } ) = f ( e _ { i } ) = p _ { i } $$ dimana p_i p_i adalah probabilitas dari nilai pengamatan a_i a_i . Parameter ini harus memenuhi kondisi $$ \\sum _ { i = 1 } ^ { m } p _ { i } = 1 $$ Fungsi massa prababilitas dapat ditulis secara utuh sebagai berikut $$ P ( X = e _ { i } ) = f ( e _ { i } ) = \\prod _ { j = 1 } ^ { m } p _ { j } ^ { e _ { i j } }Ka $$ Kareana e_ii=1 e_ii=1 dan e_ij=0 e_ij=0 funtuk $ j\\neq i$, kita dapat melihat bahwa, seperti yang diharapkan, kita miliki $$ f ( e _ { i } ) = \\prod _ { j=1 } ^ { m } p _ { j } ^ { e _ { i j } } = p _ { 1 } ^ { e _ { i 0 } } \\times \\cdots p _ { i } ^ { e _ { i i } } \\cdots \\times p _ { m } ^ { e _ { i m } } = p _ { 1 } ^ { 0 } \\times \\cdots p _ { i } ^ { 1 } \\cdots \\times p _ { m } ^ { 0 } = p _ { i } $$ \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. Contoh : Marilah kita lihat atribut sepal length ( X_1 X_1 ) untuk data Iris seperti yang ditunjukkan dalam tabel 1.2. Kita membagi sepal length kedalam empat interval yang sama, dan memberikan nama untuk setiap interval seperti yang diunjukkan dalam tabel 3.1. Kita lihat X_1 X_1 sebagai atribut kategorical dengan domain $$ {a _ { 2 } = \\text { VeryShort, } a _ { 2 } = \\text { Short, } a _ { 3 } = \\operatorname { Long } , a _ { 4 } = \\operatorname{Very Long}} $$ Kita memodelkan atribut kategorical X_1 X_1 sebagai variabel X X Bernoulli multivariate, didefinisikan dengan $$ X ( v ) = \\left{ \\begin{array} { l l } { e _ { 1 } = ( 1,0,0,0 ) } & { \\text { jika } v = a _ { 1 } } \\ { e _ { 2 } = ( 0,1,0,0 ) } & { \\text { jika } v = a _ { 2 } } \\ { e _ { 3 } = ( 0,0,1,0 ) } & { \\text { jika } v = a _ { 3 } } \\ { e _ { 4 } = ( 0,0,0,1 ) } & { \\text { jika } v = a _ { 4 } } \\end{array} \\right. $$ Misalkan, simbol x_1=Short=a_2 x_1=Short=a_2 dinyatakan dengan (0,1,0,0)^T=e_2 (0,1,0,0)^T=e_2 Mean Mean atau nilai harapan dari X X dapat diperoleh dengan $$ \\mu = E [ X ] = \\sum _ { i = 1 } ^ { m } e _ { i } f ( e _ { i } ) = \\sum _ { i = 1 } ^ { m } e _ { i } p _ { i } = \\left( \\begin{array} { l } { 1 } \\ { 0 } \\ { \\vdots } \\ { 0 } \\end{array} \\right) p _ { 1 } + \\cdots + \\left( \\begin{array} { l } { 0 } \\ { 0 } \\ { \\vdots } \\ { 1 } \\end{array} \\right) p _ { m } = \\left( \\begin{array} { c } { p _ { 1 } } \\ { p _ { 2 } } \\ { \\vdots } \\ { p _ { m } } \\end{array} \\right) = p $$","title":"Ekplorasi data"},{"location":"Eksplorasi%20data/#ekplorasi-data","text":"","title":"Ekplorasi data"},{"location":"Eksplorasi%20data/#atribut-data-numerik","text":"Dalam bab ini, kita membahas metode statistik dasar untuk analisis ekploarasi data atribut numerik. Kita membahas ukuran kecenderungan pusat (central tendency), ukuran dispersi atau sebaran, dan ukuran ketergantungan linier atau hubungan antara atribut. Kita menekankan hubungan antara probabilistik dan geometris dan aljabar dari sudut pandang data matriks","title":"Atribut   Data numerik"},{"location":"Eksplorasi%20data/#analisa-univariat","text":"Analisis univariat dilakukan pada atribut tunggal ( X X ); dengan demikian matriks data D bisa dianggap sebagai matriks n \u00d7 1 n \u00d7 1 , atau sebagai vektor kolom, yang dianyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ \\hline x _ { 1 } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana X X adalah atribut numerik yang dimaksudkan, dengan $ x _ { i } \\in \\mathbb{R} $. X X diasumsikan adalah variabel random, dengan setiap titik $ x _ { i } ( 1 \\leq i \\leq n ) $ , merupakan variabel acak. Kita asumsikan bawa data pengamatan adalah. Kami berasumsi bahwa data yang diamati adalah sampel acak yang diambil dari X X , artinya, setiap variabel x_i x_i adalah saling bebas dan berdistribus sama (iid). Dalam sudut pandang vektor, kami memperlakukan sampel sebagai vektor n-dimensi, dan menulis $ X \\in \\mathbb R ^ { n } $ Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif $ F(x),$ untuk atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi.","title":"Analisa univariat"},{"location":"Eksplorasi%20data/#fungsi-distribusi-kumulatif-empiris","text":"Fungsi distribusi kumulatif empiris (CDF ) dari X X dinyatakan dengan $$ \\hat { F } ( x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } \\leq x ) $$ dimana $$ I ( x _ { i } \\leq x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } \\leq x } \\ { 0 } & { \\text { if } x _ { i } > x } \\end{array} \\right. $$ adalah variabel indikator biner yang menyatakan variabel indikator biner yang menunjukkan apakah kondisi yang diberikan terpenuhi atau tidak.","title":"Fungsi distribusi  Kumulatif Empiris"},{"location":"Eksplorasi%20data/#fungsi-distribusi-kumulatif-invers","text":"Definisi fungsi distribusi kumulatif invers atau fungsi quantile untuk variabel acak X X sebagai berikut : $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] $$ Fungsi distribusi kumulatif Invers empiris dapat diperoleh dari persamaan (2)","title":"Fungsi distribusi kumulatif Invers"},{"location":"Eksplorasi%20data/#fungsi-massa-probabilitas-empiris","text":"Fungsi massa probabilitas empiris dari X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ dimana $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } = x } \\ { 0 } & { \\text { if } x _ { i } \\neq x } \\end{array} \\right. $$ Fungsi massa probabilitas empiris juga menempatkan massa probabitas \\frac {1}{n} \\frac {1}{n} pada setipa titik x_i x_i","title":"Fungsi massa Probabilitas Empiris"},{"location":"Eksplorasi%20data/#mengukur-kecenduran-terpusat","text":"Ukuran ini memberikan indikasi tentang konsentrasi massa probabilitas , nilai tengah dan lainnya.","title":"Mengukur kecenduran terpusat"},{"location":"Eksplorasi%20data/#mean","text":"Mean juga disebut dengan nilai harapan dari variabel acak X X adalah rata rata aritmetika dari nilai X X . Itu merupakan salah satu dari kecenderungan terpusat dari X X . Mean atau nilai harapan dari variabel acak X X didefinisikan dengan $$ \\mu = E [ X ] = \\sum _ { x } x f ( x ) $$ diman f(x) f(x) adalah fungsi massa probabilitas dari X X . Nilai harapan dari variabel acak kontinu X X dinyakan dengan $$ \\mu = E [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d x $$ dimana f(x) f(x) adalah fungsi padat probabilitas dari X X . Sample Mean . Sample mean adalah statistik, yaitu fungsi $ \\hat { \\mu } : { x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } } \\rightarrow \\mathbb R$, didefinisikan sebagai nilai rata-rata dari x_i x_i : $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ nilai adalah sebagai pengestimasi nilai mean yang tidak diketahui dari X X . Nilai tersebut diperoleh dengan memasukkan dalam fungsi massa probabilitas empiris dalam persamaan (7) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Sample mean adalah tidak bias . Estimator \\hat { \\theta } \\hat { \\theta } disebut dengan unbiased estimatore (stimator tidak bias) untuk parameter \\theta \\theta jika E[\\hat \\theta] = \\theta E[\\hat \\theta] = \\theta untuk setiap kemungkinan nilai dari \\theta \\theta . Sample mean \\hat \\mu \\hat \\mu adalah unbiased estimator untuk mean populasi \\mu \\mu sehingga $$ E [ \\hat { \\mu } ] = E [ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mu = \\mu $$ dimana kita gunakan fakta bahwa variabel acak x_i x_i adalah IID sesuai dengan X X , yang berarti bahwa mereka memiliki rata-rata \\mu \\mu yang sama seperti X X , yaitu,$ E [x_i] =\\mu$ untuk semua x_i x_i . Kita juga menggunakan fakta bahwa fungsi ekpektasi E E adalah linier operator yaitu untuk suatu dua bilangan acak X X dan Y Y dan bilangan real a a dan b b , kita memiliki E [ a X + b Y ] = a E [ X ] + b E [ Y ] E [ a X + b Y ] = a E [ X ] + b E [ Y ] Robustnes Kita mengatakan bahwa statistik adalah robust jika tidak dipengaruhi oleh suatu nilai ekstrim ( misal outlier/pencilan) dalam data. Rata-rata sampel sayangnya tidak kuat karena ada satu nilai besar (outlier) dapat mejadikan rata-rata yang tidak sebenarnya. Ukuran yang lebih robust adalah trimmed mean yang didapatkan setalah mengabaikan sebagian kecil dari nilai nilai ekstrim pada salah satu ujungnya. Median Median dari suatu variabel acak didefinisikan dengan nilai m m sehingga $$ P ( X \\leq m ) \\geq \\frac { 1 } { 2 } \\text { and } P ( X \\geq m ) \\geq \\frac { 1 } { 2 } $$ Degan kata lain, median m m adalah nilai paling tengan (middle-most). Dalam istliah (invers) cumulatif distribution function , median m m dinyatakan dengan $$ F ( m ) = 0.5 \\text { or } m = F ^ { - 1 } ( 0.5 ) $$ Sample median dapat diperoleh dari Fungsi distribusi kumulatif invers atau fungsi distribusi kumulatif invers empiris dengan dihitung $$ \\hat { F } ( m ) = 0.5 \\text { atau } m = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Pendekatan paling sederhan untuk menghitung sample median adalah pertama kai dari mengurutkan semua nilai x_i x_i (i \\in [1,n]) (i \\in [1,n]) dengan urutan naik. Jika n n adalah ganjil , media adalah nilai pada posisi \\frac {n+1}{2} \\frac {n+1}{2} . Jika n n adalah genap, nilai padan posisi \\frac {n}{2} \\frac {n}{2} dan \\frac {n}{2}+1 \\frac {n}{2}+1 adalah keduanaya median. Tidak seperti mean, media adalah robust, sehingga ia tidak dipengaruhi oleh banyak nilai extrim. Juga nilai tersebut terjadi dalam sample dan nilai yang bisa diasumsikan oleh variabel acak. Mode Nilai mode dari variabel acak adalah nilai dimana fungsi massa probabilitas atau fungsi padat probabilitas mencapai nilai maximumnya, bergantung pada apakah X X adalah diskrit atau kontinu. Sample mode adalah nila untuk fungsi probabilitas empiris mencapai nilai maksimum, dinyatakan dengan $$ mode(X) =\\arg \\underset{x}{max} {\\hat f(x)} $$ Mode ini mungkin bukan ukuran kecenderungan sentral yang sangat berguna untuk sampel, karena kemungkinan elemen yang tidak representatif menjadi elemen yang paling sering muncul. Selanjutnya, jika semua nilai dalam sampel berbeda, maka masing-masing akan menjadi mode Contoh . (Sample Mean, Median, dan Mode) . Perhatikan atribut sepal length (Xi) (Xi) dalam data iris. Data iris, dimana nilainya seperti yang ditunjukkan dalam tebel 1.2 . Sample mean dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { 150 } ( 5.9 + 6.9 + \\cdots + 7.7 + 5.1 ) = \\frac { 876.5 } { 150 } = 5.843 $$ Gambar 2.1 menunjukkan semua dari 150 nilai sepal length dan sample mean. Gambar 2.2a menunjukkan fungsi distribusi kumulatif empiri dan gambar 2.2b menunjukkan fungsi distribusi kumulatif empiris untuk sepal length Karena n=150 n=150 adalah genap, sample median adalah nilai pada posisi \\frac {n}{2}=75 \\frac {n}{2}=75 dan \\frac {n}{2}+1=76 \\frac {n}{2}+1=76 setelah diurutkan. Untuk sepal length kedua nilainya adalah 5.8, kemudian sample media adalah 5.8 . Dari fungsi distribusi kumulatif invers dalam gambar 2.2b, kita dapat melihat bahwa $$ \\hat { F } ( 5.8 ) = 0.5 \\text { or } 5.8 = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Sample mode untuk sepal length adalah 5. yang dapat dilihat dari frequency dari 5 dalam gambar 2.1. Massa probabilitas empiris pada x=5 x=5 adalah $$ \\hat { f } ( 5 ) = \\frac { 10 } { 150 } = 0.067 $$","title":"Mean"},{"location":"Eksplorasi%20data/#mengukur-sebaran-dispersion","text":"Mengukur dispersi memberikan indikasi tentang sebaran atau variasi pada nilai nilai variabel acak. Jangkauan Jangkauan nilai atau secara sederhana jangkauan (range) variabel acak X X adalah perbedaan antara nilai maximum dan nilai minimum dari X X dinyatakan dengan $$ r = \\operatorname { max } { X } - \\operatorname { min } { X } $$ Sample range adalah statistik, dinyatakan dengan $$ \\ \\hat r = {\\overset{n}{\\underset{i=1}{max }}} {{{x_i}}}-{\\overset{n}{\\underset{i=1}{min }}} {{{x_i}}} $$ Dengan definisi, jangkauan adalah sensitif terhadap nilai extrime sehingga tidak robust. Jangkauan antar interquartile Quartile adalah nilai khusus dari fungsi quantile persaman (2.2) yang membagi data kedalam empat bagian. Furthermore quartile terkati dengan nilai-nilai quantile 0.25, 0.5, dan 0.74 dan 1.0. Quantile pertama adalah nilai q_1 =F^{-1}(0.25) q_1 =F^{-1}(0.25) 25% dari sebelah kiri rentang titik, kuartile ke dua adalah sama dengan nilai median q_2 =F^{-1}(0.5) q_2 =F^{-1}(0.5) , 50 % dari sebelah kiri data dan q_3=F^{-1}(0.75) q_3=F^{-1}(0.75) adalah nilai 75% dari sebelah kiri dan quantile ke empat adalah nilai maximum dari X X , 100 % sebelah kiri dari rentang data. Ukuran yang lebih robust dari seberan X X adalah jangkauan interquartile (IQR) dinyatakan dengan $$ I Q R = q _ { 3 } - q _ { 1 } = F ^ { - 1 } ( 0.75 ) - F ^ { - 1 } ( 0.25 ) $$ Variansi dan standar deviasi Variansi dari variabel acak X X memberikan pengukuran berapa banyak nilai nilai dari penyimpangan X X dari rata-rata atau nilai harapan dari X X . Lebih tepatnya variansi adalah nilai harapan dari penyimpangan dari mean yang dikuadratkan yang didefinisikan dengan $$ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ ( X - \\mu ) ^ { 2 } ] = \\left{ \\begin{array} { l l } { \\sum _ { x } ( x - \\mu ) ^ { 2 } f ( x ) } & { \\text {jika } X \\text { adalah diskrit } } \\ { \\int _ { - \\infty } ^ { \\infty } ( x - \\mu ) ^ { 2 } f ( x ) d x } & { \\text { jika } X \\text { adalah kontinu } } \\end{array} \\right. $$ Standar deviasi \\sigma \\sigma didefinisikan sebagai akar kuadrat positif dari variansi \\sigma^2 \\sigma^2 . Kita dapat juga menulis variansi sebagai selisih antara ekpektasi X^2 X^2 dan akar dari ekpektasi X X : Variansi Sampel Variansi sampel didefinisikan dengan $$ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } $$ Standar deviasi adalah akar dari variansi sample yang dinyatakan dengan $$ \\hat { \\sigma } = \\sqrt { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } } $$","title":"Mengukur sebaran (dispersion)"},{"location":"Eksplorasi%20data/#analisa-bivariate","text":"Dalam analisa bivariate, kita memandang dua atribut pada waktu yang sama. Kita fokus untuk memahami keterkaitan atau kebergantunga antara dua variabel atau atribut tersebut, jika ada. Kita lalu membatasi pada dua variabel X_1 X_1 dan X_2 X_2 , dengan D D dinyatakan sebagai matrik dengan ukuran $ n \\times 2$ $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Secara geometri, kita dapat memandang D D dalam dua cara. Itu dapat dianggap sebagai n n titik atau vektor dalam 2-ruang dimensi terhadap atribut X_1 X_1 dan X_2 X_2 yaitu x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 .Selain itu dapat dilihat sebagai 2 titik atau vektor dalam n n -ruang dimensi yang berisi titik, yaitu setiap kolom adalah vektor dalam $ \\mathbb R^n$ sebagai berikut : $$ \\left. \\begin{array} { l } { X _ { 1 } = ( x _ { 11 } , x _ { 21 } , \\ldots , x _ { n 1 } ) ^ { T } } \\ { X _ { 2 } = ( x _ { 12 } , x _ { 22 } , \\ldots , x _ { n 2 } ) ^ { T } } \\end{array} \\right. $$ Dalam sudut pandang probabilistik, vektor kolom X=(X_1,X_2)^T X=(X_1,X_2)^T dianggapa variabel acak bivariate dan titik titik x _ { i } ( 1 \\leq i \\leq n ) x _ { i } ( 1 \\leq i \\leq n ) dinyatakan sebagai sampel acak yang diperoleh dari X X , yaitu x_i x_i dianggap independent and identically distributed (iid) seperti X X . Fungsi Massa Probabilitas Gabungan Empiris Fungsi Massa Probabilitas Gabungan Empiris untuk X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) dimana I I adalah variabel indikator yang bernilai 1 jika argumen argumennya benar $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { i 1 } = x _ { 1 } \\text { dan } x _ { i 2 } = x _ { 2 } } \\ { 0 } & { \\text { untuk yang lainnya } } \\end{array} \\right. $$ Seperti dalam kasus univariate, fungsi probabilitas menempatkan massa probabilitas \\frac {1}{n} \\frac {1}{n} pada setiap objek dalam data sampel.","title":"Analisa Bivariate"},{"location":"Eksplorasi%20data/#mengukur-dispersi","text":"Mean Rata rata bivariate didefinisikan sebagai nilai harapan dari variabel acak vektor X X , didefinisikan sebagai berikut : $$ \\mu = E [ X ] = E \\left[ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) \\right] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) $$ Dengan kata lain, rata-rata bivariate adalah nilai harapan dari masing masing atribut. Rata-rata sampel dapat diperoleh dari \\hat f_{x_1} \\hat f_{x_1} dan \\hat f_{x_2} \\hat f_{x_2} , fungsi massa probabilitas empiris dari X_1 X_1 dan X_2 X_2 , menggunakan persamaan (2.5). Dapat juga dihitung dari gabungan fungsi massa probabilitas empiris dalam persamaan (2.17) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x )\\right ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Variansi Kita dapat menghitung variansi masing masing atribut, yaitu \\sigma_1^2 \\sigma_1^2 untuk X_1 X_1 dan \\sigma_2^2 \\sigma_2^2 untuk X_2 X_2 mengggunkan persamaan (2.8). Variansi secara keseluruhan (1.4) dinyatakan dengan $$ var(D)=\\sigma_1^2 +\\sigma_2^2 $$ Variansi sampel \\hat \\sigma_1^2 + \\hat \\sigma_2^2 \\hat \\sigma_1^2 + \\hat \\sigma_2^2 dapat diestimasi dengan menggunakanpersamaan (2.10) dan jumlah variansi sample adalah \\sigma_1^2 +\\sigma_2^2 \\sigma_1^2 +\\sigma_2^2 2.2.2. Mengukur keterkaitan Covarian Kovarian antara dua atribut X_1 X_1 dan X_2 X_2 mengukur keterkaitan antara kebergantungan linier diantaranya dan didefinisikan dengan $$ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] $$ Dengan linieraritas dari harapan, kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] }\\{ = E [ X _ { 1 } X _ { 2 } - X _ { 1 } \\mu _ { 2 } - X _ { 2 } \\mu _ { 1 } + \\mu _ { 1 } \\mu _ { 2 } ] }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 2 } E [ X _ { 1 } ] - \\mu _ { 1 } E [ X _ { 2 } ] + \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] }\\end{array} \\right. $$ Persamaan (2.21) dapat dianggap sebagai generalisasi dari variansi univariate persamaan (2.9) pada kasus bivariate. Jika X_1 X_1 dan X_2 X_2 adalah variabel acak saling bebas, maka kita dapat simpulkan bahwa covariannya adalah nol. Ini karena jika X_1 X_1 dan X_2 X_2 adalah saling bebas, maka kita memiliki $$ E [ X _ { 1 } X _ { 2 } ] = E [ X _ { 1 } ] \\cdot E [ X _ { 2 } ] $$ yang pada akhirnya menyiratkan bahwa $$ \\sigma{12}= 0 $$ Namaun sebaliknya tidak benar. Yaitu jika \\sigma_{12}=0 \\sigma_{12}=0 , kita tidak dapat mengklaim bahwa $X_1 $ dan X_2 X_2 adalah saling bebas. Semuanya kita katakan bahwa tidak adalah kebergantung linier antara keduanya. Kovarian sampel antra X1 X1 dan X_2 X_2 dinyatakan dengan $$ \\hat { \\sigma } _ { 12 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) $$ Korelasi Korelasi antara variabel X_1 X_1 dan X_2 X_2 adalah standarisasi kovarian, yang didapatkan dengan menormalisasi kovarian dengan standar deviasi masing masing variabel dinyatakan dengan \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } Matrik Kovarian Variansi dari untuk dua atribut X_1 X_1 dan X_2 X_2 dapat diringkas dalam matrik covarianse bujursangkar denga ukuran $2 \\times 2 $ dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] }\\{ = E \\left[ \\left( \\begin{array} { c } { X _ { 1 } - \\mu _ { 1 } } \\ { X _ { 2 } - \\mu _ { 2 } } \\end{array} \\right) ( X _ { 1 } - \\mu _ { 1 } \\quad X _ { 2 } - \\mu _ { 2 } ) \\right ] }\\{ = \\left( \\begin{array} { c c } { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\ { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } \\end{array} \\right) }\\end{array} \\right. $$ Karena \\sigma_{12}=\\sigma_{21} \\sigma_{12}=\\sigma_{21} , $\\Sigma $ adalah matrik simetris. Matrik vovarian merekam variansi tertentu atribut pada diagonal utamanya, dan informasi covarian pada elemen element bukan diagonal. Total variance dari dua atribut dinyatakan sebagai jumlah elemen elemen diagonal dari $ \\Sigma $ , yang juga disebut trace dari $ \\Sigma $ dinyatakan dengan $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } $$ Kita segera memiliki $ tr(\\Sigma)\\geq 0$ Secara umum covarian adalah non-negatif, karena $$ | \\Sigma | = \\operatorname { det } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\sigma _ { 12 } ^ { 2 } = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\rho _ { 12 } ^ { 2 } \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } = ( 1 - \\rho _ { 12 } ^ { 2 } ) \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } $$ dimana kitu gunakan persamaan (2.23), yaiut \\rho_{12}\\sigma_1\\sigma_2 \\rho_{12}\\sigma_1\\sigma_2 . dengan |\\Sigma| |\\Sigma| adalah determinan dari matrik kovarian. Perhatikan bahwa |\\rho_{12}|\\leq 1 |\\rho_{12}|\\leq 1 menyebabkan \\rho_{12}^2 \\leq 1 \\rho_{12}^2 \\leq 1 sehingga det (\\Sigma) \\geq 1 (\\Sigma) \\geq 1 furthermore determinannya adalah non-negative. Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\sigma } _ { 1 } ^ { 2 } } & { \\hat { \\sigma } _ { 12 } } \\ { \\hat { \\sigma } _ { 12 } } & { \\hat { \\sigma } _ { 2 } ^ { 2 } } \\end{array} \\right) $$ Matrik kovarian sampe $ \\hat \\Sigma$ memilki karakteristik sama seperti \\Sigma \\Sigma , yaitu simetris dan |\\hat \\Sigma| \\geq 0 |\\hat \\Sigma| \\geq 0 dan itu dapat digunakan untum memudahkan mendapatkan total sampel dan variansi secara umum Contoh (Rata rata Sampel dan Covarian) Perhatikan atribut sepal length dan sepal width untuk data iris, seperti yang diplot dalam gambar 2.4. Ada n=150 data dalam d=2 d=2 ruang dimensi. Rata rata sampel adalah $$ \\hat { \\mu } = \\left( \\begin{array} { l } { 5.843 } \\ { 3.054 } \\end{array} \\right) $$ Matrik covarian dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.681 } & { - 0.039 } \\ { - 0.039 } & { 0.187 } \\end{array} \\right) $$ Variansi untuk sepal length adalah \\hat \\sigma_1^2=0.681 \\hat \\sigma_1^2=0.681 dan sepal width adalah \\hat \\sigma_2^2=0.187 \\hat \\sigma_2^2=0.187 . Covarian antara dua atribut adalah \\hat \\sigma_{12}=-0.039 \\hat \\sigma_{12}=-0.039 dan korelasi antara dua atribut tersebut adalah $$ \\hat { \\rho } _ { 12 } = \\frac { - 0.039 } { \\sqrt { 0.681 \\cdot 0.187 } } = - 0.109 $$ Lalu, ada korelasi yang sangat lemah antara dua atribut tersebut Total variansi sampel dinyatakan dengan $$ \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 = 0.868 $$ dan variansi secara umum dinyatakan dengan $$ \\hat { \\Sigma } | = \\operatorname { det } ( \\hat { \\Sigma } ) = 0.681 \\cdot 0.187 - ( - 0.039 ) ^ { 2 } = 0.126 $$","title":"Mengukur Dispersi"},{"location":"Eksplorasi%20data/#analisa-multivariate","text":"Dalam analisa multivariate, kita melihat atribut numerik dengan d d dimensi X_1,X_2,...X_d X_1,X_2,...X_d . Data dinyatakan degan matrik n\\times d n\\times d seperti berikut $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Jika dilihat dari baris data memiliki n n objek atatu vektor dalam d d ruang dimensi atribut $$ x _ { i } = ( x _ { i 1 } , x _ { i 2 } , \\ldots , x _ { i d } ) ^ { T } \\in \\mathbb R ^ { d } $$ Jika dilihat dari sudut pandang kolom, data diangga sebagai d d objek atau vektor dalam n n dimensi ruang dengan titik-titik data $$ X _ { j } = ( x _ { 1 j } , x _ { 2 j } , \\ldots , x _ { n j } ) ^ { T } \\in R ^ { n } $$ Jika dilihat dari sudut pandang probabilitas, d d atribut dimodelkan dengan variabel acak vektor X=(X_1,X_2,...X_d)^T X=(X_1,X_2,...X_d)^T dan titik titik x_i x_i dianggap sebagai sampel acak yang diperoleh dari X X , atribut atribut tersebut independent and identfically distributed dari X X (i.i.d X X ) Mean Generalisasi persamaan (2.18) rata-rata vektor multivariate diperoleh dari masing-masing atribut yang dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\ { \\vdots } \\ { E [ X _ { d } ] } \\end{array} \\right) = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) $$ Generalisasi persamaan (2.19) rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Matrik Kovarian Generalisasi persamaan (2.26) untuk d d dimensi, kovarian multicovariate di dinyatakan dengan matrik kovarian simetris $ d\\times d $yang menyatakan kovarian untuk setiap pasangan atribut $$ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\cdots } & { \\sigma _ { 1 d } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\cdots } & { \\sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\cdots } & { \\cdots } \\ { \\sigma _ { d 1 } } & { \\sigma _ { d 2 } } & { \\cdots } & { \\sigma _ { d } ^ { 2 } } \\end{array} \\right) $$ Elemen diagonal $\\sigma_i^2 $ menyatakan variansi atribut X_i X_i , dimana elemen-elemen bukan diagonal \\sigma_{ij} = \\sigma_{ji} \\sigma_{ij} = \\sigma_{ji} menyatakan kovarian antara atribut pasangan X_i X_i dan X_j X_j . Matrik kovarian adalah positif semidefinite Contoh Rata-rata sample dan matrik covarian. Perhatikan semua atribut numerik untuk data iris, namanya sepal length, petal length, dan petal width. Rata rata multivarean dinyatakan dengan \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 Contoh Perkalian dalam dan perkalian luar . Untuk mengdeskripsikan komputasi perkalian dalam dan perkalian luar dari matrik covarian, perhatikan data 2-dimensi $$ D = \\left( \\begin{array} { l l } { A _ { 1 } } & { A _ { 2 } } \\ \\hline 1 & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) $$ Rata-rata vektor adalah sebagai berikut $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { 15 / 3 } \\ { 8.7 / 3 } \\end{array} \\right) = \\left( \\begin{array} { c } { 5 } \\ { 2.9 } \\end{array} \\right) $$ dan matrik data terpusat dinyatakan $$ Z = D - 1 \\cdot \\mu ^ { T } = \\left( \\begin{array} { l l } { 1 } & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) - \\left( \\begin{array} { l } { 1 } \\ { 1 } \\ { 1 } \\end{array} \\right) \\left( \\begin{array} { l l } { 5 } & { 2.9 } \\end{array} \\right) = \\left( \\begin{array} { r r } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) $$ Pendekatan perkalian dalam [pers. 2.30] untuk menghitung matrik kovarian adalah $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\frac { 1 } { n } Z ^ { T } Z = \\frac { 1 } { 3 } \\left( \\begin{array} { c c c } { - 4 } & { 0 } & { 4 } \\ { - 2.1 } & { - 0.5 } & { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) }\\{ = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32 } & { 18.8 } \\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\ { 6.27 } & { 3.81 } \\end{array} \\right) }\\end{array} \\right. $$ Pendekatan lain yaitu dengan perkalian luar [pers. 2.31] dibyatakan dengan $$ \\hat { \\Sigma } = \\frac { 1 } { n } \\sum _ { j = 1 } ^ { n } z _ { i } \\cdot z _ { i } ^ { T } $$ = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. dimana data terpusat z_i z_i adalah baris dari Z Z","title":"Analisa Multivariate"},{"location":"Eksplorasi%20data/#atribut-kategorikal","text":"Kita asumsikan bahwa data terdiri dari satu atribut X X . Domain dari X X terdiri dari m m nilai simbolis dom(X)={a_1,a_2,...a_m} dom(X)={a_1,a_2,...a_m} . Data D D adalah n\\times 1 n\\times 1 matrik data simbolis yang dinyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ { x _ { 1 } } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana setiap nilai x_i \\in dom(X) x_i \\in dom(X)","title":"Atribut Kategorikal"},{"location":"Eksplorasi%20data/#variabel-bernouli","text":"Marilah kita lihat kasus ketika atribut kategorikal X X memililik domain $ {a_1,a_2}$ dengan m=2 m=2 . Kita dapat memodelkan X X sebagai variabel acak Bernouli, yang didasarkan pada dua nilai berbeda yaitu 1 dan 0, sesuai dengan pemetaan $$ X ( v ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } v = a _ { 1 } } \\ { 0 } & { \\text { if } v = a _ { 2 } } \\end{array} \\right. $$ Fungsi massa probabilitas (PMF) dari X X dinyatakan dengan $$ P ( X = x ) = f ( x ) = \\left{ \\begin{array} { l l } { p _ { 1 } } & { \\text { if } x = 1 } \\ { p _ { 0 } } & { \\text { if } x = 0 } \\end{array} \\right. $$ dimana p_1 p_1 dan p_0 p_0 adalah parameter distribusi, yang harus memenuhi kondisi $$ p_1+p_0=1 $$ Karena hanya ada satu parameter bebas, biasanya menotasikan p_1=p p_1=p maka p_0=1-p p_0=1-p . Fungsi Massa Probabilitas dari variabel acak Bernouli X X dapat kemudian ditulis dengan $$ P ( X = x ) = f ( x ) = p ^ { x } ( 1 - p ) ^ { 1 - x } $$ Kita dapat melihat bahwa P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p seperti yand diharapkan Mean dan Variansi Nilai harapan dari X X dinyatakan dengan $$ \\mu = E [ X ] = 1 \\cdot p + 0 \\cdot ( 1 - p ) = p $$ dan variansi dari X X dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ X ^ { 2 } ] - ( E [ X ] ) ^ { 2 } }\\ \\hspace{7mm}= ( 1 ^ { 2 } \\cdot p + 0 ^ { 2 } \\cdot ( 1 - p ) ) - p ^ { 2 } = p - p ^ { 2 } = p ( 1 - p ) \\\\end{array} \\right. $$ Rata-rata sampel dan Variansi Untuk mengestimasi parameter dari variabel Bernouli X X , kita asumsikan bahwa setiap simbol dipetakan ke nilai biner. Sehingga, sekumpulan nilai {x_1,x_2,...x_n} {x_1,x_2,...x_n} diasumsikan menjadi sampel acak yang diperoleh dari X X (yaitu setiap $ x_i$ adalah IID dengan X X . Rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { n _ { 1 } } { n } = \\hat { p } $$ dimana n_1 n_1 adalah banyaknya titik dengan x_1=1 x_1=1 dalam sampel acak (sama dengan banyak kejadian dari simbol a_1 a_1 ) Misal n_0=n-n_1 n_0=n-n_1 menyatakan banyak titik dengan x_i=0 x_i=0 dalam sampel acak. Variansi sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } }\\ \\hspace{7mm}{ = \\frac { n _ { 1 } } { n } ( 1 - \\hat { p } ) ^ { 2 } + \\frac { n - n _ { 1 } } { n } ( - \\hat { p } ) ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ^ { 2 } + ( 1 - \\hat { p } ) \\hat { p } ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ( 1 - \\hat { p } + \\hat { p } ) }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) }\\end{array} \\right. $$ Variansi sampel dapat juga diperoleh langsung dari persamaan(3.1) dengan mensubsitusikan \\hat p \\hat p untuk p p . Contoh Perhatikan atribut sepal length ( X X ) untuk dataset iris dalam tabel 1.1. Marilah kita definisikan bunga iris dengan Long jika bunga itu sepal length dalam range [7, \\infty ] [7, \\infty ] , dan short jika sepal length dalam range [-\\infty,7] [-\\infty,7] . Kemudian X_1 X_1 dapat dinyatakan dengan atribut kategorikan dengan domain {Long,Short}. Dari sampel yang diamati ukuran n=150 n=150 , kita menemukan 13 iris long. Rata-rata sampel dari X_1 X_1 adalah $$ \\hat { \\mu } = \\hat { p } = 13 / 150 = 0.087 $$ dan variansinya adalah $$ \\hat { \\sigma } ^ { 2 } = \\hat { p } ( 1 - \\hat { p } ) = 0.087 ( 1 - 0.087 ) = 0.087 \\cdot 0.913 = 0.079 $$ Ditribusi binomial : banyaknya kejadian Diberikan variabel Bernoulli X X , misal \\{x_1,x_2,...x_n\\} \\{x_1,x_2,...x_n\\} menyatakan sampel acak dari ukuran n n yang diperoleh dari X X . Misal N N adalah variabel acak yang menyatakan numlah kejadi dari simbol a_1 a_1 (nilai X=1 X=1 ) dalam sampe. N adalah distribusi binomial yang dinyatakan dengan $$ f ( N = n _ { 1 } | n , p ) = \\left( \\begin{array} { l } { n } \\ { n _ { 1 } } \\end{array} \\right) p ^ { n _ { 1 } } ( 1 - p ) ^ { n - n _ { 1 } } $$ Dalam kenyataannya, N N adalah jumlah dari n n variabel acak Bernoulli x_i x_i yang saling bebas dan (IID) dengan X X yaitu N=\\sum_{i=1}^n x_i N=\\sum_{i=1}^n x_i . Dengan liniearitas dari ekpektasi, mean atau jumlah harapan dari kejadian simbol a_i a_i dinyatakan dengan $$ \\mu _ { N } = E [ N ] = E \\left[ \\sum _ { i = 1 } ^ { n } x _ { i } \\right] = \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\sum _ { i = 1 } ^ { n } p = n p $$ Karena x_i x_i adalah semuanya saling bebas, variansi dari N N dinyatakan dengan $$ \\sigma _ { N } ^ { 2 } = \\operatorname { var } ( N ) = \\sum _ { i = 1 } ^ { n } \\operatorname { var } ( x _ { i } ) = \\sum _ { i = 1 } ^ { n } p ( 1 - p ) = n p ( 1 - p ) $$ Contoh 3.2. Dengan meneruskan contoh 3.1, kita dapat menggunakan parameter yang telah diestimasi \\hat p=0.087 \\hat p=0.087 untuk menghitung banyaknya kejadian yang diharapkan N long dari sepal length. distribusi binomial Iris $$ E [ N ] = n \\hat { p } = 150 \\cdot 0.087 = 13 $$ Dalam kasus ini, karena p p dihitung dari sample melalui \\hat p \\hat p , tidak mengherankan bahwa jumlah kejadian diharapkan dari Long Iris sama dengan kejadian yang sebenarnya. Akan tetapi yang lebih menarik adalah kita dapat menghitung variansi jumlah kejadian $$ \\operatorname { var } ( N ) = n \\hat { p } ( 1 - \\hat { p } ) = 150 \\cdot 0.079 = 11.9 $$ Meningkatnya ukuran sample, distribusi binomial seperti yang diberikan dapalam persamaan 3.3 cenderung ke distribusi normal dengan \\mu=13 \\mu=13 dan \\sigma=\\sqrt{11.9}=3.45 \\sigma=\\sqrt{11.9}=3.45 . Sehingga dengan kepercaan lebih besar dari 95%, kita dapat mengklam bahwa jumlah kejadian dari a_i a_i akan terletak dalam rentang \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] yang mengikuti dari fakta bahwa untuk distribusi normal 95,45% dari massa probabilitas terletak dalam dua standar deviasi dari rata-rata.","title":"Variabel Bernouli"},{"location":"Eksplorasi%20data/#variable-multivariate-bernoulli","text":"Sekarang kita memandang kasus umum ketika X X adalah atribut kategorical dengan domain \\{a_1,a_2,...a_m\\} \\{a_1,a_2,...a_m\\} . Kita dapat memodelkan X X sebagai variabel acak Bernoulli m m -dimensi X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } dimana setiap A_i A_i adalah variabel Bernoulli dengan parameter p_i p_i yang menotasikan probabilitas dari pengamatan simbol a_i a_i . Akan tetapi karena X X dapat mengasumsikan hanya satu dari nilai simbolik pada suatu waktum jika X=a_i X=a_i maka A_i=1 A_i=1 dan A_j=0 A_j=0 untuk semua j \\neq i j \\neq i . Variabel acak X \\in {0,1}^m X \\in {0,1}^m , dan jika X=a_i X=a_i , maka X=e_i X=e_i , dimana e_i e_i adalah standar vektor basis ke i, e_i\\in\\mathbb R^m e_i\\in\\mathbb R^m yang dinyatakan dengan $$ e _ { i } = ( \\overbrace { 0 , \\ldots , 0 } ^ { i - 1 } , 1 , \\overbrace { 0 , \\ldots , 0 } ^ { m - i } ) ^ { T } $$ Pada e_i e_i hanya elemen ke i adalah 1 ( e_{ii}=1 e_{ii}=1 ) , sedangkan semua elemen yang lain adalah nol, ( e_{ij}=0, \\forall j \\neq i e_{ij}=0, \\forall j \\neq i ). Disini, definis yang lebih tepat dari variabel Bernoulli multivariate , yaitu generalisasi dari variabel Bernoullii dari dua hasil ke m m hasil. Kita kemudian memodelkan atribut kategorical X X sebagai variabel Bernoulli multivariate X X didefinisikan dengan $$ X ( v ) = e _ { i } \\text { if } v = a _ { i } $$ Rentang dari X X terdiri dari m m nilai vektor berbeda \\{e_1,e_2,...e_m\\} \\{e_1,e_2,...e_m\\} dengan fungsi massa probabilitas dari X X dinyatakan dengan $$ P ( X = e _ { i } ) = f ( e _ { i } ) = p _ { i } $$ dimana p_i p_i adalah probabilitas dari nilai pengamatan a_i a_i . Parameter ini harus memenuhi kondisi $$ \\sum _ { i = 1 } ^ { m } p _ { i } = 1 $$ Fungsi massa prababilitas dapat ditulis secara utuh sebagai berikut $$ P ( X = e _ { i } ) = f ( e _ { i } ) = \\prod _ { j = 1 } ^ { m } p _ { j } ^ { e _ { i j } }Ka $$ Kareana e_ii=1 e_ii=1 dan e_ij=0 e_ij=0 funtuk $ j\\neq i$, kita dapat melihat bahwa, seperti yang diharapkan, kita miliki $$ f ( e _ { i } ) = \\prod _ { j=1 } ^ { m } p _ { j } ^ { e _ { i j } } = p _ { 1 } ^ { e _ { i 0 } } \\times \\cdots p _ { i } ^ { e _ { i i } } \\cdots \\times p _ { m } ^ { e _ { i m } } = p _ { 1 } ^ { 0 } \\times \\cdots p _ { i } ^ { 1 } \\cdots \\times p _ { m } ^ { 0 } = p _ { i } $$ \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. Contoh : Marilah kita lihat atribut sepal length ( X_1 X_1 ) untuk data Iris seperti yang ditunjukkan dalam tabel 1.2. Kita membagi sepal length kedalam empat interval yang sama, dan memberikan nama untuk setiap interval seperti yang diunjukkan dalam tabel 3.1. Kita lihat X_1 X_1 sebagai atribut kategorical dengan domain $$ {a _ { 2 } = \\text { VeryShort, } a _ { 2 } = \\text { Short, } a _ { 3 } = \\operatorname { Long } , a _ { 4 } = \\operatorname{Very Long}} $$ Kita memodelkan atribut kategorical X_1 X_1 sebagai variabel X X Bernoulli multivariate, didefinisikan dengan $$ X ( v ) = \\left{ \\begin{array} { l l } { e _ { 1 } = ( 1,0,0,0 ) } & { \\text { jika } v = a _ { 1 } } \\ { e _ { 2 } = ( 0,1,0,0 ) } & { \\text { jika } v = a _ { 2 } } \\ { e _ { 3 } = ( 0,0,1,0 ) } & { \\text { jika } v = a _ { 3 } } \\ { e _ { 4 } = ( 0,0,0,1 ) } & { \\text { jika } v = a _ { 4 } } \\end{array} \\right. $$ Misalkan, simbol x_1=Short=a_2 x_1=Short=a_2 dinyatakan dengan (0,1,0,0)^T=e_2 (0,1,0,0)^T=e_2 Mean Mean atau nilai harapan dari X X dapat diperoleh dengan $$ \\mu = E [ X ] = \\sum _ { i = 1 } ^ { m } e _ { i } f ( e _ { i } ) = \\sum _ { i = 1 } ^ { m } e _ { i } p _ { i } = \\left( \\begin{array} { l } { 1 } \\ { 0 } \\ { \\vdots } \\ { 0 } \\end{array} \\right) p _ { 1 } + \\cdots + \\left( \\begin{array} { l } { 0 } \\ { 0 } \\ { \\vdots } \\ { 1 } \\end{array} \\right) p _ { m } = \\left( \\begin{array} { c } { p _ { 1 } } \\ { p _ { 2 } } \\ { \\vdots } \\ { p _ { m } } \\end{array} \\right) = p $$","title":"Variable multivariate Bernoulli"},{"location":"authors-notes/","text":"Author's notes \u00b6 Hi, I'm Martin ( @squidfunk ) \u00b6 I'm a freelance polyglot software engineer and entrepreneur from Cologne, Germany with more than 12 years of experience in full-stack web development and system programming. Why another theme? \u00b6 Some time ago I wanted to release a project to the open, but it was in need of user documentation. I checked out the available tools and stuck with MkDocs, because it was so simple and easy to use. However, none of the available themes convinced me. I wanted to build something that was usable on all screen sizes from the ground up, something beautiful and practical at the same time. Google's Material Design appeared to be the perfect fit and this something became Material, a Material Design theme for MkDocs.","title":"Author's notes"},{"location":"authors-notes/#authors-notes","text":"","title":"Author's notes"},{"location":"authors-notes/#hi-im-martin-squidfunk","text":"I'm a freelance polyglot software engineer and entrepreneur from Cologne, Germany with more than 12 years of experience in full-stack web development and system programming.","title":"Hi, I'm Martin (@squidfunk)"},{"location":"authors-notes/#why-another-theme","text":"Some time ago I wanted to release a project to the open, but it was in need of user documentation. I checked out the available tools and stuck with MkDocs, because it was so simple and easy to use. However, none of the available themes convinced me. I wanted to build something that was usable on all screen sizes from the ground up, something beautiful and practical at the same time. Google's Material Design appeared to be the perfect fit and this something became Material, a Material Design theme for MkDocs.","title":"Why another theme?"},{"location":"compliance/","text":"Compliance with GDPR \u00b6 Material does not process any personal data \u00b6 Material is a theme for MkDocs, a static site generator. In itself, Material does not perform any tracking or processing of personal data. However, some of the third-party services that Material integrates with may actually be in breach with the General Data Protection Regulation (GDPR) and need to be evaluated carefully. Third-party services \u00b6 Google Fonts \u00b6 Material makes fonts easily configurable by relying on Google Fonts CDN. However, embedding fonts from Google is currently within a gray area as there's no official statement or ruling regarding GDPR compliance and the topic is still actively discussed . For this reason, if you need to ensure GDPR compliance, you should disable the usage of the Google Font CDN with: theme : font : false When Google Fonts are disabled, Material will default to Helvetica Neue and Monaco with their corresponding fall backs, relying on system fonts. You could however include your own, self-hosted webfont by overriding the fonts block. The icon fonts (Material and FontAwesome) are bundled with the theme, and thus self-hosted so there's no third-party involved. Google Analytics and Disqus \u00b6 Material comes with Google Analytics and Disqus integrations that need to be enabled explicitly . Disable both integrations in order to be in compliance with the GDPR.","title":"Evaluasi"},{"location":"compliance/#compliance-with-gdpr","text":"","title":"Compliance with GDPR"},{"location":"compliance/#material-does-not-process-any-personal-data","text":"Material is a theme for MkDocs, a static site generator. In itself, Material does not perform any tracking or processing of personal data. However, some of the third-party services that Material integrates with may actually be in breach with the General Data Protection Regulation (GDPR) and need to be evaluated carefully.","title":"Material does not process any personal data"},{"location":"compliance/#third-party-services","text":"","title":"Third-party services"},{"location":"compliance/#google-fonts","text":"Material makes fonts easily configurable by relying on Google Fonts CDN. However, embedding fonts from Google is currently within a gray area as there's no official statement or ruling regarding GDPR compliance and the topic is still actively discussed . For this reason, if you need to ensure GDPR compliance, you should disable the usage of the Google Font CDN with: theme : font : false When Google Fonts are disabled, Material will default to Helvetica Neue and Monaco with their corresponding fall backs, relying on system fonts. You could however include your own, self-hosted webfont by overriding the fonts block. The icon fonts (Material and FontAwesome) are bundled with the theme, and thus self-hosted so there's no third-party involved.","title":"Google Fonts"},{"location":"compliance/#google-analytics-and-disqus","text":"Material comes with Google Analytics and Disqus integrations that need to be enabled explicitly . Disable both integrations in order to be in compliance with the GDPR.","title":"Google Analytics and Disqus"},{"location":"contributing/","text":"../CONTRIBUTING.md","title":"Contributing"},{"location":"customization/","text":"Customization \u00b6 A great starting point \u00b6 Project documentation is as diverse as the projects themselves and the Material theme is a good starting point for making it look great. However, as you write your documentation, you may reach a point where some small adjustments are necessary to preserve the desired style. Adding assets \u00b6 MkDocs provides several ways to interfere with themes. In order to make a few tweaks to an existing theme, you can just add your stylesheets and JavaScript files to the docs directory. Additional stylesheets \u00b6 If you want to tweak some colors or change the spacing of certain elements, you can do this in a separate stylesheet. The easiest way is by creating a new stylesheet file in your docs directory: mkdir docs/stylesheets touch docs/stylesheets/extra.css Then, add the following line to your mkdocs.yml : extra_css : - 'stylesheets/extra.css' Spin up the development server with mkdocs serve and start typing your changes in your additional stylesheet file \u2013 you can see them instantly after saving, as the MkDocs development server implements live reloading. Additional JavaScript \u00b6 The same is true for additional JavaScript. If you want to integrate another syntax highlighter or add some custom logic to your theme, create a new JavaScript file in your docs directory: mkdir docs/javascripts touch docs/javascripts/extra.js Then, add the following line to your mkdocs.yml : extra_javascript : - 'javascripts/extra.js' Further assistance can be found in the MkDocs documentation . Extending the theme \u00b6 If you want to alter the HTML source (e.g. add or remove some part), you can extend the theme. From version 0.16 on MkDocs implements theme extension , an easy way to override parts of a theme without forking and changing the main theme. Setup and theme structure \u00b6 Reference the Material theme as usual in your mkdocs.yml , and create a new folder for overrides, e.g. theme , which you reference using custom_dir : theme : name : 'material' custom_dir : 'theme' Theme extension prerequisites As the custom_dir variable is used for the theme extension process, the Material theme needs to be installed via pip and referenced with the name parameter in your mkdocs.yml . The structure in the theme directory must mirror the directory structure of the original theme, as any file in the theme directory will replace the file with the same name which is part of the original theme. Besides, further assets may also be put in the theme directory. The directory layout of the Material theme is as follows: . \u251c\u2500 assets/ \u2502 \u251c\u2500 images/ # Images and icons \u2502 \u251c\u2500 javascripts/ # JavaScript \u2502 \u2514\u2500 stylesheets/ # Stylesheets \u251c\u2500 partials/ \u2502 \u251c\u2500 integrations/ # 3rd-party integrations \u2502 \u251c\u2500 language/ # Localized languages \u2502 \u251c\u2500 footer.html # Footer bar \u2502 \u251c\u2500 header.html # Header bar \u2502 \u251c\u2500 hero.html # Hero teaser \u2502 \u251c\u2500 language.html # Localized labels \u2502 \u251c\u2500 nav-item.html # Main navigation item \u2502 \u251c\u2500 nav.html # Main navigation \u2502 \u251c\u2500 search.html # Search box \u2502 \u251c\u2500 social.html # Social links \u2502 \u251c\u2500 source.html # Repository information \u2502 \u251c\u2500 tabs-item.html # Tabs navigation item \u2502 \u251c\u2500 tabs.html # Tabs navigation \u2502 \u251c\u2500 toc-item.html # Table of contents item \u2502 \u2514\u2500 toc.html # Table of contents \u251c\u2500 404 .html # 404 error page \u251c\u2500 base.html # Base template \u2514\u2500 main.html # Default page Overriding partials \u00b6 In order to override the footer, we can replace the footer.html partial with our own partial. To do this, create the file partials/footer.html in the theme directory. MkDocs will now use the new partial when rendering the theme. This can be done with any file. Overriding template blocks \u00b6 Besides overriding partials, one can also override so called template blocks, which are defined inside the Material theme and wrap specific features. To override a template block, create a main.html inside the theme directory and define the block, e.g.: {% extends \"base.html\" %} {% block htmltitle %} <title>Lorem ipsum dolor sit amet</title> {% endblock %} The Material theme provides the following template blocks: Block name Wrapped contents analytics Wraps the Google Analytics integration content Wraps the main content disqus Wraps the disqus integration extrahead Empty block to define additional meta tags fonts Wraps the webfont definitions footer Wraps the footer with navigation and copyright header Wraps the fixed header bar hero Wraps the hero teaser htmltitle Wraps the <title> tag libs Wraps the JavaScript libraries, e.g. Modernizr scripts Wraps the JavaScript application logic source Wraps the linked source files site_meta Wraps the meta tags in the document head site_nav Wraps the site navigation and table of contents styles Wraps the stylesheets (also extra sources) For more on this topic refer to the MkDocs documentation Theme development \u00b6 The Material theme uses Webpack as a build tool to leverage modern web technologies like Babel and SASS . If you want to make more fundamental changes, it may be necessary to make the adjustments directly in the source of the Material theme and recompile it. This is fairly easy. Environment setup \u00b6 In order to start development on the Material theme, a Node.js version of at least 8 is required. First, clone the repository: git clone https://github.com/squidfunk/mkdocs-material Next, all dependencies need to be installed, which is done with: cd mkdocs-material pip install -r requirements.txt npm install If you're on Windows, you may also need to install GNU Make Development mode \u00b6 The development server can be started with: npm run watch This will also start the MkDocs development server which will monitor changes on assets, templates and documentation. Point your browser to localhost:8000 and you should see this documentation in front of you. For example, changing the color palette is as simple as changing the $md-color-primary and $md-color-accent variables in src/assets/stylesheets/_config.scss : $ md-color-primary : $ clr-red-400 ; $ md-color-accent : $ clr-teal-a700 ; Automatically generated files Never make any changes in the material directory, as the contents of this directory are automatically generated from the src directory and will be overridden when the theme is built. Build process \u00b6 When you've finished making your changes, you can build the theme by invoking: npm run build This triggers the production-level compilation and minification of all stylesheets and JavaScript sources. When the command exits, the final theme is located in the material directory. Add the theme_dir variable pointing to the aforementioned directory in your original mkdocs.yml . Now you can run mkdocs build and you should see your documentation with your changes to the original Material theme.","title":"Modeling"},{"location":"customization/#customization","text":"","title":"Customization"},{"location":"customization/#a-great-starting-point","text":"Project documentation is as diverse as the projects themselves and the Material theme is a good starting point for making it look great. However, as you write your documentation, you may reach a point where some small adjustments are necessary to preserve the desired style.","title":"A great starting point"},{"location":"customization/#adding-assets","text":"MkDocs provides several ways to interfere with themes. In order to make a few tweaks to an existing theme, you can just add your stylesheets and JavaScript files to the docs directory.","title":"Adding assets"},{"location":"customization/#additional-stylesheets","text":"If you want to tweak some colors or change the spacing of certain elements, you can do this in a separate stylesheet. The easiest way is by creating a new stylesheet file in your docs directory: mkdir docs/stylesheets touch docs/stylesheets/extra.css Then, add the following line to your mkdocs.yml : extra_css : - 'stylesheets/extra.css' Spin up the development server with mkdocs serve and start typing your changes in your additional stylesheet file \u2013 you can see them instantly after saving, as the MkDocs development server implements live reloading.","title":"Additional stylesheets"},{"location":"customization/#additional-javascript","text":"The same is true for additional JavaScript. If you want to integrate another syntax highlighter or add some custom logic to your theme, create a new JavaScript file in your docs directory: mkdir docs/javascripts touch docs/javascripts/extra.js Then, add the following line to your mkdocs.yml : extra_javascript : - 'javascripts/extra.js' Further assistance can be found in the MkDocs documentation .","title":"Additional JavaScript"},{"location":"customization/#extending-the-theme","text":"If you want to alter the HTML source (e.g. add or remove some part), you can extend the theme. From version 0.16 on MkDocs implements theme extension , an easy way to override parts of a theme without forking and changing the main theme.","title":"Extending the theme"},{"location":"customization/#setup-and-theme-structure","text":"Reference the Material theme as usual in your mkdocs.yml , and create a new folder for overrides, e.g. theme , which you reference using custom_dir : theme : name : 'material' custom_dir : 'theme' Theme extension prerequisites As the custom_dir variable is used for the theme extension process, the Material theme needs to be installed via pip and referenced with the name parameter in your mkdocs.yml . The structure in the theme directory must mirror the directory structure of the original theme, as any file in the theme directory will replace the file with the same name which is part of the original theme. Besides, further assets may also be put in the theme directory. The directory layout of the Material theme is as follows: . \u251c\u2500 assets/ \u2502 \u251c\u2500 images/ # Images and icons \u2502 \u251c\u2500 javascripts/ # JavaScript \u2502 \u2514\u2500 stylesheets/ # Stylesheets \u251c\u2500 partials/ \u2502 \u251c\u2500 integrations/ # 3rd-party integrations \u2502 \u251c\u2500 language/ # Localized languages \u2502 \u251c\u2500 footer.html # Footer bar \u2502 \u251c\u2500 header.html # Header bar \u2502 \u251c\u2500 hero.html # Hero teaser \u2502 \u251c\u2500 language.html # Localized labels \u2502 \u251c\u2500 nav-item.html # Main navigation item \u2502 \u251c\u2500 nav.html # Main navigation \u2502 \u251c\u2500 search.html # Search box \u2502 \u251c\u2500 social.html # Social links \u2502 \u251c\u2500 source.html # Repository information \u2502 \u251c\u2500 tabs-item.html # Tabs navigation item \u2502 \u251c\u2500 tabs.html # Tabs navigation \u2502 \u251c\u2500 toc-item.html # Table of contents item \u2502 \u2514\u2500 toc.html # Table of contents \u251c\u2500 404 .html # 404 error page \u251c\u2500 base.html # Base template \u2514\u2500 main.html # Default page","title":"Setup and theme structure"},{"location":"customization/#overriding-partials","text":"In order to override the footer, we can replace the footer.html partial with our own partial. To do this, create the file partials/footer.html in the theme directory. MkDocs will now use the new partial when rendering the theme. This can be done with any file.","title":"Overriding partials"},{"location":"customization/#overriding-template-blocks","text":"Besides overriding partials, one can also override so called template blocks, which are defined inside the Material theme and wrap specific features. To override a template block, create a main.html inside the theme directory and define the block, e.g.: {% extends \"base.html\" %} {% block htmltitle %} <title>Lorem ipsum dolor sit amet</title> {% endblock %} The Material theme provides the following template blocks: Block name Wrapped contents analytics Wraps the Google Analytics integration content Wraps the main content disqus Wraps the disqus integration extrahead Empty block to define additional meta tags fonts Wraps the webfont definitions footer Wraps the footer with navigation and copyright header Wraps the fixed header bar hero Wraps the hero teaser htmltitle Wraps the <title> tag libs Wraps the JavaScript libraries, e.g. Modernizr scripts Wraps the JavaScript application logic source Wraps the linked source files site_meta Wraps the meta tags in the document head site_nav Wraps the site navigation and table of contents styles Wraps the stylesheets (also extra sources) For more on this topic refer to the MkDocs documentation","title":"Overriding template blocks"},{"location":"customization/#theme-development","text":"The Material theme uses Webpack as a build tool to leverage modern web technologies like Babel and SASS . If you want to make more fundamental changes, it may be necessary to make the adjustments directly in the source of the Material theme and recompile it. This is fairly easy.","title":"Theme development"},{"location":"customization/#environment-setup","text":"In order to start development on the Material theme, a Node.js version of at least 8 is required. First, clone the repository: git clone https://github.com/squidfunk/mkdocs-material Next, all dependencies need to be installed, which is done with: cd mkdocs-material pip install -r requirements.txt npm install If you're on Windows, you may also need to install GNU Make","title":"Environment setup"},{"location":"customization/#development-mode","text":"The development server can be started with: npm run watch This will also start the MkDocs development server which will monitor changes on assets, templates and documentation. Point your browser to localhost:8000 and you should see this documentation in front of you. For example, changing the color palette is as simple as changing the $md-color-primary and $md-color-accent variables in src/assets/stylesheets/_config.scss : $ md-color-primary : $ clr-red-400 ; $ md-color-accent : $ clr-teal-a700 ; Automatically generated files Never make any changes in the material directory, as the contents of this directory are automatically generated from the src directory and will be overridden when the theme is built.","title":"Development mode"},{"location":"customization/#build-process","text":"When you've finished making your changes, you can build the theme by invoking: npm run build This triggers the production-level compilation and minification of all stylesheets and JavaScript sources. When the command exits, the final theme is located in the material directory. Add the theme_dir variable pointing to the aforementioned directory in your original mkdocs.yml . Now you can run mkdocs build and you should see your documentation with your changes to the original Material theme.","title":"Build process"},{"location":"license/","text":"License \u00b6 MIT License Copyright \u00a9 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright \u00a9 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"memahami-data/","text":"Memahami Data \u00b6 Macam macam Data \u00b6 Dalam data data mining dan maha datar, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut: Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based) Data Terstruktur \u00b6 Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur Macam- macam atribut \u00b6 Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut Macam macam tipe data atribut \u00b6 Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen Data Tidak Terstruktur \u00b6 Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email Bahasa Alami \u00b6 Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia Data yang dibangkitkan oleh Mesin \u00b6 Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut. Data jaringan atau data berbasis Graph \u00b6 Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL Data Audio,Vidio dan Citra \u00b6 Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut. Data streamming \u00b6 Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan Distribusi Data \u00b6 Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data Statistik Deskriptif \u00b6 Ukuran Kecenderungan Terpusat \u00b6 Rata-rata (Mean) \u00b6 Pada bagian ini, kami melihat cara untuk mengukur kecenderungan pusat data. Misalkan kita mempunyai atribut hasil pretest yang dinyatakan dengan atribut X. Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi himpunan nilai N yang diamati atau pengamatan untuk X. Di sini, nilai-nilai ini juga dapat disebut set data (untuk X). Jika kita merencanakan pengamatan untuk nilai pretest, di mana sebagian besar nilai berada? Ini memberi kita gambaran tentang kecenderungan pusat dari data. Ukuran kecenderungan pusat data ukurannya adalah rata-rata(mean), median, modus (mode), dan midrange. Atribut numerik yang paling umum dan efektif dari \"pusat\" dari set data adalah mean (aritmatika). Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi satu set nilai N atau pengamatan, Rata-rata dari nilai pretes dinyatakan dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} Kadang-kadang, setiap nilai x_i x_i dalam satu data dapat dikaitkan dengan bobot w_i w_i untuk i= 1, .., N i= 1, .., N . Bobot tersebut mencerminkan signifikansi, kepentingan, atau frekuensi kejadian yang melekat pada masing masing nilai. Dalam hal ini, kita dapat menghitungnya dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} Meskipun rata-rata adalah jumlah yang sangat berguna untuk menggambarkan kumpulan data, itu tidak selalu cara terbaik untuk mengukur pusat data. Masalah utama dengan mean adalah sensitivitasnya terhadap nilai ekstrim (mis., outlier). Bahkan beberapa nilai ekstrem saja dapat merusak mean. Misalnya, gaji rata-rata di suatu perusahaan mungkin sangat besar didorong oleh beberapa manajer bergaji tinggi. Demikian pula, nilai rata-rata kelas di ujian dapat rata-rata rendah karena beberapa ada beberap skor nilai saja yang sangat rendah. Untuk mengimbangi efek tersebut kita bisa menggunakan rata-rata yang dipangkas (trimmed mean), yang merupakan rata-rata yang diperoleh setelah memangkas nilai paling tinggi dan nilai yang paling rendah. Untuk contoh, kita dapat mengurutkan nilai gaji yang diamati kemudian menghapus 2% atas dan bawah nilai tersebut sebelum menghitung mean. Kita harus menghindari pemotongan bagian yang terlalu besar (seperti 20%) pada kedua ujungnya, karena hal ini dapat mengakibatkan hilangnya informasi yang berharga) Median \u00b6 Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai $(\\frac{N}{2}+1) +(\\frac{N}{2}-1) $ Namun pada data berkelompok, dengan data yang berbentuk kelas interval, kita tidak bisa langsung mengetahui nilai median jika kelas mediannya sudah diketahui dengan formula $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} Contoh Gambar 2.5. Data berkelompok Mode adalah ukuran lain dari kecenderungan sentral. Mode (modus) untuk satu set data adalah nilai yang paling sering terjadi di set. Oleh karena itu, dapat ditentukan untuk atribut kualitatif dan kuantitatif. Dimungkinkan untuk frekuensi terbesar untuk bersesuaian beberapa nilai berbeda, yang menghasilkan lebih dari satu mode. Kumpulan data dengan satu, dua, atau tiga mode masing-masing disebut unimodal, bimodal, dan trimodal. Jika data hanya mengandung nilai data terjadi hanya sekali, maka tidak ada modus Untuk data numerik unimodal yang cukup miring (asimetris), kami memiliki hubungan empiris: \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) Ini menyiratkan bahwa mode untuk kurva frekuensi unimodal yang cukup miring dapat dengan mudah didekati jika nilai rata-rata dan median diketahui. Mengukur Sebaran Data \u00b6 Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data. Rentang (Range), Quartil, and Rentang Interquartile \u00b6 Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot Variansi dan Standar Deviasi \u00b6 Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0 Skewness \u00b6 Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness) Implementasi \u00b6 Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 ))) Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Memahami Data"},{"location":"memahami-data/#memahami-data","text":"","title":"Memahami Data"},{"location":"memahami-data/#macam-macam-data","text":"Dalam data data mining dan maha datar, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut: Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based)","title":"Macam macam Data"},{"location":"memahami-data/#data-terstruktur","text":"Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur","title":"Data Terstruktur"},{"location":"memahami-data/#macam-macam-atribut","text":"Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut","title":"Macam- macam atribut"},{"location":"memahami-data/#macam-macam-tipe-data-atribut","text":"Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen","title":"Macam macam tipe data atribut"},{"location":"memahami-data/#data-tidak-terstruktur","text":"Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email","title":"Data Tidak Terstruktur"},{"location":"memahami-data/#bahasa-alami","text":"Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia","title":"Bahasa Alami"},{"location":"memahami-data/#data-yang-dibangkitkan-oleh-mesin","text":"Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut.","title":"Data yang dibangkitkan oleh Mesin"},{"location":"memahami-data/#data-jaringan-atau-data-berbasis-graph","text":"Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL","title":"Data jaringan atau data berbasis Graph"},{"location":"memahami-data/#data-audiovidio-dan-citra","text":"Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut.","title":"Data Audio,Vidio dan Citra"},{"location":"memahami-data/#data-streamming","text":"Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan","title":"Data streamming"},{"location":"memahami-data/#distribusi-data","text":"Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data","title":"Distribusi Data"},{"location":"memahami-data/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"memahami-data/#ukuran-kecenderungan-terpusat","text":"","title":"Ukuran Kecenderungan Terpusat"},{"location":"memahami-data/#rata-rata-mean","text":"Pada bagian ini, kami melihat cara untuk mengukur kecenderungan pusat data. Misalkan kita mempunyai atribut hasil pretest yang dinyatakan dengan atribut X. Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi himpunan nilai N yang diamati atau pengamatan untuk X. Di sini, nilai-nilai ini juga dapat disebut set data (untuk X). Jika kita merencanakan pengamatan untuk nilai pretest, di mana sebagian besar nilai berada? Ini memberi kita gambaran tentang kecenderungan pusat dari data. Ukuran kecenderungan pusat data ukurannya adalah rata-rata(mean), median, modus (mode), dan midrange. Atribut numerik yang paling umum dan efektif dari \"pusat\" dari set data adalah mean (aritmatika). Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi satu set nilai N atau pengamatan, Rata-rata dari nilai pretes dinyatakan dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} Kadang-kadang, setiap nilai x_i x_i dalam satu data dapat dikaitkan dengan bobot w_i w_i untuk i= 1, .., N i= 1, .., N . Bobot tersebut mencerminkan signifikansi, kepentingan, atau frekuensi kejadian yang melekat pada masing masing nilai. Dalam hal ini, kita dapat menghitungnya dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} Meskipun rata-rata adalah jumlah yang sangat berguna untuk menggambarkan kumpulan data, itu tidak selalu cara terbaik untuk mengukur pusat data. Masalah utama dengan mean adalah sensitivitasnya terhadap nilai ekstrim (mis., outlier). Bahkan beberapa nilai ekstrem saja dapat merusak mean. Misalnya, gaji rata-rata di suatu perusahaan mungkin sangat besar didorong oleh beberapa manajer bergaji tinggi. Demikian pula, nilai rata-rata kelas di ujian dapat rata-rata rendah karena beberapa ada beberap skor nilai saja yang sangat rendah. Untuk mengimbangi efek tersebut kita bisa menggunakan rata-rata yang dipangkas (trimmed mean), yang merupakan rata-rata yang diperoleh setelah memangkas nilai paling tinggi dan nilai yang paling rendah. Untuk contoh, kita dapat mengurutkan nilai gaji yang diamati kemudian menghapus 2% atas dan bawah nilai tersebut sebelum menghitung mean. Kita harus menghindari pemotongan bagian yang terlalu besar (seperti 20%) pada kedua ujungnya, karena hal ini dapat mengakibatkan hilangnya informasi yang berharga)","title":"Rata-rata (Mean)"},{"location":"memahami-data/#median","text":"Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai $(\\frac{N}{2}+1) +(\\frac{N}{2}-1) $ Namun pada data berkelompok, dengan data yang berbentuk kelas interval, kita tidak bisa langsung mengetahui nilai median jika kelas mediannya sudah diketahui dengan formula $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} Contoh Gambar 2.5. Data berkelompok Mode adalah ukuran lain dari kecenderungan sentral. Mode (modus) untuk satu set data adalah nilai yang paling sering terjadi di set. Oleh karena itu, dapat ditentukan untuk atribut kualitatif dan kuantitatif. Dimungkinkan untuk frekuensi terbesar untuk bersesuaian beberapa nilai berbeda, yang menghasilkan lebih dari satu mode. Kumpulan data dengan satu, dua, atau tiga mode masing-masing disebut unimodal, bimodal, dan trimodal. Jika data hanya mengandung nilai data terjadi hanya sekali, maka tidak ada modus Untuk data numerik unimodal yang cukup miring (asimetris), kami memiliki hubungan empiris: \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) Ini menyiratkan bahwa mode untuk kurva frekuensi unimodal yang cukup miring dapat dengan mudah didekati jika nilai rata-rata dan median diketahui.","title":"Median"},{"location":"memahami-data/#mengukur-sebaran-data","text":"Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data.","title":"Mengukur Sebaran Data"},{"location":"memahami-data/#rentang-range-quartil-and-rentang-interquartile","text":"Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot","title":"Rentang (Range), Quartil, and Rentang Interquartile"},{"location":"memahami-data/#variansi-dan-standar-deviasi","text":"Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0","title":"Variansi dan Standar Deviasi"},{"location":"memahami-data/#skewness","text":"Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness)","title":"Skewness"},{"location":"memahami-data/#implementasi","text":"Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 )))","title":"Implementasi"},{"location":"memahami-data/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"memahami-data/#mengukur-jarak-tipe-numerik","text":"Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak  Tipe Numerik"},{"location":"memahami-data/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"memahami-data/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right|","title":"Manhattan distance"},{"location":"memahami-data/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"memahami-data/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } }","title":"Average Distance"},{"location":"memahami-data/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"memahami-data/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}}","title":"Chord distance"},{"location":"memahami-data/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"memahami-data/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } }","title":"Cosine measure"},{"location":"memahami-data/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"memahami-data/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"memahami-data/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical"},{"location":"memahami-data/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"memahami-data/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"memahami-data/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"memahami-data/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"memahami-data/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"memahami/","text":"Memahami Data \u00b6 Macam macam Data \u00b6 Dalam data data mining dan maha data, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut: Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based) Data Terstruktur \u00b6 Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur Macam- macam atribut \u00b6 Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut Macam macam tipe data atribut \u00b6 Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen Data Tidak Terstruktur \u00b6 Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email Bahasa Alami \u00b6 Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia Data yang dibangkitkan oleh Mesin \u00b6 Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut. Data jaringan atau data berbasis Graph \u00b6 Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL Data Audio,Vidio dan Citra \u00b6 Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut. Data streamming \u00b6 Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan Distribusi Data \u00b6 Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data Statistik Deskriptif \u00b6 Ukuran Kecenderungan Terpusat \u00b6 Rata-rata (Mean) \u00b6 Pada bagian ini, kami melihat cara untuk mengukur kecenderungan pusat data. Misalkan kita mempunyai atribut hasil pretest yang dinyatakan dengan atribut X. Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi himpunan nilai N yang diamati atau pengamatan untuk X. Di sini, nilai-nilai ini juga dapat disebut set data (untuk X). Jika kita merencanakan pengamatan untuk nilai pretest, di mana sebagian besar nilai berada? Ini memberi kita gambaran tentang kecenderungan pusat dari data. Ukuran kecenderungan pusat data ukurannya adalah rata-rata(mean), median, modus (mode), dan midrange. Atribut numerik yang paling umum dan efektif dari \"pusat\" dari set data adalah mean (aritmatika). Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi satu set nilai N atau pengamatan, Rata-rata dari nilai pretes dinyatakan dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} Kadang-kadang, setiap nilai x_i x_i dalam satu data dapat dikaitkan dengan bobot w_i w_i untuk i= 1, .., N i= 1, .., N . Bobot tersebut mencerminkan signifikansi, kepentingan, atau frekuensi kejadian yang melekat pada masing masing nilai. Dalam hal ini, kita dapat menghitungnya dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} Meskipun rata-rata adalah jumlah yang sangat berguna untuk menggambarkan kumpulan data, itu tidak selalu cara terbaik untuk mengukur pusat data. Masalah utama dengan mean adalah sensitivitasnya terhadap nilai ekstrim (mis., outlier). Bahkan beberapa nilai ekstrem saja dapat merusak mean. Misalnya, gaji rata-rata di suatu perusahaan mungkin sangat besar didorong oleh beberapa manajer bergaji tinggi. Demikian pula, nilai rata-rata kelas di ujian dapat rata-rata rendah karena beberapa ada beberap skor nilai saja yang sangat rendah. Untuk mengimbangi efek tersebut kita bisa menggunakan rata-rata yang dipangkas (trimmed mean), yang merupakan rata-rata yang diperoleh setelah memangkas nilai paling tinggi dan nilai yang paling rendah. Untuk contoh, kita dapat mengurutkan nilai gaji yang diamati kemudian menghapus 2% atas dan bawah nilai tersebut sebelum menghitung mean. Kita harus menghindari pemotongan bagian yang terlalu besar (seperti 20%) pada kedua ujungnya, karena hal ini dapat mengakibatkan hilangnya informasi yang berharga) Median \u00b6 Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai $(\\frac{N}{2}+1) +(\\frac{N}{2}-1) $ Namun pada data berkelompok, dengan data yang berbentuk kelas interval, kita tidak bisa langsung mengetahui nilai median jika kelas mediannya sudah diketahui dengan formula $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} Contoh Gambar 2.5. Data berkelompok Mode adalah ukuran lain dari kecenderungan sentral. Mode (modus) untuk satu set data adalah nilai yang paling sering terjadi di set. Oleh karena itu, dapat ditentukan untuk atribut kualitatif dan kuantitatif. Dimungkinkan untuk frekuensi terbesar untuk bersesuaian beberapa nilai berbeda, yang menghasilkan lebih dari satu mode. Kumpulan data dengan satu, dua, atau tiga mode masing-masing disebut unimodal, bimodal, dan trimodal. Jika data hanya mengandung nilai data terjadi hanya sekali, maka tidak ada modus Untuk data numerik unimodal yang cukup miring (asimetris), kami memiliki hubungan empiris: \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) Ini menyiratkan bahwa mode untuk kurva frekuensi unimodal yang cukup miring dapat dengan mudah didekati jika nilai rata-rata dan median diketahui. Mengukur Sebaran Data \u00b6 Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data. Rentang (Range), Quartil, and Rentang Interquartile \u00b6 Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot Variansi dan Standar Deviasi \u00b6 Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0 Skewness \u00b6 Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness) Implementasi \u00b6 Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 ))) Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Memahami Data"},{"location":"memahami/#memahami-data","text":"","title":"Memahami Data"},{"location":"memahami/#macam-macam-data","text":"Dalam data data mining dan maha data, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut: Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based)","title":"Macam macam Data"},{"location":"memahami/#data-terstruktur","text":"Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur","title":"Data Terstruktur"},{"location":"memahami/#macam-macam-atribut","text":"Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut","title":"Macam- macam atribut"},{"location":"memahami/#macam-macam-tipe-data-atribut","text":"Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen","title":"Macam macam tipe data atribut"},{"location":"memahami/#data-tidak-terstruktur","text":"Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email","title":"Data Tidak Terstruktur"},{"location":"memahami/#bahasa-alami","text":"Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia","title":"Bahasa Alami"},{"location":"memahami/#data-yang-dibangkitkan-oleh-mesin","text":"Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut.","title":"Data yang dibangkitkan oleh Mesin"},{"location":"memahami/#data-jaringan-atau-data-berbasis-graph","text":"Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL","title":"Data jaringan atau data berbasis Graph"},{"location":"memahami/#data-audiovidio-dan-citra","text":"Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut.","title":"Data Audio,Vidio dan Citra"},{"location":"memahami/#data-streamming","text":"Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan","title":"Data streamming"},{"location":"memahami/#distribusi-data","text":"Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data","title":"Distribusi Data"},{"location":"memahami/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"memahami/#ukuran-kecenderungan-terpusat","text":"","title":"Ukuran Kecenderungan Terpusat"},{"location":"memahami/#rata-rata-mean","text":"Pada bagian ini, kami melihat cara untuk mengukur kecenderungan pusat data. Misalkan kita mempunyai atribut hasil pretest yang dinyatakan dengan atribut X. Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi himpunan nilai N yang diamati atau pengamatan untuk X. Di sini, nilai-nilai ini juga dapat disebut set data (untuk X). Jika kita merencanakan pengamatan untuk nilai pretest, di mana sebagian besar nilai berada? Ini memberi kita gambaran tentang kecenderungan pusat dari data. Ukuran kecenderungan pusat data ukurannya adalah rata-rata(mean), median, modus (mode), dan midrange. Atribut numerik yang paling umum dan efektif dari \"pusat\" dari set data adalah mean (aritmatika). Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi satu set nilai N atau pengamatan, Rata-rata dari nilai pretes dinyatakan dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} Kadang-kadang, setiap nilai x_i x_i dalam satu data dapat dikaitkan dengan bobot w_i w_i untuk i= 1, .., N i= 1, .., N . Bobot tersebut mencerminkan signifikansi, kepentingan, atau frekuensi kejadian yang melekat pada masing masing nilai. Dalam hal ini, kita dapat menghitungnya dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} Meskipun rata-rata adalah jumlah yang sangat berguna untuk menggambarkan kumpulan data, itu tidak selalu cara terbaik untuk mengukur pusat data. Masalah utama dengan mean adalah sensitivitasnya terhadap nilai ekstrim (mis., outlier). Bahkan beberapa nilai ekstrem saja dapat merusak mean. Misalnya, gaji rata-rata di suatu perusahaan mungkin sangat besar didorong oleh beberapa manajer bergaji tinggi. Demikian pula, nilai rata-rata kelas di ujian dapat rata-rata rendah karena beberapa ada beberap skor nilai saja yang sangat rendah. Untuk mengimbangi efek tersebut kita bisa menggunakan rata-rata yang dipangkas (trimmed mean), yang merupakan rata-rata yang diperoleh setelah memangkas nilai paling tinggi dan nilai yang paling rendah. Untuk contoh, kita dapat mengurutkan nilai gaji yang diamati kemudian menghapus 2% atas dan bawah nilai tersebut sebelum menghitung mean. Kita harus menghindari pemotongan bagian yang terlalu besar (seperti 20%) pada kedua ujungnya, karena hal ini dapat mengakibatkan hilangnya informasi yang berharga)","title":"Rata-rata (Mean)"},{"location":"memahami/#median","text":"Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai $(\\frac{N}{2}+1) +(\\frac{N}{2}-1) $ Namun pada data berkelompok, dengan data yang berbentuk kelas interval, kita tidak bisa langsung mengetahui nilai median jika kelas mediannya sudah diketahui dengan formula $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} Contoh Gambar 2.5. Data berkelompok Mode adalah ukuran lain dari kecenderungan sentral. Mode (modus) untuk satu set data adalah nilai yang paling sering terjadi di set. Oleh karena itu, dapat ditentukan untuk atribut kualitatif dan kuantitatif. Dimungkinkan untuk frekuensi terbesar untuk bersesuaian beberapa nilai berbeda, yang menghasilkan lebih dari satu mode. Kumpulan data dengan satu, dua, atau tiga mode masing-masing disebut unimodal, bimodal, dan trimodal. Jika data hanya mengandung nilai data terjadi hanya sekali, maka tidak ada modus Untuk data numerik unimodal yang cukup miring (asimetris), kami memiliki hubungan empiris: \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) Ini menyiratkan bahwa mode untuk kurva frekuensi unimodal yang cukup miring dapat dengan mudah didekati jika nilai rata-rata dan median diketahui.","title":"Median"},{"location":"memahami/#mengukur-sebaran-data","text":"Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data.","title":"Mengukur Sebaran Data"},{"location":"memahami/#rentang-range-quartil-and-rentang-interquartile","text":"Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot","title":"Rentang (Range), Quartil, and Rentang Interquartile"},{"location":"memahami/#variansi-dan-standar-deviasi","text":"Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0","title":"Variansi dan Standar Deviasi"},{"location":"memahami/#skewness","text":"Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness)","title":"Skewness"},{"location":"memahami/#implementasi","text":"Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 )))","title":"Implementasi"},{"location":"memahami/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"memahami/#mengukur-jarak-tipe-numerik","text":"Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak  Tipe Numerik"},{"location":"memahami/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"memahami/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right|","title":"Manhattan distance"},{"location":"memahami/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"memahami/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } }","title":"Average Distance"},{"location":"memahami/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"memahami/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}}","title":"Chord distance"},{"location":"memahami/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"memahami/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } }","title":"Cosine measure"},{"location":"memahami/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"memahami/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"memahami/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical"},{"location":"memahami/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"memahami/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"memahami/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"memahami/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"memahami/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"release-notes/","text":"Release notes \u00b6 Upgrading \u00b6 To upgrade Material to the latest version, use pip : pip install --upgrade mkdocs-material To inspect the currently installed version, use the following command: pip show mkdocs-material Material 3.x to 4.x \u00b6 Material for MkDocs 4.x finally fixes incorrect layout on Chinese systems. The fix includes a mandatory change of the base font-size from 10px to 20px which means all rem values needed to be updated. Within the theme, px to rem calculation is now encapsulated in a new function called px2rem which is part of the SASS code base. If you use Material with custom CSS that is based on rem values, note that those values must now be divided by 2. Now, 1.0rem doesn't map to 10px , but 20px . To learn more about the problem and implications, please refer to the issue in which the problem was discovered and fixed. Material 2.x to 3.x \u00b6 Material for MkDocs 3.x requires MkDocs 1.0 because the way paths are resolved internally changed significantly. Furthermore, pages was renamed to nav , so remember to adjust your mkdocs.yml file. All extended templates should continue to work but in order to make them future-proof the url filter should be introduced on all paths. Please see the official release notes for further guidance. Material 1.x to 2.x \u00b6 Material for MkDocs 2.x requires MkDocs 0.17.1, as this version introduced changes to the way themes can define options. The following variables inside your project's mkdocs.yml need to be renamed: extra.feature becomes theme.feature extra.palette becomes theme.palette extra.font becomes theme.font extra.logo becomes theme.logo Favicon support has been dropped by MkDocs, it must now be defined in theme.favicon (previously site_favicon ). Localization is now separated into theme language and search language. While there can only be a single language on theme-level, the search supports multiple languages which can be separated by commas. See the getting started guide for more guidance. The search tokenizer can now be set through extra.search.tokenizer . Changelog \u00b6 4.4.0 _ June 15, 2019 \u00b6 Added Slovenian translations Reverted template minification in favor of mkdocs-minify-plugin Fixed #1114 : Tabs don't reappear when default font-size is smaller than 16 4.3.1 _ May 23, 2019 \u00b6 Fixed spelling error in Danish translations 4.3.0 _ May 17, 2019 \u00b6 Added support for changing header through metadata title property Added font-display: swap to Google Font loading logic Removed whitespace from templates, saving 4kb ( .7kb gzipped) per request Fixed alignment of repository icons on tablet and desktop 4.2.0 _ April 28, 2019 \u00b6 Added Norwegian (Nynorsk) translations Fixed loss of focus in non-form input elements due to search hotkeys Fixed #1067 : Search hotkeys not working for mobile/tablet screensize Fixed #1068 : Search not correctly aligned for tablet screensize 4.1.2 _ April 16, 2019 \u00b6 Fixed #1072 : HTML tags appearing in navigation link titles 4.1.1 _ March 28, 2019 \u00b6 Fixed minor CSS errors detected during validation 4.1.0 _ March 22, 2019 \u00b6 Fixed #1023 : Search for Asian languages broken after Lunr.js update Fixed #1026 : contenteditable elements loose focus on hotkeys 4.0.2 _ March 1, 2019 \u00b6 Fixed #1012 : HTML character entities appear in search result titles 4.0.1 _ February 13, 2019 \u00b6 Fixed #762 , #816 : Glitch in sidebar when collapsing items Fixed #869 : Automatically expand details before printing 4.0.0 _ February 13, 2019 \u00b6 Added background on hover for table rows Removed Google Tag Manager and reverted to Google Analytics Removed blocks in partials - Jinja doesn't support them Fixed #911 : Chrome breaks layout if system language is Chinese [BREAKING] Fixed #976 : Removed FastClick 3.3.0 _ January 29, 2019 \u00b6 Moved Google Analytics integration into head using Google Tag Manager Fixed #972 : Unicode slugifier breaks table of contents blur on scroll Fixed #974 : Additional links in table of contents break blur on scroll 3.2.0 _ December 28, 2018 \u00b6 Added support for redirects using metadata refresh Fixed #921 : Load Google Analytics snippet asynchronously 3.1.0 _ November 17, 2018 \u00b6 Added support for Progressive Web App Manifest Fixed #915 : Search bug in Safari (upgraded Lunr.js) 3.0.6 _ October 26, 2018 \u00b6 Added Taiwanese translations Fixed #906 : JavaScript code blocks evaluated in search results 3.0.5 _ October 23, 2018 \u00b6 Added Croatian and Indonesian translations Fixed #899 : Skip-to-content link invalid from 2 nd level on Fixed #902 : Missing URL filter in footer for FontAwesome link 3.0.4 _ September 3, 2018 \u00b6 Updated Dutch translations Fixed #856 : Removed preconnect meta tag if Google Fonts are disabled 3.0.3 _ August 7, 2018 \u00b6 Fixed #841 : Additional path levels for extra CSS and JS 3.0.2 _ August 6, 2018 \u00b6 Fixed #839 : Lunr.js stemmer imports incorrect 3.0.1 _ August 5, 2018 \u00b6 Fixed #838 : Search result links incorrect 3.0.0 _ August 5, 2018 \u00b6 Upgraded MkDocs to 1.0 [BREAKING] Upgraded Python in official Docker image to 3.6 Added Serbian and Serbo-Croatian translations 2.9.4 _ July 29, 2018 \u00b6 Fixed build error after MkDocs upgrade 2.9.3 _ July 29, 2018 \u00b6 Added link to home for logo in drawer Fixed dependency problems between MkDocs and Tornado 2.9.2 _ June 29, 2018 \u00b6 Added Hindi and Czech translations 2.9.1 _ June 18, 2018 \u00b6 Added support for different spellings for theme color Fixed #799 : Added support for web font minification in production Fixed #800 : Added .highlighttable as an alias for .codehilitetable 2.9.0 _ June 13, 2018 \u00b6 Added support for theme color on Android Fixed #796 : Rendering of nested tabbed code blocks 2.8.0 _ June 10, 2018 \u00b6 Added support for grouping code blocks with tabs Added Material and FontAwesome icon fonts to distribution files (GDPR) Added note on compliance with GDPR Added Slovak translations Fixed #790 : Prefixed id attributes with __ to avoid name clashes 2.7.3 _ April 26, 2018 \u00b6 Added Finnish translations 2.7.2 _ April 9, 2018 \u00b6 Fixed rendering issue for details on Edge 2.7.1 _ March 21, 2018 \u00b6 Added Galician translations Fixed #730 : Scroll chasing error on home page if Disqus is enabled Fixed #736 : Reset drawer and search upon back button invocation 2.7.0 _ March 6, 2018 \u00b6 Added ability to set absolute URL for logo Added Hebrew translations 2.6.6 _ February 22, 2018 \u00b6 Added preconnect for Google Fonts for faster loading Fixed #710 : With tabs sidebar disappears if JavaScript is not available 2.6.5 _ February 22, 2018 \u00b6 Reverted --dev-addr flag removal from Dockerfile 2.6.4 _ February 21, 2018 \u00b6 Added Catalan translations Fixed incorrect margins for buttons in Firefox and Safari Replaced package manager yarn with npm 5.6 Reverted GitHub stars rounding method Removed --dev-addr flag from Dockerfile for Windows compatibility 2.6.3 _ February 18, 2018 \u00b6 Added Vietnamese translations 2.6.2 _ February 12, 2018 \u00b6 Added Arabic translations Fixed incorrect rounding of amount of GitHub stars Fixed double-layered borders for tables 2.6.1 _ February 11, 2018 \u00b6 Added ability to override Disqus integration using metadata Fixed #690 : Duplicate slashes in source file URLs Fixed #696 : Active page highlight not working with default palette Adjusted German translations 2.6.0 _ February 2, 2018 \u00b6 Moved default search configuration to default translation (English) Added support to automatically set text direction from translation Added support to disable search stop word filter in translation Added support to disable search trimmer in translation Added Persian translations Fixed support for Polish search Fixed disappearing GitHub, GitLab and Bitbucket repository icons 2.5.5 _ January 31, 2018 \u00b6 Added Hungarian translations 2.5.4 _ January 29, 2018 \u00b6 Fixed #683 : gh-deploy fails inside Docker 2.5.3 _ January 25, 2018 \u00b6 Added Ukrainian translations 2.5.2 _ January 22, 2018 \u00b6 Added default search language mappings for all localizations Fixed #673 : Error loading non-existent search language Fixed #675 : Uncaught reference error when search plugin disabled 2.5.1 _ January 20, 2018 \u00b6 Fixed permalink for main headline Improved missing translation handling with English as a fallback Improved accessibility with skip-to-content link 2.5.0 _ January 13, 2018 \u00b6 Added support for right-to-left languages 2.4.0 _ January 11, 2018 \u00b6 Added focus state for clipboard buttons Fixed #400 : Search bar steals tab focus Fixed search not closing on Enter when result is selected Fixed search not closing when losing focus due to Tab Fixed collapsed navigation links getting focus Fixed outline being cut off on Tab focus of navigation links Fixed bug with first search result navigation being ignored Removed search result navigation via Tab (use Up and Down ) Removed outline resets for links Improved general tabbing behavior on desktop 2.3.0 _ January 9, 2018 \u00b6 Added example (synonym: snippet ) style for Admonition Added synonym abstract for summary style for Admonition 2.2.6 _ December 27, 2017 \u00b6 Added Turkish translations Fixed unclickable area below header in case JavaScript is not available 2.2.5 _ December 18, 2017 \u00b6 Fixed #639 : Broken default favicon 2.2.4 _ December 18, 2017 \u00b6 Fixed #638 : Build breaks with Jinja < 2.9 2.2.3 _ December 13, 2017 \u00b6 Fixed #630 : Admonition sets padding on any last child Adjusted Chinese (Traditional) translations 2.2.2 _ December 8, 2017 \u00b6 Added Dutch translations Adjusted targeted link and footnote offsets Simplified Admonition styles and fixed padding bug 2.2.1 _ December 2, 2017 \u00b6 Fixed #616 : Minor styling error with title-only admonition blocks Removed border for table of contents and improved spacing 2.2.0 _ November 22, 2017 \u00b6 Added support for hero teaser Added Portuguese translations Fixed #586 : Footnote backref target offset regression Fixed #605 : Search stemmers not correctly loaded 2.1.1 _ November 21, 2017 \u00b6 Replaced deprecated babel-preset-es2015 with babel-preset-env Refactored Gulp build pipeline with Webpack Removed right border on sidebars Fixed broken color transition on header 2.1.0 _ November 19, 2017 \u00b6 Added support for white as a primary color Added support for sliding site name and title Fixed redundant clipboard button when using line numbers on code blocks Improved header appearance by making it taller Improved tabs appearance Improved CSS customizability by leveraging inheritance Removed scroll shadows via background-attachment 2.0.4 _ November 5, 2017 \u00b6 Fixed details not opening with footnote reference 2.0.3 _ November 5, 2017 \u00b6 Added Japanese translations Fixed #540 : Jumping to anchor inside details doesn't open it Fixed active link colors in footer 2.0.2 _ November 1, 2017 \u00b6 Added Russian translations Fixed #542 : Horizontal scrollbar between 1220px and 1234px Fixed #553 : Metadata values only rendering first character Fixed #558 : Flash of unstyled content Fixed favicon regression caused by deprecation upstream 2.0.1 _ October 31, 2017 \u00b6 Fixed error when initializing search Fixed styles for link to edit the current page Fixed styles on nested admonition in details 2.0.0 _ October 31, 2017 \u00b6 Added support for MkDocs 0.17.1 theme configuration options Added support for easier configuration of search tokenizer Added support to disable search Added Korean translations Removed support for MkDocs 0.16.x [BREAKING] 1.12.2 _ October 26, 2017 \u00b6 Added Italian, Norwegian, French and Chinese translations 1.12.1 _ October 22, 2017 \u00b6 Added Polish, Swedish and Spanish translations Improved downward compatibility with custom partials Temporarily pinned MkDocs version within Docker image to 0.16.3 Fixed #519 : Missing theme configuration file 1.12.0 _ October 20, 2017 \u00b6 Added support for setting language(s) via mkdocs.yml Added support for default localization Added German and Danish translations Fixed #374 : Search bar misalignment on big screens 1.11.0 _ October 19, 2017 \u00b6 Added localization to clipboard Refactored localization logic 1.10.4 _ October 18, 2017 \u00b6 Improved print styles of code blocks Improved search UX (don't close on enter if no selection) Fixed #495 : Vertical scrollbar on short pages 1.10.3 _ October 11, 2017 \u00b6 Fixed #484 : Vertical scrollbar on some MathJax formulas Fixed #483 : Footnote backref target offset regression 1.10.2 _ October 6, 2017 \u00b6 Fixed #468 : Sidebar shows scrollbar if content is shorter (in Safari) 1.10.1 _ September 14, 2017 \u00b6 Fixed #455 : Bold code blocks rendered with normal font weight 1.10.0 _ September 1, 2017 \u00b6 Added support to make logo default icon configurable Fixed uninitialized overflow scrolling on main pane for iOS Fixed error in mobile navigation in case JavaScript is not available Fixed incorrect color transition for nested panes in mobile navigation Improved checkbox styles for Tasklist from PyMdown Extension package 1.9.0 _ August 29, 2017 \u00b6 Added info (synonym: todo ) style for Admonition Added question (synonym: help , faq ) style for Admonition Added support for Details from PyMdown Extensions package Improved Admonition styles to match Details Improved styles for social links in footer Replaced ligatures with Unicode code points to avoid broken layout Upgraded PyMdown Extensions package dependency to >= 3.4 1.8.1 _ August 7, 2017 \u00b6 Fixed #421 : Missing pagination for GitHub API 1.8.0 _ August 2, 2017 \u00b6 Added support for lazy-loading of search results for better performance Added support for customization of search tokenizer/separator Fixed #424 : Search doesn't handle capital letters anymore Fixed #419 : Search doesn't work on whole words 1.7.5 _ July 25, 2017 \u00b6 Fixed #398 : Forms broken due to search shortcuts Improved search overall user experience Improved search matching and highlighting Improved search accessibility 1.7.4 _ June 21, 2017 \u00b6 Fixed functional link colors in table of contents for active palette Fixed #368 : Compatibility issues with IE11 1.7.3 _ June 7, 2017 \u00b6 Fixed error when setting language to Japanese for site search 1.7.2 _ June 6, 2017 \u00b6 Fixed offset of search box when repo_url is not set Fixed non-disappearing tooltip 1.7.1 _ June 1, 2017 \u00b6 Fixed wrong z-index order of header, overlay and drawer Fixed wrong offset of targeted footnote back references 1.7.0 _ June 1, 2017 \u00b6 Added \"copy to clipboard\" buttons to code blocks Added support for multilingual site search Fixed search term highlighting for non-latin languages 1.6.4 _ May 24, 2017 \u00b6 Fixed #337 : JavaScript error for GitHub organization URLs 1.6.3 _ May 16, 2017 \u00b6 Fixed #329 : Broken source stats for private or unknown GitHub repos 1.6.2 _ May 15, 2017 \u00b6 Fixed #316 : Fatal error for git clone on Windows Fixed #320 : Chrome 58 creates double underline for abbr tags Fixed #323 : Ligatures rendered inside code blocks Fixed miscalculated sidebar height due to missing margin collapse Changed deprecated MathJax CDN to Cloudflare 1.6.1 _ April 23, 2017 \u00b6 Fixed following of active/focused element if search input is focused Fixed layer order of search component elements 1.6.0 _ April 22, 2017 \u00b6 Added build test for Docker image on Travis Added search overlay for better user experience (focus) Added language from localizations to html tag Fixed #270 : source links broken for absolute URLs Fixed missing top spacing for first targeted element in content Fixed too small footnote divider when using larger font sizes 1.5.5 _ April 20, 2017 \u00b6 Fixed #282 : Browser search ( Meta + F ) is hijacked 1.5.4 _ April 8, 2017 \u00b6 Fixed broken highlighting for two or more search terms Fixed missing search results when only a h1 is present Fixed unresponsive overlay on Android 1.5.3 _ April 7, 2017 \u00b6 Fixed deprecated calls for template variables Fixed wrong palette color for focused search result Fixed JavaScript errors on 404 page Fixed missing top spacing on 404 page Fixed missing right spacing on overflow of source container 1.5.2 _ April 5, 2017 \u00b6 Added requirements as explicit dependencies in setup.py Fixed non-synchronized transitions in search form 1.5.1 _ March 30, 2017 \u00b6 Fixed rendering and offset of targetted footnotes Fixed #238 : Link on logo is not set to site_url 1.5.0 _ March 24, 2017 \u00b6 Added support for localization of search placeholder Added keyboard events for quick access of search Added keyboard events for search control Added opacity on hover for search buttons Added git hook to skip CI build on non-src changes Fixed non-resetting search placeholder when input is cleared Fixed error for unescaped parentheses in search term Fixed #229 : Button to clear search missing Fixed #231 : Escape key doesn't exit search Removed old-style figures from font feature settings 1.4.1 _ March 16, 2017 \u00b6 Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE) 1.4.0 _ March 16, 2017 \u00b6 Added support for grouping searched sections by documents Added support for highlighting of search terms Added support for localization of search results Fixed #216 : table of contents icon doesn't show if h1 is not present Reworked style and layout of search results for better usability 1.3.0 _ March 11, 2017 \u00b6 Added support for page-specific title and description using metadata Added support for linking source files to documentation Fixed jitter and offset of sidebar when zooming browser Fixed incorrectly initialized tablet sidebar height Fixed regression for #1 : GitHub stars break if repo_url ends with a / Fixed undesired white line below copyright footer due to base font scaling Fixed issue with whitespace in path for scripts Fixed #205 : support non-fixed (static) header Refactored footnote references for better visibility Reduced repaints to a minimum for non-tabs configuration Reduced contrast of edit button (slightly) 1.2.0 _ March 3, 2017 \u00b6 Added quote (synonym: cite ) style for Admonition Added help message to build pipeline Fixed wrong navigation link colors when applying palette Fixed #197 : Link missing in tabs navigation on deeply nested items Removed unnecessary dev dependencies 1.1.1 _ February 26, 2017 \u00b6 Fixed incorrectly displayed nested lists when using tabs 1.1.0 _ February 26, 2017 \u00b6 Added tabs navigation feature (optional) Added Disqus integration (optional) Added a high resolution Favicon with the new logo Added static type checking using Facebook's Flow Fixed #173 : Dictionary elements have no bottom spacing Fixed #175 : Tables cannot be set to 100% width Fixed race conditions in build related to asset revisioning Fixed accidentally re-introduced Permalink on top-level headline Fixed alignment of logo in drawer on IE11 Refactored styles related to tables Refactored and automated Docker build and PyPI release Refactored build scripts 1.0.5 _ February 18, 2017 \u00b6 Fixed #153 : Sidebar flows out of constrained area in Chrome 56 Fixed #159 : Footer jitter due to JavaScript if content is short 1.0.4 _ February 16, 2017 \u00b6 Fixed #142 : Documentation build errors if h1 is defined as raw HTML Fixed #164 : PyPI release does not build and install Fixed offsets of targeted headlines Increased sidebar font size by 0.12rem 1.0.3 _ January 22, 2017 \u00b6 Fixed #117 : Table of contents items don't blur on fast scrolling Refactored sidebar positioning logic Further reduction of repaints 1.0.2 _ January 15, 2017 \u00b6 Fixed #108 : Horizontal scrollbar in content area 1.0.1 _ January 14, 2017 \u00b6 Fixed massive repaints happening when scrolling Fixed footer back reference positions in case of overflow Fixed header logo from showing when the menu icon is rendered Changed scrollbar behavior to only show when content overflows 1.0.0 _ January 13, 2017 \u00b6 Introduced Webpack for more sophisticated JavaScript bundling Introduced ESLint and Stylelint for code style checks Introduced more accurate Material Design colors and shadows Introduced modular scales for harmonic font sizing Introduced git-hooks for better development workflow Rewrite of CSS using the BEM methodology and SassDoc guidelines Rewrite of JavaScript using ES6 and Babel as a transpiler Rewrite of Admonition, Permalinks and CodeHilite integration Rewrite of the complete typographical system Rewrite of Gulp asset pipeline in ES6 and separation of tasks Removed Bower as a dependency in favor of NPM Removed custom icon build in favor of the Material Design iconset Removed _blank targets on links due to vulnerability: http://bit.ly/1Mk2Rtw Removed unversioned assets from build directory Restructured templates into base templates and partials Added build and watch scripts in package.json Added support for Metadata and Footnotes Markdown extensions Added support for PyMdown Extensions package Added support for collapsible sections in navigation Added support for separate table of contents Added support for better accessibility through REM-based layout Added icons for GitHub, GitLab and BitBucket integrations Added more detailed documentation on specimen, extensions etc. Added a 404.html error page for deployment on GitHub Pages Fixed live reload chain in watch mode when saving a template Fixed variable references to work with MkDocs 0.16 0.2.4 _ June 26, 2016 \u00b6 Fixed improperly set default favicon Fixed #33 : Protocol relative URL for webfonts doesn't work with file:// Fixed #34 : IE11 on Windows 7 doesn't honor max-width on main tag Fixed #35 : Add styling for blockquotes 0.2.3 _ May 16, 2016 \u00b6 Fixed #25 : Highlight inline fenced blocks Fixed #26 : Better highlighting for keystrokes Fixed #30 : Suboptimal syntax highlighting for PHP 0.2.2 _ March 20, 2016 \u00b6 Fixed #15 : Document Pygments dependency for CodeHilite Fixed #16 : Favicon could not be set through mkdocs.yml Fixed #17 : Put version into own container for styling Fixed #20 : Fix rounded borders for tables 0.2.1 _ March 12, 2016 \u00b6 Fixed #10 : Invisible header after closing search bar with ESC key Fixed #13 : Table cells don't wrap Fixed empty list in table of contents when no headline is defined Corrected wrong path for static asset monitoring in Gulpfile.js Set up tracking of site search for Google Analytics 0.2.0 _ February 24, 2016 \u00b6 Fixed #6 : Include multiple color palettes via mkdocs.yml Fixed #7 : Better colors for links inside admonition notes and warnings Fixed #9 : Text for prev/next footer navigation should be customizable Refactored templates (replaced if / else with modifiers where possible) 0.1.3 _ February 21, 2016 \u00b6 Fixed #3 : Ordered lists within an unordered list have ::before content Fixed #4 : Click on Logo/Title without Github-Repository: \"None\" Fixed #5 : Page without headlines renders empty list in table of contents Moved Modernizr to top to ensure basic usability in IE8 0.1.2 _ February 16, 2016 \u00b6 Fixed styles for deep navigational hierarchies Fixed webfont delivery problem when hosted in subdirectories Fixed print styles in mobile/tablet configuration Added option to configure fonts in mkdocs.yml with fallbacks Changed styles for admonition notes and warnings Set download link to latest version if available Set up tracking of outgoing links and actions for Google Analytics 0.1.1 _ February 11, 2016 \u00b6 Fixed #1 : GitHub stars don't work if the repo_url ends with a / Updated NPM and Bower dependencies to most recent versions Changed footer/copyright link to Material theme to GitHub pages Made MkDocs building/serving in build process optional Set up continuous integration with Travis 0.1.0 _ February 9, 2016 \u00b6 Initial release","title":"Release notes"},{"location":"release-notes/#release-notes","text":"","title":"Release notes"},{"location":"release-notes/#upgrading","text":"To upgrade Material to the latest version, use pip : pip install --upgrade mkdocs-material To inspect the currently installed version, use the following command: pip show mkdocs-material","title":"Upgrading"},{"location":"release-notes/#material-3x-to-4x","text":"Material for MkDocs 4.x finally fixes incorrect layout on Chinese systems. The fix includes a mandatory change of the base font-size from 10px to 20px which means all rem values needed to be updated. Within the theme, px to rem calculation is now encapsulated in a new function called px2rem which is part of the SASS code base. If you use Material with custom CSS that is based on rem values, note that those values must now be divided by 2. Now, 1.0rem doesn't map to 10px , but 20px . To learn more about the problem and implications, please refer to the issue in which the problem was discovered and fixed.","title":"Material 3.x to 4.x"},{"location":"release-notes/#material-2x-to-3x","text":"Material for MkDocs 3.x requires MkDocs 1.0 because the way paths are resolved internally changed significantly. Furthermore, pages was renamed to nav , so remember to adjust your mkdocs.yml file. All extended templates should continue to work but in order to make them future-proof the url filter should be introduced on all paths. Please see the official release notes for further guidance.","title":"Material 2.x to 3.x"},{"location":"release-notes/#material-1x-to-2x","text":"Material for MkDocs 2.x requires MkDocs 0.17.1, as this version introduced changes to the way themes can define options. The following variables inside your project's mkdocs.yml need to be renamed: extra.feature becomes theme.feature extra.palette becomes theme.palette extra.font becomes theme.font extra.logo becomes theme.logo Favicon support has been dropped by MkDocs, it must now be defined in theme.favicon (previously site_favicon ). Localization is now separated into theme language and search language. While there can only be a single language on theme-level, the search supports multiple languages which can be separated by commas. See the getting started guide for more guidance. The search tokenizer can now be set through extra.search.tokenizer .","title":"Material 1.x to 2.x"},{"location":"release-notes/#changelog","text":"","title":"Changelog"},{"location":"release-notes/#440-_-june-15-2019","text":"Added Slovenian translations Reverted template minification in favor of mkdocs-minify-plugin Fixed #1114 : Tabs don't reappear when default font-size is smaller than 16","title":"4.4.0 _ June 15, 2019"},{"location":"release-notes/#431-_-may-23-2019","text":"Fixed spelling error in Danish translations","title":"4.3.1 _ May 23, 2019"},{"location":"release-notes/#430-_-may-17-2019","text":"Added support for changing header through metadata title property Added font-display: swap to Google Font loading logic Removed whitespace from templates, saving 4kb ( .7kb gzipped) per request Fixed alignment of repository icons on tablet and desktop","title":"4.3.0 _ May 17, 2019"},{"location":"release-notes/#420-_-april-28-2019","text":"Added Norwegian (Nynorsk) translations Fixed loss of focus in non-form input elements due to search hotkeys Fixed #1067 : Search hotkeys not working for mobile/tablet screensize Fixed #1068 : Search not correctly aligned for tablet screensize","title":"4.2.0 _ April 28, 2019"},{"location":"release-notes/#412-_-april-16-2019","text":"Fixed #1072 : HTML tags appearing in navigation link titles","title":"4.1.2 _ April 16, 2019"},{"location":"release-notes/#411-_-march-28-2019","text":"Fixed minor CSS errors detected during validation","title":"4.1.1 _ March 28, 2019"},{"location":"release-notes/#410-_-march-22-2019","text":"Fixed #1023 : Search for Asian languages broken after Lunr.js update Fixed #1026 : contenteditable elements loose focus on hotkeys","title":"4.1.0 _ March 22, 2019"},{"location":"release-notes/#402-_-march-1-2019","text":"Fixed #1012 : HTML character entities appear in search result titles","title":"4.0.2 _ March 1, 2019"},{"location":"release-notes/#401-_-february-13-2019","text":"Fixed #762 , #816 : Glitch in sidebar when collapsing items Fixed #869 : Automatically expand details before printing","title":"4.0.1 _ February 13, 2019"},{"location":"release-notes/#400-_-february-13-2019","text":"Added background on hover for table rows Removed Google Tag Manager and reverted to Google Analytics Removed blocks in partials - Jinja doesn't support them Fixed #911 : Chrome breaks layout if system language is Chinese [BREAKING] Fixed #976 : Removed FastClick","title":"4.0.0 _ February 13, 2019"},{"location":"release-notes/#330-_-january-29-2019","text":"Moved Google Analytics integration into head using Google Tag Manager Fixed #972 : Unicode slugifier breaks table of contents blur on scroll Fixed #974 : Additional links in table of contents break blur on scroll","title":"3.3.0 _ January 29, 2019"},{"location":"release-notes/#320-_-december-28-2018","text":"Added support for redirects using metadata refresh Fixed #921 : Load Google Analytics snippet asynchronously","title":"3.2.0 _ December 28, 2018"},{"location":"release-notes/#310-_-november-17-2018","text":"Added support for Progressive Web App Manifest Fixed #915 : Search bug in Safari (upgraded Lunr.js)","title":"3.1.0 _ November 17, 2018"},{"location":"release-notes/#306-_-october-26-2018","text":"Added Taiwanese translations Fixed #906 : JavaScript code blocks evaluated in search results","title":"3.0.6 _ October 26, 2018"},{"location":"release-notes/#305-_-october-23-2018","text":"Added Croatian and Indonesian translations Fixed #899 : Skip-to-content link invalid from 2 nd level on Fixed #902 : Missing URL filter in footer for FontAwesome link","title":"3.0.5 _ October 23, 2018"},{"location":"release-notes/#304-_-september-3-2018","text":"Updated Dutch translations Fixed #856 : Removed preconnect meta tag if Google Fonts are disabled","title":"3.0.4 _ September 3, 2018"},{"location":"release-notes/#303-_-august-7-2018","text":"Fixed #841 : Additional path levels for extra CSS and JS","title":"3.0.3 _ August 7, 2018"},{"location":"release-notes/#302-_-august-6-2018","text":"Fixed #839 : Lunr.js stemmer imports incorrect","title":"3.0.2 _ August 6, 2018"},{"location":"release-notes/#301-_-august-5-2018","text":"Fixed #838 : Search result links incorrect","title":"3.0.1 _ August 5, 2018"},{"location":"release-notes/#300-_-august-5-2018","text":"Upgraded MkDocs to 1.0 [BREAKING] Upgraded Python in official Docker image to 3.6 Added Serbian and Serbo-Croatian translations","title":"3.0.0 _ August 5, 2018"},{"location":"release-notes/#294-_-july-29-2018","text":"Fixed build error after MkDocs upgrade","title":"2.9.4 _ July 29, 2018"},{"location":"release-notes/#293-_-july-29-2018","text":"Added link to home for logo in drawer Fixed dependency problems between MkDocs and Tornado","title":"2.9.3 _ July 29, 2018"},{"location":"release-notes/#292-_-june-29-2018","text":"Added Hindi and Czech translations","title":"2.9.2 _ June 29, 2018"},{"location":"release-notes/#291-_-june-18-2018","text":"Added support for different spellings for theme color Fixed #799 : Added support for web font minification in production Fixed #800 : Added .highlighttable as an alias for .codehilitetable","title":"2.9.1 _ June 18, 2018"},{"location":"release-notes/#290-_-june-13-2018","text":"Added support for theme color on Android Fixed #796 : Rendering of nested tabbed code blocks","title":"2.9.0 _ June 13, 2018"},{"location":"release-notes/#280-_-june-10-2018","text":"Added support for grouping code blocks with tabs Added Material and FontAwesome icon fonts to distribution files (GDPR) Added note on compliance with GDPR Added Slovak translations Fixed #790 : Prefixed id attributes with __ to avoid name clashes","title":"2.8.0 _ June 10, 2018"},{"location":"release-notes/#273-_-april-26-2018","text":"Added Finnish translations","title":"2.7.3 _ April 26, 2018"},{"location":"release-notes/#272-_-april-9-2018","text":"Fixed rendering issue for details on Edge","title":"2.7.2 _ April 9, 2018"},{"location":"release-notes/#271-_-march-21-2018","text":"Added Galician translations Fixed #730 : Scroll chasing error on home page if Disqus is enabled Fixed #736 : Reset drawer and search upon back button invocation","title":"2.7.1 _ March 21, 2018"},{"location":"release-notes/#270-_-march-6-2018","text":"Added ability to set absolute URL for logo Added Hebrew translations","title":"2.7.0 _ March 6, 2018"},{"location":"release-notes/#266-_-february-22-2018","text":"Added preconnect for Google Fonts for faster loading Fixed #710 : With tabs sidebar disappears if JavaScript is not available","title":"2.6.6 _ February 22, 2018"},{"location":"release-notes/#265-_-february-22-2018","text":"Reverted --dev-addr flag removal from Dockerfile","title":"2.6.5 _ February 22, 2018"},{"location":"release-notes/#264-_-february-21-2018","text":"Added Catalan translations Fixed incorrect margins for buttons in Firefox and Safari Replaced package manager yarn with npm 5.6 Reverted GitHub stars rounding method Removed --dev-addr flag from Dockerfile for Windows compatibility","title":"2.6.4 _ February 21, 2018"},{"location":"release-notes/#263-_-february-18-2018","text":"Added Vietnamese translations","title":"2.6.3 _ February 18, 2018"},{"location":"release-notes/#262-_-february-12-2018","text":"Added Arabic translations Fixed incorrect rounding of amount of GitHub stars Fixed double-layered borders for tables","title":"2.6.2 _ February 12, 2018"},{"location":"release-notes/#261-_-february-11-2018","text":"Added ability to override Disqus integration using metadata Fixed #690 : Duplicate slashes in source file URLs Fixed #696 : Active page highlight not working with default palette Adjusted German translations","title":"2.6.1 _ February 11, 2018"},{"location":"release-notes/#260-_-february-2-2018","text":"Moved default search configuration to default translation (English) Added support to automatically set text direction from translation Added support to disable search stop word filter in translation Added support to disable search trimmer in translation Added Persian translations Fixed support for Polish search Fixed disappearing GitHub, GitLab and Bitbucket repository icons","title":"2.6.0 _ February 2, 2018"},{"location":"release-notes/#255-_-january-31-2018","text":"Added Hungarian translations","title":"2.5.5 _ January 31, 2018"},{"location":"release-notes/#254-_-january-29-2018","text":"Fixed #683 : gh-deploy fails inside Docker","title":"2.5.4 _ January 29, 2018"},{"location":"release-notes/#253-_-january-25-2018","text":"Added Ukrainian translations","title":"2.5.3 _ January 25, 2018"},{"location":"release-notes/#252-_-january-22-2018","text":"Added default search language mappings for all localizations Fixed #673 : Error loading non-existent search language Fixed #675 : Uncaught reference error when search plugin disabled","title":"2.5.2 _ January 22, 2018"},{"location":"release-notes/#251-_-january-20-2018","text":"Fixed permalink for main headline Improved missing translation handling with English as a fallback Improved accessibility with skip-to-content link","title":"2.5.1 _ January 20, 2018"},{"location":"release-notes/#250-_-january-13-2018","text":"Added support for right-to-left languages","title":"2.5.0 _ January 13, 2018"},{"location":"release-notes/#240-_-january-11-2018","text":"Added focus state for clipboard buttons Fixed #400 : Search bar steals tab focus Fixed search not closing on Enter when result is selected Fixed search not closing when losing focus due to Tab Fixed collapsed navigation links getting focus Fixed outline being cut off on Tab focus of navigation links Fixed bug with first search result navigation being ignored Removed search result navigation via Tab (use Up and Down ) Removed outline resets for links Improved general tabbing behavior on desktop","title":"2.4.0 _ January 11, 2018"},{"location":"release-notes/#230-_-january-9-2018","text":"Added example (synonym: snippet ) style for Admonition Added synonym abstract for summary style for Admonition","title":"2.3.0 _ January 9, 2018"},{"location":"release-notes/#226-_-december-27-2017","text":"Added Turkish translations Fixed unclickable area below header in case JavaScript is not available","title":"2.2.6 _ December 27, 2017"},{"location":"release-notes/#225-_-december-18-2017","text":"Fixed #639 : Broken default favicon","title":"2.2.5 _ December 18, 2017"},{"location":"release-notes/#224-_-december-18-2017","text":"Fixed #638 : Build breaks with Jinja < 2.9","title":"2.2.4 _ December 18, 2017"},{"location":"release-notes/#223-_-december-13-2017","text":"Fixed #630 : Admonition sets padding on any last child Adjusted Chinese (Traditional) translations","title":"2.2.3 _ December 13, 2017"},{"location":"release-notes/#222-_-december-8-2017","text":"Added Dutch translations Adjusted targeted link and footnote offsets Simplified Admonition styles and fixed padding bug","title":"2.2.2 _ December 8, 2017"},{"location":"release-notes/#221-_-december-2-2017","text":"Fixed #616 : Minor styling error with title-only admonition blocks Removed border for table of contents and improved spacing","title":"2.2.1 _ December 2, 2017"},{"location":"release-notes/#220-_-november-22-2017","text":"Added support for hero teaser Added Portuguese translations Fixed #586 : Footnote backref target offset regression Fixed #605 : Search stemmers not correctly loaded","title":"2.2.0 _ November 22, 2017"},{"location":"release-notes/#211-_-november-21-2017","text":"Replaced deprecated babel-preset-es2015 with babel-preset-env Refactored Gulp build pipeline with Webpack Removed right border on sidebars Fixed broken color transition on header","title":"2.1.1 _ November 21, 2017"},{"location":"release-notes/#210-_-november-19-2017","text":"Added support for white as a primary color Added support for sliding site name and title Fixed redundant clipboard button when using line numbers on code blocks Improved header appearance by making it taller Improved tabs appearance Improved CSS customizability by leveraging inheritance Removed scroll shadows via background-attachment","title":"2.1.0 _ November 19, 2017"},{"location":"release-notes/#204-_-november-5-2017","text":"Fixed details not opening with footnote reference","title":"2.0.4 _ November 5, 2017"},{"location":"release-notes/#203-_-november-5-2017","text":"Added Japanese translations Fixed #540 : Jumping to anchor inside details doesn't open it Fixed active link colors in footer","title":"2.0.3 _ November 5, 2017"},{"location":"release-notes/#202-_-november-1-2017","text":"Added Russian translations Fixed #542 : Horizontal scrollbar between 1220px and 1234px Fixed #553 : Metadata values only rendering first character Fixed #558 : Flash of unstyled content Fixed favicon regression caused by deprecation upstream","title":"2.0.2 _ November 1, 2017"},{"location":"release-notes/#201-_-october-31-2017","text":"Fixed error when initializing search Fixed styles for link to edit the current page Fixed styles on nested admonition in details","title":"2.0.1 _ October 31, 2017"},{"location":"release-notes/#200-_-october-31-2017","text":"Added support for MkDocs 0.17.1 theme configuration options Added support for easier configuration of search tokenizer Added support to disable search Added Korean translations Removed support for MkDocs 0.16.x [BREAKING]","title":"2.0.0 _ October 31, 2017"},{"location":"release-notes/#1122-_-october-26-2017","text":"Added Italian, Norwegian, French and Chinese translations","title":"1.12.2 _ October 26, 2017"},{"location":"release-notes/#1121-_-october-22-2017","text":"Added Polish, Swedish and Spanish translations Improved downward compatibility with custom partials Temporarily pinned MkDocs version within Docker image to 0.16.3 Fixed #519 : Missing theme configuration file","title":"1.12.1 _ October 22, 2017"},{"location":"release-notes/#1120-_-october-20-2017","text":"Added support for setting language(s) via mkdocs.yml Added support for default localization Added German and Danish translations Fixed #374 : Search bar misalignment on big screens","title":"1.12.0 _ October 20, 2017"},{"location":"release-notes/#1110-_-october-19-2017","text":"Added localization to clipboard Refactored localization logic","title":"1.11.0 _ October 19, 2017"},{"location":"release-notes/#1104-_-october-18-2017","text":"Improved print styles of code blocks Improved search UX (don't close on enter if no selection) Fixed #495 : Vertical scrollbar on short pages","title":"1.10.4 _ October 18, 2017"},{"location":"release-notes/#1103-_-october-11-2017","text":"Fixed #484 : Vertical scrollbar on some MathJax formulas Fixed #483 : Footnote backref target offset regression","title":"1.10.3 _ October 11, 2017"},{"location":"release-notes/#1102-_-october-6-2017","text":"Fixed #468 : Sidebar shows scrollbar if content is shorter (in Safari)","title":"1.10.2 _ October 6, 2017"},{"location":"release-notes/#1101-_-september-14-2017","text":"Fixed #455 : Bold code blocks rendered with normal font weight","title":"1.10.1 _ September 14, 2017"},{"location":"release-notes/#1100-_-september-1-2017","text":"Added support to make logo default icon configurable Fixed uninitialized overflow scrolling on main pane for iOS Fixed error in mobile navigation in case JavaScript is not available Fixed incorrect color transition for nested panes in mobile navigation Improved checkbox styles for Tasklist from PyMdown Extension package","title":"1.10.0 _ September 1, 2017"},{"location":"release-notes/#190-_-august-29-2017","text":"Added info (synonym: todo ) style for Admonition Added question (synonym: help , faq ) style for Admonition Added support for Details from PyMdown Extensions package Improved Admonition styles to match Details Improved styles for social links in footer Replaced ligatures with Unicode code points to avoid broken layout Upgraded PyMdown Extensions package dependency to >= 3.4","title":"1.9.0 _ August 29, 2017"},{"location":"release-notes/#181-_-august-7-2017","text":"Fixed #421 : Missing pagination for GitHub API","title":"1.8.1 _ August 7, 2017"},{"location":"release-notes/#180-_-august-2-2017","text":"Added support for lazy-loading of search results for better performance Added support for customization of search tokenizer/separator Fixed #424 : Search doesn't handle capital letters anymore Fixed #419 : Search doesn't work on whole words","title":"1.8.0 _ August 2, 2017"},{"location":"release-notes/#175-_-july-25-2017","text":"Fixed #398 : Forms broken due to search shortcuts Improved search overall user experience Improved search matching and highlighting Improved search accessibility","title":"1.7.5 _ July 25, 2017"},{"location":"release-notes/#174-_-june-21-2017","text":"Fixed functional link colors in table of contents for active palette Fixed #368 : Compatibility issues with IE11","title":"1.7.4 _ June 21, 2017"},{"location":"release-notes/#173-_-june-7-2017","text":"Fixed error when setting language to Japanese for site search","title":"1.7.3 _ June 7, 2017"},{"location":"release-notes/#172-_-june-6-2017","text":"Fixed offset of search box when repo_url is not set Fixed non-disappearing tooltip","title":"1.7.2 _ June 6, 2017"},{"location":"release-notes/#171-_-june-1-2017","text":"Fixed wrong z-index order of header, overlay and drawer Fixed wrong offset of targeted footnote back references","title":"1.7.1 _ June 1, 2017"},{"location":"release-notes/#170-_-june-1-2017","text":"Added \"copy to clipboard\" buttons to code blocks Added support for multilingual site search Fixed search term highlighting for non-latin languages","title":"1.7.0 _ June 1, 2017"},{"location":"release-notes/#164-_-may-24-2017","text":"Fixed #337 : JavaScript error for GitHub organization URLs","title":"1.6.4 _ May 24, 2017"},{"location":"release-notes/#163-_-may-16-2017","text":"Fixed #329 : Broken source stats for private or unknown GitHub repos","title":"1.6.3 _ May 16, 2017"},{"location":"release-notes/#162-_-may-15-2017","text":"Fixed #316 : Fatal error for git clone on Windows Fixed #320 : Chrome 58 creates double underline for abbr tags Fixed #323 : Ligatures rendered inside code blocks Fixed miscalculated sidebar height due to missing margin collapse Changed deprecated MathJax CDN to Cloudflare","title":"1.6.2 _ May 15, 2017"},{"location":"release-notes/#161-_-april-23-2017","text":"Fixed following of active/focused element if search input is focused Fixed layer order of search component elements","title":"1.6.1 _ April 23, 2017"},{"location":"release-notes/#160-_-april-22-2017","text":"Added build test for Docker image on Travis Added search overlay for better user experience (focus) Added language from localizations to html tag Fixed #270 : source links broken for absolute URLs Fixed missing top spacing for first targeted element in content Fixed too small footnote divider when using larger font sizes","title":"1.6.0 _ April 22, 2017"},{"location":"release-notes/#155-_-april-20-2017","text":"Fixed #282 : Browser search ( Meta + F ) is hijacked","title":"1.5.5 _ April 20, 2017"},{"location":"release-notes/#154-_-april-8-2017","text":"Fixed broken highlighting for two or more search terms Fixed missing search results when only a h1 is present Fixed unresponsive overlay on Android","title":"1.5.4 _ April 8, 2017"},{"location":"release-notes/#153-_-april-7-2017","text":"Fixed deprecated calls for template variables Fixed wrong palette color for focused search result Fixed JavaScript errors on 404 page Fixed missing top spacing on 404 page Fixed missing right spacing on overflow of source container","title":"1.5.3 _ April 7, 2017"},{"location":"release-notes/#152-_-april-5-2017","text":"Added requirements as explicit dependencies in setup.py Fixed non-synchronized transitions in search form","title":"1.5.2 _ April 5, 2017"},{"location":"release-notes/#151-_-march-30-2017","text":"Fixed rendering and offset of targetted footnotes Fixed #238 : Link on logo is not set to site_url","title":"1.5.1 _ March 30, 2017"},{"location":"release-notes/#150-_-march-24-2017","text":"Added support for localization of search placeholder Added keyboard events for quick access of search Added keyboard events for search control Added opacity on hover for search buttons Added git hook to skip CI build on non-src changes Fixed non-resetting search placeholder when input is cleared Fixed error for unescaped parentheses in search term Fixed #229 : Button to clear search missing Fixed #231 : Escape key doesn't exit search Removed old-style figures from font feature settings","title":"1.5.0 _ March 24, 2017"},{"location":"release-notes/#141-_-march-16-2017","text":"Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE)","title":"1.4.1 _ March 16, 2017"},{"location":"release-notes/#140-_-march-16-2017","text":"Added support for grouping searched sections by documents Added support for highlighting of search terms Added support for localization of search results Fixed #216 : table of contents icon doesn't show if h1 is not present Reworked style and layout of search results for better usability","title":"1.4.0 _ March 16, 2017"},{"location":"release-notes/#130-_-march-11-2017","text":"Added support for page-specific title and description using metadata Added support for linking source files to documentation Fixed jitter and offset of sidebar when zooming browser Fixed incorrectly initialized tablet sidebar height Fixed regression for #1 : GitHub stars break if repo_url ends with a / Fixed undesired white line below copyright footer due to base font scaling Fixed issue with whitespace in path for scripts Fixed #205 : support non-fixed (static) header Refactored footnote references for better visibility Reduced repaints to a minimum for non-tabs configuration Reduced contrast of edit button (slightly)","title":"1.3.0 _ March 11, 2017"},{"location":"release-notes/#120-_-march-3-2017","text":"Added quote (synonym: cite ) style for Admonition Added help message to build pipeline Fixed wrong navigation link colors when applying palette Fixed #197 : Link missing in tabs navigation on deeply nested items Removed unnecessary dev dependencies","title":"1.2.0 _ March 3, 2017"},{"location":"release-notes/#111-_-february-26-2017","text":"Fixed incorrectly displayed nested lists when using tabs","title":"1.1.1 _ February 26, 2017"},{"location":"release-notes/#110-_-february-26-2017","text":"Added tabs navigation feature (optional) Added Disqus integration (optional) Added a high resolution Favicon with the new logo Added static type checking using Facebook's Flow Fixed #173 : Dictionary elements have no bottom spacing Fixed #175 : Tables cannot be set to 100% width Fixed race conditions in build related to asset revisioning Fixed accidentally re-introduced Permalink on top-level headline Fixed alignment of logo in drawer on IE11 Refactored styles related to tables Refactored and automated Docker build and PyPI release Refactored build scripts","title":"1.1.0 _ February 26, 2017"},{"location":"release-notes/#105-_-february-18-2017","text":"Fixed #153 : Sidebar flows out of constrained area in Chrome 56 Fixed #159 : Footer jitter due to JavaScript if content is short","title":"1.0.5 _ February 18, 2017"},{"location":"release-notes/#104-_-february-16-2017","text":"Fixed #142 : Documentation build errors if h1 is defined as raw HTML Fixed #164 : PyPI release does not build and install Fixed offsets of targeted headlines Increased sidebar font size by 0.12rem","title":"1.0.4 _ February 16, 2017"},{"location":"release-notes/#103-_-january-22-2017","text":"Fixed #117 : Table of contents items don't blur on fast scrolling Refactored sidebar positioning logic Further reduction of repaints","title":"1.0.3 _ January 22, 2017"},{"location":"release-notes/#102-_-january-15-2017","text":"Fixed #108 : Horizontal scrollbar in content area","title":"1.0.2 _ January 15, 2017"},{"location":"release-notes/#101-_-january-14-2017","text":"Fixed massive repaints happening when scrolling Fixed footer back reference positions in case of overflow Fixed header logo from showing when the menu icon is rendered Changed scrollbar behavior to only show when content overflows","title":"1.0.1 _ January 14, 2017"},{"location":"release-notes/#100-_-january-13-2017","text":"Introduced Webpack for more sophisticated JavaScript bundling Introduced ESLint and Stylelint for code style checks Introduced more accurate Material Design colors and shadows Introduced modular scales for harmonic font sizing Introduced git-hooks for better development workflow Rewrite of CSS using the BEM methodology and SassDoc guidelines Rewrite of JavaScript using ES6 and Babel as a transpiler Rewrite of Admonition, Permalinks and CodeHilite integration Rewrite of the complete typographical system Rewrite of Gulp asset pipeline in ES6 and separation of tasks Removed Bower as a dependency in favor of NPM Removed custom icon build in favor of the Material Design iconset Removed _blank targets on links due to vulnerability: http://bit.ly/1Mk2Rtw Removed unversioned assets from build directory Restructured templates into base templates and partials Added build and watch scripts in package.json Added support for Metadata and Footnotes Markdown extensions Added support for PyMdown Extensions package Added support for collapsible sections in navigation Added support for separate table of contents Added support for better accessibility through REM-based layout Added icons for GitHub, GitLab and BitBucket integrations Added more detailed documentation on specimen, extensions etc. Added a 404.html error page for deployment on GitHub Pages Fixed live reload chain in watch mode when saving a template Fixed variable references to work with MkDocs 0.16","title":"1.0.0 _ January 13, 2017"},{"location":"release-notes/#024-_-june-26-2016","text":"Fixed improperly set default favicon Fixed #33 : Protocol relative URL for webfonts doesn't work with file:// Fixed #34 : IE11 on Windows 7 doesn't honor max-width on main tag Fixed #35 : Add styling for blockquotes","title":"0.2.4 _ June 26, 2016"},{"location":"release-notes/#023-_-may-16-2016","text":"Fixed #25 : Highlight inline fenced blocks Fixed #26 : Better highlighting for keystrokes Fixed #30 : Suboptimal syntax highlighting for PHP","title":"0.2.3 _ May 16, 2016"},{"location":"release-notes/#022-_-march-20-2016","text":"Fixed #15 : Document Pygments dependency for CodeHilite Fixed #16 : Favicon could not be set through mkdocs.yml Fixed #17 : Put version into own container for styling Fixed #20 : Fix rounded borders for tables","title":"0.2.2 _ March 20, 2016"},{"location":"release-notes/#021-_-march-12-2016","text":"Fixed #10 : Invisible header after closing search bar with ESC key Fixed #13 : Table cells don't wrap Fixed empty list in table of contents when no headline is defined Corrected wrong path for static asset monitoring in Gulpfile.js Set up tracking of site search for Google Analytics","title":"0.2.1 _ March 12, 2016"},{"location":"release-notes/#020-_-february-24-2016","text":"Fixed #6 : Include multiple color palettes via mkdocs.yml Fixed #7 : Better colors for links inside admonition notes and warnings Fixed #9 : Text for prev/next footer navigation should be customizable Refactored templates (replaced if / else with modifiers where possible)","title":"0.2.0 _ February 24, 2016"},{"location":"release-notes/#013-_-february-21-2016","text":"Fixed #3 : Ordered lists within an unordered list have ::before content Fixed #4 : Click on Logo/Title without Github-Repository: \"None\" Fixed #5 : Page without headlines renders empty list in table of contents Moved Modernizr to top to ensure basic usability in IE8","title":"0.1.3 _ February 21, 2016"},{"location":"release-notes/#012-_-february-16-2016","text":"Fixed styles for deep navigational hierarchies Fixed webfont delivery problem when hosted in subdirectories Fixed print styles in mobile/tablet configuration Added option to configure fonts in mkdocs.yml with fallbacks Changed styles for admonition notes and warnings Set download link to latest version if available Set up tracking of outgoing links and actions for Google Analytics","title":"0.1.2 _ February 16, 2016"},{"location":"release-notes/#011-_-february-11-2016","text":"Fixed #1 : GitHub stars don't work if the repo_url ends with a / Updated NPM and Bower dependencies to most recent versions Changed footer/copyright link to Material theme to GitHub pages Made MkDocs building/serving in build process optional Set up continuous integration with Travis","title":"0.1.1 _ February 11, 2016"},{"location":"release-notes/#010-_-february-9-2016","text":"Initial release","title":"0.1.0 _ February 9, 2016"},{"location":"specimen/","text":"Specimen \u00b6 Body copy \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum. Headings \u00b6 The 3 rd level \u00b6 The 4 th level \u00b6 The 5 th level \u00b6 The 6 th level \u00b6 Headings with secondary text \u00b6 The 3 rd level with secondary text \u00b6 The 4 th level with secondary text \u00b6 The 5 th level with secondary text \u00b6 The 6 th level with secondary text \u00b6 Blockquotes \u00b6 Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur. Blockquote nesting \u00b6 Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks \u00b6 Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero. Lists \u00b6 Unordered lists \u00b6 Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Ordered lists \u00b6 Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget var _extends ornare tellus, ut gravida mi. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo. Definition lists \u00b6 Lorem ipsum dolor sit amet Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Code blocks \u00b6 Inline \u00b6 Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus. Listing \u00b6 1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Horizontal rules \u00b6 Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Data tables \u00b6 Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Praproses data"},{"location":"specimen/#specimen","text":"","title":"Specimen"},{"location":"specimen/#body-copy","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum.","title":"Body copy"},{"location":"specimen/#headings","text":"","title":"Headings"},{"location":"specimen/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"specimen/#the-4th-level","text":"","title":"The 4th level"},{"location":"specimen/#the-5th-level","text":"","title":"The 5th level"},{"location":"specimen/#the-6th-level","text":"","title":"The 6th level"},{"location":"specimen/#headings-with-secondary-text","text":"","title":"Headings with secondary text"},{"location":"specimen/#the-3rd-level-with-secondary-text","text":"","title":"The 3rd level with secondary text"},{"location":"specimen/#the-4th-level-with-secondary-text","text":"","title":"The 4th level with secondary text"},{"location":"specimen/#the-5th-level-with-secondary-text","text":"","title":"The 5th level with secondary text"},{"location":"specimen/#the-6th-level-with-secondary-text","text":"","title":"The 6th level with secondary text"},{"location":"specimen/#blockquotes","text":"Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur.","title":"Blockquotes"},{"location":"specimen/#blockquote-nesting","text":"Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa.","title":"Blockquote nesting"},{"location":"specimen/#other-content-blocks","text":"Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero.","title":"Other content blocks"},{"location":"specimen/#lists","text":"","title":"Lists"},{"location":"specimen/#unordered-lists","text":"Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam.","title":"Unordered lists"},{"location":"specimen/#ordered-lists","text":"Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget var _extends ornare tellus, ut gravida mi. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo.","title":"Ordered lists"},{"location":"specimen/#definition-lists","text":"Lorem ipsum dolor sit amet Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris.","title":"Definition lists"},{"location":"specimen/#code-blocks","text":"","title":"Code blocks"},{"location":"specimen/#inline","text":"Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.","title":"Inline"},{"location":"specimen/#listing","text":"1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; };","title":"Listing"},{"location":"specimen/#horizontal-rules","text":"Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales.","title":"Horizontal rules"},{"location":"specimen/#data-tables","text":"Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Data tables"},{"location":"extensions/admonition/","text":"Pengantar Data Science \u00b6 silahkan ditunggu kontennya","title":"Admonition"},{"location":"extensions/admonition/#pengantar-data-science","text":"silahkan ditunggu kontennya","title":"Pengantar Data Science"},{"location":"extensions/codehilite/","text":"CodeHilite \u00b6 CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed. Installation \u00b6 CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions : - codehilite Usage \u00b6 Specifying the language \u00b6 The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways. via Markdown syntax recommended \u00b6 In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf via Shebang \u00b6 Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf via three colons \u00b6 If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf Adding line numbers \u00b6 Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions : - codehilite : linenums : true Example: ``` python \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Grouping code blocks \u00b6 The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab=\"Bash\" #!/bin/bash echo \"Hello world!\" ``` ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` ``` c++ tab=\"C++\" #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Highlighting specific lines \u00b6 Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Supported languages excerpt \u00b6 CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt. Bash \u00b6 #!/bin/bash for OPT in \" $@ \" do case \" $OPT \" in '-f' ) canonicalize = 1 ;; '-n' ) switchlf = \"-n\" ;; esac done # readlink -f function __readlink_f { target = \" $1 \" while test -n \" $target \" ; do filepath = \" $target \" cd ` dirname \" $filepath \" ` target = ` readlink \" $filepath \" ` done /bin/echo $switchlf ` pwd -P ` / ` basename \" $filepath \" ` } if [ ! \" $canonicalize \" ] ; then readlink $switchlf \" $@ \" else for file in \" $@ \" do case \" $file \" in -* ) ;; * ) __readlink_f \" $file \" ;; esac done fi exit $? C \u00b6 extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data && left ); left = left > 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask & mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] & 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ } C++ \u00b6 Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ \"signature\" ] = descriptor_ -> full_name (); /* Prepare message symbol */ variables_ [ \"message\" ] = StringReplace ( variables_ [ \"signature\" ], \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"message\" ])); /* Suffix scope to identifiers, if given */ string suffix ( \"\" ); if ( scope_ ) { suffix = scope_ -> full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ -> file () -> package (). compare ( descriptor_ -> file () -> package ())) suffix = StripPrefixString ( suffix , scope_ -> file () -> package () + \".\" ); /* Append to signature */ variables_ [ \"signature\" ] += \".[\" + suffix + \"]\" ; suffix = \"_\" + suffix ; } /* Prepare extension symbol */ variables_ [ \"extension\" ] = StringReplace ( suffix , \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"extension\" ])); } C# \u00b6 public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount > startTickCount + timeout ) throw new Exception ( \"Timeout.\" ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent < size ); } Clojure \u00b6 ( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % \">>\" )) [ \"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\" ]) Diff \u00b6 Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js', Docker \u00b6 FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [ \"x11vnc\" , \"-forever\" , \"-usepw\" , \"-create\" ] Elixir \u00b6 require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info \"Accepting connections on port #{ port } \" loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket |> read_line () |> write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end Erlang \u00b6 circular ( Defs ) -> [ { { Type , Base }, Fields } || { { Type , Base }, Fields } <- Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) -> Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) -> false ; circular ( Defs , [ Field | Fields ], Path ) -> case Field #field.type of { msg , Type } -> case lists : member ( Type , Path ) of false -> Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false -> circular ( Defs , Fields , Path ); true -> true end ; true -> Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ -> circular ( Defs , Fields , Path ) end . F# \u00b6 /// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [< Js >] getRectangles () : Async < Rectangle [] > = async { let req = XMLHttpRequest () req . Open ( \"POST\" , \"/get\" , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [< Js >] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects |> Array . iter createRectangle } Go \u00b6 package main import \"fmt\" func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i < 10000000 ; i ++ { fmt . Println ( \"process\" , id , \" send\" , i ) channel <- 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( \"receiving\" ) x += i } fmt . Println ( x ) } HTML \u00b6 <!doctype html> < html class = \"no-js\" lang = \"\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"x-ua-compatible\" content = \"ie=edge\" > < title > HTML5 Boilerplate </ title > < meta name = \"description\" content = \"\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"apple-touch-icon\" href = \"apple-touch-icon.png\" > < link rel = \"stylesheet\" href = \"css/normalize.css\" > < link rel = \"stylesheet\" href = \"css/main.css\" > < script src = \"js/vendor/modernizr-2.8.3.min.js\" ></ script > </ head > < body > < p > Hello world! This is HTML5 Boilerplate. </ p > </ body > </ html > Java \u00b6 import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet < E > { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList < E >[] con ; public UnsortedHashSet () { con = ( LinkedList < E >[] )( new LinkedList [ 10 ] ); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList < E > (); if ( ! con [ index ] . contains ( obj )) { con [ index ] . add ( obj ); size ++ ; } if ( 1.0 * size / con . length > LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet < E > temp = new UnsortedHashSet < E > (); temp . con = ( LinkedList < E >[] )( new LinkedList [ con . length * 2 + 1 ] ); for ( int i = 0 ; i < con . length ; i ++ ) { if ( con [ i ] != null ) for ( E e : con [ i ] ) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } } JavaScript \u00b6 var Math = require ( 'lib/math' ); var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ 'default' ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ 'default' ], exports ); JSON \u00b6 { \"name\" : \"mkdocs-material\" , \"version\" : \"0.2.4\" , \"description\" : \"A Material Design theme for MkDocs\" , \"homepage\" : \"http://squidfunk.github.io/mkdocs-material/\" , \"authors\" : [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\" : \"MIT\" , \"main\" : \"Gulpfile.js\" , \"scripts\" : { \"start\" : \"./node_modules/.bin/gulp watch --mkdocs\" , \"build\" : \"./node_modules/.bin/gulp build --production\" } ... } Julia \u00b6 using MXNet mlp = @mx . chain mx . Variable ( : data ) => mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) => mx . Activation ( name =: relu1 , act_type =: relu ) => mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) => mx . Activation ( name =: relu2 , act_type =: relu ) => mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) => mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( \"MXNet\" , \"examples\" , \"mnist\" , \"mnist-data.jl\" )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider ) Lua \u00b6 local ffi = require ( \"ffi\" ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == \"Windows\" then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( \".\" ); io.flush () sleep ( 0.01 ) end io.write ( \" \\n \" ) MySQL \u00b6 SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ; PHP \u00b6 <?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( '<html><body>Lucky number: ' . $number . '</body></html>' ); } } Protocol Buffers \u00b6 syntax = \"proto2\" ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; } Python \u00b6 \"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( 'data_dir' , '/tmp/data/' , 'Directory for storing data' ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b ) Ruby \u00b6 require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError ; end class MissingCallback < StandardError ; end class InvalidState < StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, & block @finity ||= Machine . new self , options , & block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end XML \u00b6 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" > <!-- This is a sample comment --> <childTag attribute= \"Quoted Value\" another-attribute= 'Single quoted value' a-third-attribute= '123' > <withTextContent> Some text content </withTextContent> <withEntityContent> Some text content with &lt; entities &gt; and mentioning uint8_t and int32_t </withEntityContent> <otherTag attribute= 'Single quoted Value' /> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"CodeHilite"},{"location":"extensions/codehilite/#codehilite","text":"CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed.","title":"CodeHilite"},{"location":"extensions/codehilite/#installation","text":"CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions : - codehilite","title":"Installation"},{"location":"extensions/codehilite/#usage","text":"","title":"Usage"},{"location":"extensions/codehilite/#specifying-the-language","text":"The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways.","title":"Specifying the language"},{"location":"extensions/codehilite/#via-markdown-syntax-recommended","text":"In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf","title":"via Markdown syntax recommended"},{"location":"extensions/codehilite/#via-shebang","text":"Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf","title":"via Shebang"},{"location":"extensions/codehilite/#via-three-colons","text":"If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf","title":"via three colons"},{"location":"extensions/codehilite/#adding-line-numbers","text":"Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions : - codehilite : linenums : true Example: ``` python \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Adding line numbers"},{"location":"extensions/codehilite/#grouping-code-blocks","text":"The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab=\"Bash\" #!/bin/bash echo \"Hello world!\" ``` ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` ``` c++ tab=\"C++\" #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Grouping code blocks"},{"location":"extensions/codehilite/#highlighting-specific-lines","text":"Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"extensions/codehilite/#supported-languages-excerpt","text":"CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt.","title":"Supported languages excerpt"},{"location":"extensions/codehilite/#bash","text":"#!/bin/bash for OPT in \" $@ \" do case \" $OPT \" in '-f' ) canonicalize = 1 ;; '-n' ) switchlf = \"-n\" ;; esac done # readlink -f function __readlink_f { target = \" $1 \" while test -n \" $target \" ; do filepath = \" $target \" cd ` dirname \" $filepath \" ` target = ` readlink \" $filepath \" ` done /bin/echo $switchlf ` pwd -P ` / ` basename \" $filepath \" ` } if [ ! \" $canonicalize \" ] ; then readlink $switchlf \" $@ \" else for file in \" $@ \" do case \" $file \" in -* ) ;; * ) __readlink_f \" $file \" ;; esac done fi exit $?","title":"Bash"},{"location":"extensions/codehilite/#c","text":"extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data && left ); left = left > 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask & mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] & 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ }","title":"C"},{"location":"extensions/codehilite/#c_1","text":"Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ \"signature\" ] = descriptor_ -> full_name (); /* Prepare message symbol */ variables_ [ \"message\" ] = StringReplace ( variables_ [ \"signature\" ], \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"message\" ])); /* Suffix scope to identifiers, if given */ string suffix ( \"\" ); if ( scope_ ) { suffix = scope_ -> full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ -> file () -> package (). compare ( descriptor_ -> file () -> package ())) suffix = StripPrefixString ( suffix , scope_ -> file () -> package () + \".\" ); /* Append to signature */ variables_ [ \"signature\" ] += \".[\" + suffix + \"]\" ; suffix = \"_\" + suffix ; } /* Prepare extension symbol */ variables_ [ \"extension\" ] = StringReplace ( suffix , \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"extension\" ])); }","title":"C++"},{"location":"extensions/codehilite/#c_2","text":"public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount > startTickCount + timeout ) throw new Exception ( \"Timeout.\" ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent < size ); }","title":"C&#35;"},{"location":"extensions/codehilite/#clojure","text":"( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % \">>\" )) [ \"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\" ])","title":"Clojure"},{"location":"extensions/codehilite/#diff","text":"Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js',","title":"Diff"},{"location":"extensions/codehilite/#docker","text":"FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [ \"x11vnc\" , \"-forever\" , \"-usepw\" , \"-create\" ]","title":"Docker"},{"location":"extensions/codehilite/#elixir","text":"require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info \"Accepting connections on port #{ port } \" loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket |> read_line () |> write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end","title":"Elixir"},{"location":"extensions/codehilite/#erlang","text":"circular ( Defs ) -> [ { { Type , Base }, Fields } || { { Type , Base }, Fields } <- Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) -> Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) -> false ; circular ( Defs , [ Field | Fields ], Path ) -> case Field #field.type of { msg , Type } -> case lists : member ( Type , Path ) of false -> Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false -> circular ( Defs , Fields , Path ); true -> true end ; true -> Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ -> circular ( Defs , Fields , Path ) end .","title":"Erlang"},{"location":"extensions/codehilite/#f","text":"/// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [< Js >] getRectangles () : Async < Rectangle [] > = async { let req = XMLHttpRequest () req . Open ( \"POST\" , \"/get\" , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [< Js >] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects |> Array . iter createRectangle }","title":"F&#35;"},{"location":"extensions/codehilite/#go","text":"package main import \"fmt\" func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i < 10000000 ; i ++ { fmt . Println ( \"process\" , id , \" send\" , i ) channel <- 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( \"receiving\" ) x += i } fmt . Println ( x ) }","title":"Go"},{"location":"extensions/codehilite/#html","text":"<!doctype html> < html class = \"no-js\" lang = \"\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"x-ua-compatible\" content = \"ie=edge\" > < title > HTML5 Boilerplate </ title > < meta name = \"description\" content = \"\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"apple-touch-icon\" href = \"apple-touch-icon.png\" > < link rel = \"stylesheet\" href = \"css/normalize.css\" > < link rel = \"stylesheet\" href = \"css/main.css\" > < script src = \"js/vendor/modernizr-2.8.3.min.js\" ></ script > </ head > < body > < p > Hello world! This is HTML5 Boilerplate. </ p > </ body > </ html >","title":"HTML"},{"location":"extensions/codehilite/#java","text":"import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet < E > { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList < E >[] con ; public UnsortedHashSet () { con = ( LinkedList < E >[] )( new LinkedList [ 10 ] ); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList < E > (); if ( ! con [ index ] . contains ( obj )) { con [ index ] . add ( obj ); size ++ ; } if ( 1.0 * size / con . length > LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet < E > temp = new UnsortedHashSet < E > (); temp . con = ( LinkedList < E >[] )( new LinkedList [ con . length * 2 + 1 ] ); for ( int i = 0 ; i < con . length ; i ++ ) { if ( con [ i ] != null ) for ( E e : con [ i ] ) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } }","title":"Java"},{"location":"extensions/codehilite/#javascript","text":"var Math = require ( 'lib/math' ); var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ 'default' ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ 'default' ], exports );","title":"JavaScript"},{"location":"extensions/codehilite/#json","text":"{ \"name\" : \"mkdocs-material\" , \"version\" : \"0.2.4\" , \"description\" : \"A Material Design theme for MkDocs\" , \"homepage\" : \"http://squidfunk.github.io/mkdocs-material/\" , \"authors\" : [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\" : \"MIT\" , \"main\" : \"Gulpfile.js\" , \"scripts\" : { \"start\" : \"./node_modules/.bin/gulp watch --mkdocs\" , \"build\" : \"./node_modules/.bin/gulp build --production\" } ... }","title":"JSON"},{"location":"extensions/codehilite/#julia","text":"using MXNet mlp = @mx . chain mx . Variable ( : data ) => mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) => mx . Activation ( name =: relu1 , act_type =: relu ) => mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) => mx . Activation ( name =: relu2 , act_type =: relu ) => mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) => mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( \"MXNet\" , \"examples\" , \"mnist\" , \"mnist-data.jl\" )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider )","title":"Julia"},{"location":"extensions/codehilite/#lua","text":"local ffi = require ( \"ffi\" ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == \"Windows\" then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( \".\" ); io.flush () sleep ( 0.01 ) end io.write ( \" \\n \" )","title":"Lua"},{"location":"extensions/codehilite/#mysql","text":"SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ;","title":"MySQL"},{"location":"extensions/codehilite/#php","text":"<?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( '<html><body>Lucky number: ' . $number . '</body></html>' ); } }","title":"PHP"},{"location":"extensions/codehilite/#protocol-buffers","text":"syntax = \"proto2\" ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; }","title":"Protocol Buffers"},{"location":"extensions/codehilite/#python","text":"\"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( 'data_dir' , '/tmp/data/' , 'Directory for storing data' ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b )","title":"Python"},{"location":"extensions/codehilite/#ruby","text":"require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError ; end class MissingCallback < StandardError ; end class InvalidState < StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, & block @finity ||= Machine . new self , options , & block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end","title":"Ruby"},{"location":"extensions/codehilite/#xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" > <!-- This is a sample comment --> <childTag attribute= \"Quoted Value\" another-attribute= 'Single quoted value' a-third-attribute= '123' > <withTextContent> Some text content </withTextContent> <withEntityContent> Some text content with &lt; entities &gt; and mentioning uint8_t and int32_t </withEntityContent> <otherTag attribute= 'Single quoted Value' /> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"XML"},{"location":"extensions/footnotes/","text":"Footnotes \u00b6 Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions : - footnotes Usage \u00b6 The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document. Inserting the reference \u00b6 The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Inserting the content \u00b6 The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference. on a single line \u00b6 Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page on multiple lines \u00b6 Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Footnotes"},{"location":"extensions/footnotes/#footnotes","text":"Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation.","title":"Footnotes"},{"location":"extensions/footnotes/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - footnotes","title":"Installation"},{"location":"extensions/footnotes/#usage","text":"The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document.","title":"Usage"},{"location":"extensions/footnotes/#inserting-the-reference","text":"The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"Inserting the reference"},{"location":"extensions/footnotes/#inserting-the-content","text":"The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference.","title":"Inserting the content"},{"location":"extensions/footnotes/#on-a-single-line","text":"Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page","title":"on a single line"},{"location":"extensions/footnotes/#on-multiple-lines","text":"Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"on multiple lines"},{"location":"extensions/metadata/","text":"Metadata \u00b6 The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions : - meta Usage \u00b6 Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material. Setting a hero text \u00b6 Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts Linking sources \u00b6 When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output. Redirecting to another page \u00b6 It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url . Overrides \u00b6 Page title \u00b6 The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title. Page description \u00b6 The page description can also be overridden on a per-document level: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value. Disqus \u00b6 As described in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Metadata"},{"location":"extensions/metadata/#metadata","text":"The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context.","title":"Metadata"},{"location":"extensions/metadata/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - meta","title":"Installation"},{"location":"extensions/metadata/#usage","text":"Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material.","title":"Usage"},{"location":"extensions/metadata/#setting-a-hero-text","text":"Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts","title":"Setting a hero text"},{"location":"extensions/metadata/#linking-sources","text":"When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output.","title":"Linking sources"},{"location":"extensions/metadata/#redirecting-to-another-page","text":"It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url .","title":"Redirecting to another page"},{"location":"extensions/metadata/#overrides","text":"","title":"Overrides"},{"location":"extensions/metadata/#page-title","text":"The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title.","title":"Page title"},{"location":"extensions/metadata/#page-description","text":"The page description can also be overridden on a per-document level: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value.","title":"Page description"},{"location":"extensions/metadata/#disqus","text":"As described in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Disqus"},{"location":"extensions/permalinks/","text":"Permalinks \u00b6 Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document. Installation \u00b6 To enable permalinks, add the following to your mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link Usage \u00b6 When enabled, permalinks are inserted automatically.","title":"Permalinks"},{"location":"extensions/permalinks/#permalinks","text":"Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document.","title":"Permalinks"},{"location":"extensions/permalinks/#installation","text":"To enable permalinks, add the following to your mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link","title":"Installation"},{"location":"extensions/permalinks/#usage","text":"When enabled, permalinks are inserted automatically.","title":"Usage"},{"location":"extensions/pymdown/","text":"PyMdown Extensions \u00b6 PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme. Installation \u00b6 The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde Usage \u00b6 Arithmatex MathJax \u00b6 Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript : - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \"\\\\(\" , \"\\\\)\" ] ], displayMath : [ [ \"\\\\[\" , \"\\\\]\" ] ] }, TeX : { TagSide : \"right\" , TagIndent : \".8em\" , MultLineWidth : \"85%\" , equationNumbers : { autoNumber : \"AMS\" , }, unicode : { fonts : \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign : \"left\" , showProcessingMessages : false , messageStyle : \"none\" }; In your mkdocs.yml , include it with: extra_javascript : - 'javascripts/extra.js' - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' Blocks \u00b6 Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline \u00b6 Inline equations need to be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} BetterEm \u00b6 BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes . Caret \u00b6 Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ . Critic \u00b6 Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Details \u00b6 Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes. Emoji \u00b6 Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution. InlineHilite \u00b6 InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be achieved by prefixing inline code with a shebang and language identifier, e.g. #!js . MagicLink \u00b6 MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses. Mark \u00b6 Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== . SmartSymbols \u00b6 SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...). SuperFences \u00b6 SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs . Tasklist \u00b6 Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Tilde \u00b6 Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#pymdown-extensions","text":"PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#installation","text":"The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde","title":"Installation"},{"location":"extensions/pymdown/#usage","text":"","title":"Usage"},{"location":"extensions/pymdown/#arithmatex-mathjax","text":"Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript : - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \"\\\\(\" , \"\\\\)\" ] ], displayMath : [ [ \"\\\\[\" , \"\\\\]\" ] ] }, TeX : { TagSide : \"right\" , TagIndent : \".8em\" , MultLineWidth : \"85%\" , equationNumbers : { autoNumber : \"AMS\" , }, unicode : { fonts : \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign : \"left\" , showProcessingMessages : false , messageStyle : \"none\" }; In your mkdocs.yml , include it with: extra_javascript : - 'javascripts/extra.js' - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'","title":"Arithmatex MathJax"},{"location":"extensions/pymdown/#blocks","text":"Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Blocks"},{"location":"extensions/pymdown/#inline","text":"Inline equations need to be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline"},{"location":"extensions/pymdown/#betterem","text":"BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes .","title":"BetterEm"},{"location":"extensions/pymdown/#caret","text":"Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ .","title":"Caret"},{"location":"extensions/pymdown/#critic","text":"Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.","title":"Critic"},{"location":"extensions/pymdown/#details","text":"Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes.","title":"Details"},{"location":"extensions/pymdown/#emoji","text":"Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution.","title":"Emoji"},{"location":"extensions/pymdown/#inlinehilite","text":"InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be achieved by prefixing inline code with a shebang and language identifier, e.g. #!js .","title":"InlineHilite"},{"location":"extensions/pymdown/#magiclink","text":"MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses.","title":"MagicLink"},{"location":"extensions/pymdown/#mark","text":"Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== .","title":"Mark"},{"location":"extensions/pymdown/#smartsymbols","text":"SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...).","title":"SmartSymbols"},{"location":"extensions/pymdown/#superfences","text":"SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs .","title":"SuperFences"},{"location":"extensions/pymdown/#tasklist","text":"Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Tasklist"},{"location":"extensions/pymdown/#tilde","text":"Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"Tilde"}]}